{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CS 598: Deep Learning for Healthcare\n",
        "\n",
        "Instructor: Dr. Jimeng Sun\n",
        "\n",
        "4/14/2024\n",
        "\n",
        "Carmelita Valimento\n",
        "\n",
        "William Su\n",
        "\n",
        "Austin Harmon\n",
        "\n",
        "**Project Draft for Replicating the Paper: “Investigating Sleep Apnea in Children with a Specialized Multi-Modal Transformer”**"
      ],
      "metadata": {
        "id": "24_70I1IdFRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Github Repository: https://github.com/dlh-team-4/t4 #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/Shareddrives\""
      ],
      "metadata": {
        "id": "pq3s27yjfrlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "OsjJe2xle2sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "if (!require(devtools)) {\n",
        "  install.packages(\"devtools\")\n",
        "}\n",
        "devtools::install_github(\"muschellij2/nsrr\")"
      ],
      "metadata": {
        "id": "1ot_hIagdyEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# setting our token for data download REMOVE THIS AT THE END\n",
        "Sys.setenv(NSRR_TOKEN = \"abc-123\")"
      ],
      "metadata": {
        "id": "lPRqOP7keEjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# check for authentication\n",
        "library(nsrr)\n",
        "nsrr_auth()"
      ],
      "metadata": {
        "id": "opSY9VP_eHL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "dataset <- \"chat\"\n",
        "path <- \"/content/drive/Shared with me/DLH 20204 Team 4/nchsdb/health_data\"\n",
        "files <- nsrr_all_dataset_files(dataset, path)\n"
      ],
      "metadata": {
        "id": "V63LjYUveJrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "#to download another file from CHAT, view sleepdata.org/datasets/chat/files to find the correct path and file\n",
        "dl = nsrr_download_file(\"chat\", path = \"datasets/CHANGELOG.md\")\n",
        "\n",
        "dl$outfile\n",
        "\n",
        "print(dl) #to download the file, click the blue URL next to \"Response\""
      ],
      "metadata": {
        "id": "ALUAb8JiiZwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Sleep apnea in children poses a significant health concern, affecting approximately one to five percent of children in the United States. Unlike adult sleep apnea, pediatric sleep apnea presents distinct clinical causes and characteristics, demanding specialized attention. However, research dedicated to pediatric sleep apnea detection has been comparatively limited, especially concerning at-home testing tools and algorithmic approaches for automatic detection. Addressing this gap is crucial due to the potential adverse effects of untreated pediatric sleep apnea on both physical and mental health.\n",
        "\n",
        "Polysomnography, the gold standard for diagnosing sleep-related breathing disorders, including apnea and hypopnea, presents numerous challenges, such as complexity, cost, and the need for clinical involvement. Consequently, there is a growing interest in developing accessible and effective diagnostic methods, particularly for pediatric populations, to mitigate the limitations of traditional polysomnography.\n",
        "\n",
        "Recent advancements in artificial intelligence (AI) have spurred research into machine learning-based approaches for diagnosing sleep apnea without relying on polysomnography. While some studies have shown promising results in adults, research focusing on pediatric populations remains scarce. This scarcity underscores the importance of the present study, which introduces a machine learning-based model specifically tailored for detecting apnea events in children using commonly collected sleep signals.\n",
        "\n",
        "The paper proposed a machine learning-based model for detecting obstructive sleep apnea-hypopnea syndrome (OSAHS) in pediatric patients using commonly collected sleep signals. It aimed to address the gap in pediatric sleep apnea detection by presenting a method that could achieve adult-level performance in detecting OSAHS patterns in children. It also introduced a customized transformer-based architecture for detecting OSAHS, which showed superior performance compared to existing methods. It utilized a novel data representation technique to handle polysomnography modalities, enabling the model to effectively process signals from different sources.\n",
        "\n",
        "The study extensively explored the role of different combinations of common modalities and demonstrated that using only two easier-to-collect signals (ECG and SpO2) could achieve close to maximum performance. The proposed method outperformed state-of-the-art methods across two public datasets, as determined by the F1-score and AUROC measures. Additionally, using only ECG and SpO2 signals, which are easier to collect at home, the method achieved very competitive results, addressing concerns about collecting various sleep signals from children outside the clinic. The paper contributes significantly to the research regime by addressing the gap in pediatric sleep apnea detection, which has been understudied compared to adult sleep apnea, introducing a novel machine learning-based approach specifically tailored for pediatric OSAHS detection, which can potentially improve the accessibility of pediatric sleep apnea testing and treatment interventions, and demonstrating the feasibility of achieving adult-level performance in detecting OSAHS in children, thereby advancing the field towards more accessible and timely diagnosis and treatment of pediatric sleep disorders.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: It is possible to achieve adult-level performance in detecting obstructive sleep apnea-hypopnea syndrome (OSAHS) in pediatric populations using machine learning-based methods.\n",
        "2.   Hypothesis 2: It is possible to achieve polysomnography (PSG)-level performance using only two easier-to-collect signals: ECG and SpO2.\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ],
      "metadata": {
        "id": "LM4WUjz64C3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "#drive.mount('/dlh')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_dir = \"/content/drive/Shared with me/DLH 20204 Team 4/Datasets/CHAT_EDF_EXAMPLE.png\"\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread(img_dir)\n",
        "cv2_imshow(img)\n"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources: The data used in this study was obtained from National Sleep Research Resource (NSRR). We are using the Childhood Adenotonsillectomy Trial (CHAT) data. A formal data access request was made by filling out a form with information about our research project. After approval, an access token was provided, which is used for downloading the necessary dataset files. The appropriate files were uploaded to Google Drive and used by our program.\n",
        "\n",
        "Installation: To install the data, we requested access by first taking the UIUC HIPAA training and then presenting our use case to NSRR for approval.\n",
        "Once approved, we used nsrr-gem to download the datasets we needed.\n",
        "For the CHAT data, we executed the command:\n",
        "\n",
        "nsrr download chat/polysomnography/edfs/baseline\n",
        "\n",
        "We also needed the NSRR annotations for each edf file downloaded using the previous command:\n",
        "\n",
        "nsrr download chat/polysomnography/annotations-events-nsrr/baseline\n",
        "\n",
        "Once downloaded, we took the dataset directory and supplied it to the directory variables in our python notebook.\n",
        "\n",
        "Data Process: After the data was downloaded, the information was preprocessed to produce .npz files. Compressing the data in another format was a necessary step to prepare the data for the data loader. The data loader produces additional compressed files that are designed to be used with the main training function.\n",
        "\n"
      ],
      "metadata": {
        "id": "OeLTv65eMGHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "#CHAT_Preprocessor\n",
        "!pip install mne numpy pandas\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "from itertools import compress\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "from mne import make_fixed_length_events\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "PSG_DIR = \"./data/chat/\"\n",
        "OUT_DIR = './data/chat/preprocessed'\n",
        "\n",
        "THRESHOLD = 3\n",
        "NUM_WORKER = 1\n",
        "FREQ = 128.0\n",
        "EPOCH_LENGTH = 30.0\n",
        "\n",
        "channels = [\n",
        "    'E1',  # 0\n",
        "    'E2',  # 1\n",
        "    'F3',  # 2\n",
        "    'F4',  # 3\n",
        "    'C3',  # 4\n",
        "    'C4',  # 5\n",
        "    'M1',  # 6\n",
        "    'M2',  # 7\n",
        "    'O1',  # 8\n",
        "    'O2',  # 9\n",
        "    'ECG1',  # 10\n",
        "    'ECG3',  # 11\n",
        "\n",
        "    'CANNULAFLOW',  # 12\n",
        "    'AIRFLOW',  # 13\n",
        "    'CHEST',  # 14\n",
        "    'ABD',  # 15\n",
        "    'SAO2',  # 16\n",
        "    'CAP',  # 17\n",
        "]\n",
        "\n",
        "APNEA_EVENT_DICT = {\n",
        "    \"Obstructive apnea|Obstructive Apnea\": 2,\n",
        "    \"Central apnea|Central Apnea\": 2,\n",
        "}\n",
        "\n",
        "HYPOPNEA_EVENT_DICT = {\n",
        "    \"Hypopnea|Hypopnea\": 1,\n",
        "}\n",
        "\n",
        "POS_EVENT_DICT = {\n",
        "    \"Hypopnea|Hypopnea\": 1,\n",
        "    \"Obstructive apnea|Obstructive Apnea\": 2,\n",
        "    \"Central apnea|Central Apnea\": 2,\n",
        "}\n",
        "\n",
        "NEG_EVENT_DICT = {\n",
        "    'Stage 1 sleep|1': 0,\n",
        "    'Stage 2 sleep|2': 0,\n",
        "    'Stage 3 sleep|3': 0,\n",
        "    'REM sleep|5': 0,\n",
        "}\n",
        "\n",
        "WAKE_DICT = {\n",
        "    \"Wake\": 10,\n",
        "    \"Wake|0\": 10\n",
        "}\n",
        "def identity(df):\n",
        "    # just returns whatever DataFrame df is passed to it, essentially acting as a placeholder function.\n",
        "    return df\n",
        "\n",
        "def parseScoredEvents(annotation_path):\n",
        "    with open(annotation_path, \"r\") as f:\n",
        "        xml_data = f.read()\n",
        "    root = ET.fromstring(xml_data)\n",
        "    scored_events = []\n",
        "    for scored_event in root.find('ScoredEvents'):\n",
        "        # Leaving this here just in case I mapped incorrectly\n",
        "        # But all we need are fields onset, duration and description\n",
        "        event_data = {\n",
        "            'event_type': scored_event.find('EventType').text,\n",
        "            'description': scored_event.find('EventConcept').text,\n",
        "            'onset': scored_event.find('Start').text,\n",
        "            'duration': scored_event.find('Duration').text,\n",
        "            'clock_time': scored_event.find('ClockTime').text if scored_event.find('ClockTime') is not None else None,\n",
        "            'signal_location': scored_event.find('SignalLocation').text if scored_event.find('SignalLocation') is not None else None\n",
        "        }\n",
        "        scored_events.append(event_data)\n",
        "    df = pd.DataFrame(scored_events)\n",
        "    return df.drop(df.index[0])\n",
        "\n",
        "def load_study_chat(edf_path, annotation_path, annotation_func, preload=False, exclude=[], verbose='CRITICAL'):\n",
        "    # load data from an EDF file\n",
        "    raw = mne.io.read_raw_edf(input_fname=edf_path, exclude=exclude, preload=preload, verbose=verbose)\n",
        "\n",
        "    df = annotation_func(parseScoredEvents(annotation_path))\n",
        "    annotations = mne.Annotations(df.onset, df.duration, df.description)\n",
        "    raw.set_annotations(annotations)\n",
        "\n",
        "    raw.rename_channels({name: name.upper() for name in raw.info['ch_names']})\n",
        "\n",
        "    return raw\n",
        "\n",
        "def preprocess(path, annotation_modifier, out_dir):\n",
        "    print(path)\n",
        "    is_apnea_available, is_hypopnea_available = True, True\n",
        "    raw = load_study_chat(path[0], path[1], annotation_modifier, verbose=True)\n",
        "\n",
        "    ### Channel Check ###\n",
        "    if not all([name in raw.ch_names for name in channels]):\n",
        "        print([name in raw.ch_names for name in channels])\n",
        "        print(\"study \" + os.path.basename(path[0]) + \" skipped since insufficient channels\")\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=POS_EVENT_DICT, chunk_duration=1.0,\n",
        "                                                              verbose=None)\n",
        "    except ValueError as e:\n",
        "        print(str(e))\n",
        "        print(\"No Chunk found!\")\n",
        "        return 0\n",
        "\n",
        "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
        "    print(str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %s' % os.path.basename(path[0]))\n",
        "\n",
        "\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=APNEA_EVENT_DICT, chunk_duration=1.0,\n",
        "                                                              verbose=None)\n",
        "    except ValueError:\n",
        "        is_apnea_available = False\n",
        "\n",
        "    try:\n",
        "        hypopnea_events, event_ids = mne.events_from_annotations(raw, event_id=HYPOPNEA_EVENT_DICT, chunk_duration=1.0,\n",
        "                                                                 verbose=None)\n",
        "    except ValueError:\n",
        "        is_hypopnea_available = False\n",
        "\n",
        "    wake_events, event_ids = mne.events_from_annotations(raw, event_id=WAKE_DICT, chunk_duration=1.0, verbose=None)\n",
        "\n",
        "    ####################################################################################################################\n",
        "\n",
        "    sfreq = raw.info['sfreq']\n",
        "    tmax = EPOCH_LENGTH - 1. / sfreq\n",
        "\n",
        "    raw = raw.pick_channels(channels, ordered=True)\n",
        "    fixed_events = make_fixed_length_events(raw, id=0, duration=EPOCH_LENGTH, overlap=0.)\n",
        "\n",
        "    try:\n",
        "        epochs = mne.Epochs(raw, fixed_events, event_id=[0], tmin=0, tmax=tmax, baseline=None, preload=True, proj=False, verbose=None)\n",
        "        epochs.load_data()\n",
        "\n",
        "    except AssertionError:\n",
        "        return 0\n",
        "\n",
        "    if sfreq != FREQ:\n",
        "        epochs = epochs.resample(FREQ, npad='auto', n_jobs=8, verbose=None)\n",
        "    data = epochs.get_data()\n",
        "\n",
        "    ####################################################################################################################\n",
        "    if is_apnea_available:\n",
        "        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n",
        "    if is_hypopnea_available:\n",
        "        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n",
        "    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n",
        "\n",
        "    starts = (epochs.events[:, 0] / sfreq).astype(int)\n",
        "\n",
        "    labels_apnea = []\n",
        "    labels_hypopnea = []\n",
        "    labels_not_awake = []\n",
        "    total_apnea_event_second = 0\n",
        "    total_hypopnea_event_second = 0\n",
        "\n",
        "    for seq in range(data.shape[0]):\n",
        "        epoch_set = set(range(starts[seq], starts[seq] + int(EPOCH_LENGTH)))\n",
        "        if is_apnea_available:\n",
        "            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n",
        "            total_apnea_event_second += apnea_seconds\n",
        "            labels_apnea.append(apnea_seconds)\n",
        "        else:\n",
        "            labels_apnea.append(0)\n",
        "\n",
        "        if is_hypopnea_available:\n",
        "            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n",
        "            total_hypopnea_event_second += hypopnea_seconds\n",
        "            labels_hypopnea.append(hypopnea_seconds)\n",
        "        else:\n",
        "            labels_hypopnea.append(0)\n",
        "\n",
        "        labels_not_awake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n",
        "    ####################################################################################################################\n",
        "    data = data[labels_not_awake, :, :]\n",
        "    labels_apnea = list(compress(labels_apnea, labels_not_awake))\n",
        "    labels_hypopnea = list(compress(labels_hypopnea, labels_not_awake))\n",
        "    ####################################################################################################################\n",
        "\n",
        "    new_data = np.zeros_like(data)\n",
        "    for i in range(data.shape[0]):\n",
        "\n",
        "        new_data[i, 0, :] = data[i, 0, :] - data[i, 7, :]  # E1 - M2\n",
        "        new_data[i, 1, :] = data[i, 1, :] - data[i, 6, :]  # E2 - M1\n",
        "\n",
        "        new_data[i, 2, :] = data[i, 2, :] - data[i, 7, :]  # F3 - M2\n",
        "        new_data[i, 3, :] = data[i, 3, :] - data[i, 6, :]  # F4 - M1\n",
        "        new_data[i, 4, :] = data[i, 4, :] - data[i, 7, :]  # C3 - M2\n",
        "        new_data[i, 5, :] = data[i, 5, :] - data[i, 6, :]  # C4 - M1\n",
        "        new_data[i, 6, :] = data[i, 8, :] - data[i, 7, :]  # O1 - M2\n",
        "        new_data[i, 7, :] = data[i, 9, :] - data[i, 6, :]  # O2 - M1\n",
        "\n",
        "        new_data[i, 8, :] = data[i, 10,:] - data[i, 11,:]  # ECG\n",
        "\n",
        "        new_data[i, 9, :] = data[i, 12, :]  # CANULAFLOW\n",
        "        new_data[i, 10, :] = data[i, 13, :]  # AIRFLOW\n",
        "        new_data[i, 11, :] = data[i, 14, :]  # CHEST\n",
        "        new_data[i, 12, :] = data[i, 15, :]  # ABD\n",
        "        new_data[i, 13, :] = data[i, 16, :]  # SAO2\n",
        "        new_data[i, 14, :] = data[i, 17, :]  # CAP\n",
        "    data = new_data[:, :15, :]\n",
        "    ####################################################################################################################\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_dir + '\\\\' + os.path.basename(path[0]) + \"_\" + str(total_apnea_event_second) + \"_\" + str(\n",
        "            total_hypopnea_event_second),\n",
        "        data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n",
        "\n",
        "    return data.shape[0]\n",
        "\n",
        "mne.set_log_file('log.txt', overwrite=False)\n",
        "edf_files = glob.glob(PSG_DIR + \"*.edf\")\n",
        "for edf_file in edf_files:\n",
        "    # For each EDF file, construct the corresponding annotation file path by replacing the extension with -nsrr.tsv.\n",
        "    annot_file = edf_file.replace(\".edf\", \"-nsrr.xml\")\n",
        "    preprocess((edf_file, annot_file), identity, OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHAT_Data_Loader\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import resample\n",
        "from scipy.interpolate import splev, splrep\n",
        "from __future__ import absolute_import, division, print_function\n",
        "from six.moves import map, range, zip\n",
        "import six\n",
        "import collections\n",
        "import copy\n",
        "import keyword\n",
        "import re\n",
        "import scipy.signal as ss\n",
        "from scipy import interpolate, optimize\n",
        "from scipy.stats import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "\n",
        "CHAT_SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
        "chat_s_count = len(CHAT_SIGS)\n",
        "\n",
        "CHAT_THRESHOLD = 3\n",
        "CHAT_PREPROCESSED_PATH = \"./data/chat/preprocessed\"\n",
        "CHAT_FREQ = 128\n",
        "CHAT_EPOCH_LENGTH = 30\n",
        "CHAT_ECG_SIG = 8\n",
        "CHAT_OUT_PATH = \"./data/chat/dataloading/\"\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "biosppy.utils\n",
        "-------------\n",
        "\n",
        "This module provides several frequently used functions and hacks.\n",
        "\n",
        ":copyright: (c) 2015-2018 by Instituto de Telecomunicacoes\n",
        ":license: BSD 3-clause, see LICENSE for more details.\n",
        "\"\"\"\n",
        "\n",
        "class Utils:\n",
        "    @staticmethod\n",
        "    def normpath(path):\n",
        "        \"\"\"Normalize a path.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str\n",
        "            The path to normalize.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        npath : str\n",
        "            The normalized path.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if '~' in path:\n",
        "            out = os.path.abspath(os.path.expanduser(path))\n",
        "        else:\n",
        "            out = os.path.abspath(path)\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def fileparts(path):\n",
        "        \"\"\"split a file path into its directory, name, and extension.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str\n",
        "            Input file path.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dirname : str\n",
        "            File directory.\n",
        "        fname : str\n",
        "            File name.\n",
        "        ext : str\n",
        "            File extension.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Removes the dot ('.') from the extension.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        dirname, fname = os.path.split(path)\n",
        "        fname, ext = os.path.splitext(fname)\n",
        "        ext = ext.replace('.', '')\n",
        "\n",
        "        return dirname, fname, ext\n",
        "\n",
        "    @staticmethod\n",
        "    def fullfile(*args):\n",
        "        \"\"\"Join one or more file path components, assuming the last is\n",
        "        the extension.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ``*args`` : list, optional\n",
        "            Components to concatenate.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        fpath : str\n",
        "            The concatenated file path.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        nb = len(args)\n",
        "        if nb == 0:\n",
        "            return ''\n",
        "        elif nb == 1:\n",
        "            return args[0]\n",
        "        elif nb == 2:\n",
        "            return os.path.join(*args)\n",
        "\n",
        "        fpath = os.path.join(*args[:-1]) + '.' + args[-1]\n",
        "\n",
        "        return fpath\n",
        "\n",
        "    @staticmethod\n",
        "    def walktree(top=None, spec=None):\n",
        "        \"\"\"Iterator to recursively descend a directory and return all files\n",
        "        matching the spec.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        top : str, optional\n",
        "            Starting directory; if None, defaults to the current working directoty.\n",
        "        spec : str, optional\n",
        "            Regular expression to match the desired files;\n",
        "            if None, matches all files; typical patterns:\n",
        "                * `r'\\.txt$'` - matches files with '.txt' extension;\n",
        "                * `r'^File_'` - matches files starting with 'File\\_'\n",
        "                * `r'^File_.+\\.txt$'` - matches files starting with 'File\\_' and ending with the '.txt' extension.\n",
        "\n",
        "        Yields\n",
        "        ------\n",
        "        fpath : str\n",
        "            Absolute file path.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Partial matches are also selected.\n",
        "\n",
        "        See Also\n",
        "        --------\n",
        "        * https://docs.python.org/3/library/re.html\n",
        "        * https://regex101.com/\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if top is None:\n",
        "            top = os.getcwd()\n",
        "\n",
        "        if spec is None:\n",
        "            spec = r'.*?'\n",
        "\n",
        "        prog = re.compile(spec)\n",
        "\n",
        "        for root, _, files in os.walk(top):\n",
        "            for name in files:\n",
        "                if prog.search(name):\n",
        "                    fname = os.path.join(root, name)\n",
        "                    yield fname\n",
        "\n",
        "    @staticmethod\n",
        "    def remainderAllocator(votes, k, reverse=True, check=False):\n",
        "        \"\"\"Allocate k seats proportionally using the Remainder Method.\n",
        "\n",
        "        Also known as Hare-Niemeyer Method. Uses the Hare quota.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        votes : list\n",
        "            Number of votes for each class/party/cardinal.\n",
        "        k : int\n",
        "            Total number o seats to allocate.\n",
        "        reverse : bool, optional\n",
        "            If True, allocates remaining seats largest quota first.\n",
        "        check : bool, optional\n",
        "            If True, limits the number of seats to the total number of votes.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        seats : list\n",
        "            Number of seats for each class/party/cardinal.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check total number of votes\n",
        "        tot = np.sum(votes)\n",
        "        if check and k > tot:\n",
        "            k = tot\n",
        "\n",
        "        # frequencies\n",
        "        length = len(votes)\n",
        "        freqs = np.array(votes, dtype='float') / tot\n",
        "\n",
        "        # assign items\n",
        "        aux = k * freqs\n",
        "        seats = aux.astype('int')\n",
        "\n",
        "        # leftovers\n",
        "        nb = k - seats.sum()\n",
        "        if nb > 0:\n",
        "            if reverse:\n",
        "                ind = np.argsort(aux - seats)[::-1]\n",
        "            else:\n",
        "                ind = np.argsort(aux - seats)\n",
        "\n",
        "            for i in range(nb):\n",
        "                seats[ind[i % length]] += 1\n",
        "\n",
        "        return seats.tolist()\n",
        "\n",
        "    @staticmethod\n",
        "    def highestAveragesAllocator(votes, k, divisor='dHondt', check=False):\n",
        "        \"\"\"Allocate k seats proportionally using the Highest Averages Method.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        votes : list\n",
        "            Number of votes for each class/party/cardinal.\n",
        "        k : int\n",
        "            Total number o seats to allocate.\n",
        "        divisor : str, optional\n",
        "            Divisor method; one of 'dHondt', 'Huntington-Hill', 'Sainte-Lague',\n",
        "            'Imperiali', or 'Danish'.\n",
        "        check : bool, optional\n",
        "            If True, limits the number of seats to the total number of votes.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        seats : list\n",
        "            Number of seats for each class/party/cardinal.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check total number of cardinals\n",
        "        tot = np.sum(votes)\n",
        "        if check and k > tot:\n",
        "            k = tot\n",
        "\n",
        "        # select divisor\n",
        "        if divisor == 'dHondt':\n",
        "            fcn = lambda i: float(i)\n",
        "        elif divisor == 'Huntington-Hill':\n",
        "            fcn = lambda i: np.sqrt(i * (i + 1.))\n",
        "        elif divisor == 'Sainte-Lague':\n",
        "            fcn = lambda i: i - 0.5\n",
        "        elif divisor == 'Imperiali':\n",
        "            fcn = lambda i: float(i + 1)\n",
        "        elif divisor == 'Danish':\n",
        "            fcn = lambda i: 3. * (i - 1.) + 1.\n",
        "        else:\n",
        "            raise ValueError(\"Unknown divisor method.\")\n",
        "\n",
        "        # compute coefficients\n",
        "        tab = []\n",
        "        length = len(votes)\n",
        "        D = [fcn(i) for i in range(1, k + 1)]\n",
        "        for i in range(length):\n",
        "            for j in range(k):\n",
        "                tab.append((i, votes[i] / D[j]))\n",
        "\n",
        "        # sort\n",
        "        tab.sort(key=lambda item: item[1], reverse=True)\n",
        "        tab = tab[:k]\n",
        "        tab = np.array([item[0] for item in tab], dtype='int')\n",
        "\n",
        "        seats = np.zeros(length, dtype='int')\n",
        "        for i in range(length):\n",
        "            seats[i] = np.sum(tab == i)\n",
        "\n",
        "        return seats.tolist()\n",
        "\n",
        "    @staticmethod\n",
        "    def random_fraction(indx, fraction, sort=True):\n",
        "        \"\"\"Select a random fraction of an input list of elements.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indx : list, array\n",
        "            Elements to partition.\n",
        "        fraction : int, float\n",
        "            Fraction to select.\n",
        "        sort : bool, optional\n",
        "            If True, output lists will be sorted.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        use : list, array\n",
        "            Selected elements.\n",
        "        unuse : list, array\n",
        "            Remaining elements.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # number of elements to use\n",
        "        fraction = float(fraction)\n",
        "        nb = int(fraction * len(indx))\n",
        "\n",
        "        # copy because shuffle works in place\n",
        "        aux = copy.deepcopy(indx)\n",
        "\n",
        "        # shuffle\n",
        "        np.random.shuffle(indx)\n",
        "\n",
        "        # select\n",
        "        use = aux[:nb]\n",
        "        unuse = aux[nb:]\n",
        "\n",
        "        # sort\n",
        "        if sort:\n",
        "            use.sort()\n",
        "            unuse.sort()\n",
        "\n",
        "        return use, unuse\n",
        "\n",
        "\n",
        "    class ReturnTuple(tuple):\n",
        "        \"\"\"A named tuple to use as a hybrid tuple-dict return object.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        values : iterable\n",
        "            Return values.\n",
        "        names : iterable, optional\n",
        "            Names for return values.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the number of values differs from the number of names.\n",
        "        ValueError\n",
        "            If any of the items in names:\n",
        "            * contain non-alphanumeric characters;\n",
        "            * are Python keywords;\n",
        "            * start with a number;\n",
        "            * are duplicates.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        def __new__(cls, values, names=None):\n",
        "\n",
        "            return tuple.__new__(cls, tuple(values))\n",
        "\n",
        "        def __init__(self, values, names=None):\n",
        "\n",
        "            nargs = len(values)\n",
        "\n",
        "            if names is None:\n",
        "                # create names\n",
        "                names = ['_%d' % i for i in range(nargs)]\n",
        "            else:\n",
        "                # check length\n",
        "                if len(names) != nargs:\n",
        "                    raise ValueError(\"Number of names and values mismatch.\")\n",
        "\n",
        "                # convert to str\n",
        "                names = list(map(str, names))\n",
        "\n",
        "                # check for keywords, alphanumeric, digits, repeats\n",
        "                seen = set()\n",
        "                for name in names:\n",
        "                    if not all(c.isalnum() or (c == '_') for c in name):\n",
        "                        raise ValueError(\"Names can only contain alphanumeric \\\n",
        "                                          characters and underscores: %r.\" % name)\n",
        "\n",
        "                    if keyword.iskeyword(name):\n",
        "                        raise ValueError(\"Names cannot be a keyword: %r.\" % name)\n",
        "\n",
        "                    if name[0].isdigit():\n",
        "                        raise ValueError(\"Names cannot start with a number: %r.\" %\n",
        "                                         name)\n",
        "\n",
        "                    if name in seen:\n",
        "                        raise ValueError(\"Encountered duplicate name: %r.\" % name)\n",
        "\n",
        "                    seen.add(name)\n",
        "\n",
        "            self._names = names\n",
        "\n",
        "        def as_dict(self):\n",
        "            \"\"\"Convert to an ordered dictionary.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            out : OrderedDict\n",
        "                An OrderedDict representing the return values.\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            return collections.OrderedDict(zip(self._names, self))\n",
        "\n",
        "        __dict__ = property(as_dict)\n",
        "\n",
        "        def __getitem__(self, key):\n",
        "            \"\"\"Get item as an index or keyword.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            out : object\n",
        "                The object corresponding to the key, if it exists.\n",
        "\n",
        "            Raises\n",
        "            ------\n",
        "            KeyError\n",
        "                If the key is a string and it does not exist in the mapping.\n",
        "            IndexError\n",
        "                If the key is an int and it is out of range.\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            if isinstance(key, six.string_types):\n",
        "                if key not in self._names:\n",
        "                    raise KeyError(\"Unknown key: %r.\" % key)\n",
        "\n",
        "                key = self._names.index(key)\n",
        "\n",
        "            return super(ReturnTuple, self).__getitem__(key)\n",
        "\n",
        "        def __repr__(self):\n",
        "            \"\"\"Return representation string.\"\"\"\n",
        "\n",
        "            tpl = '%s=%r'\n",
        "\n",
        "            rp = ', '.join(tpl % item for item in zip(self._names, self))\n",
        "\n",
        "            return 'ReturnTuple(%s)' % rp\n",
        "\n",
        "        def __getnewargs__(self):\n",
        "            \"\"\"Return self as a plain tuple; used for copy and pickle.\"\"\"\n",
        "\n",
        "            return tuple(self)\n",
        "\n",
        "        def keys(self):\n",
        "            \"\"\"Return the value names.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            out : list\n",
        "                The keys in the mapping.\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            return list(self._names)\n",
        "\n",
        "utils = Utils\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "biosppy.signals.tools\n",
        "---------------------\n",
        "\n",
        "This module provides various signal analysis methods in the time and\n",
        "frequency domains.\n",
        "\n",
        ":copyright: (c) 2015-2018 by Instituto de Telecomunicacoes\n",
        ":license: BSD 3-clause, see LICENSE for more details.\n",
        "\"\"\"\n",
        "class Tools:\n",
        "    @staticmethod\n",
        "    def _norm_freq(frequency=None, sampling_rate=1000.):\n",
        "        \"\"\"Normalize frequency to Nyquist Frequency (Fs/2).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        frequency : int, float, list, array\n",
        "            Frequencies to normalize.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        wn : float, array\n",
        "            Normalized frequencies.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if frequency is None:\n",
        "            raise TypeError(\"Please specify a frequency to normalize.\")\n",
        "\n",
        "        # convert inputs to correct representation\n",
        "        try:\n",
        "            frequency = float(frequency)\n",
        "        except TypeError:\n",
        "            # maybe frequency is a list or array\n",
        "            frequency = np.array(frequency, dtype='float')\n",
        "\n",
        "        Fs = float(sampling_rate)\n",
        "\n",
        "        wn = 2. * frequency / Fs\n",
        "\n",
        "        return wn\n",
        "\n",
        "    @staticmethod\n",
        "    def _filter_init(b, a, alpha=1.):\n",
        "        \"\"\"Get an initial filter state that corresponds to the steady-state\n",
        "        of the step response.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        b : array\n",
        "            Numerator coefficients.\n",
        "        a : array\n",
        "            Denominator coefficients.\n",
        "        alpha : float, optional\n",
        "            Scaling factor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        zi : array\n",
        "            Initial filter state.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        zi = alpha * ss.lfilter_zi(b, a)\n",
        "\n",
        "        return zi\n",
        "\n",
        "    @staticmethod\n",
        "    def _filter_signal(b, a, signal, zi=None, check_phase=True, **kwargs):\n",
        "        \"\"\"Filter a signal with given coefficients.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        b : array\n",
        "            Numerator coefficients.\n",
        "        a : array\n",
        "            Denominator coefficients.\n",
        "        signal : array\n",
        "            Signal to filter.\n",
        "        zi : array, optional\n",
        "            Initial filter state.\n",
        "        check_phase : bool, optional\n",
        "            If True, use the forward-backward technique.\n",
        "        ``**kwargs`` : dict, optional\n",
        "            Additional keyword arguments are passed to the underlying filtering\n",
        "            function.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        filtered : array\n",
        "            Filtered signal.\n",
        "        zf : array\n",
        "            Final filter state.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * If check_phase is True, zi cannot be set.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if check_phase and zi is not None:\n",
        "            raise ValueError(\n",
        "                \"Incompatible arguments: initial filter state cannot be set when \\\n",
        "                check_phase is True.\")\n",
        "\n",
        "        if zi is None:\n",
        "            zf = None\n",
        "            if check_phase:\n",
        "                filtered = ss.filtfilt(b, a, signal, **kwargs)\n",
        "            else:\n",
        "                filtered = ss.lfilter(b, a, signal, **kwargs)\n",
        "        else:\n",
        "            filtered, zf = ss.lfilter(b, a, signal, zi=zi, **kwargs)\n",
        "\n",
        "        return filtered, zf\n",
        "\n",
        "    @staticmethod\n",
        "    def _filter_resp(b, a, sampling_rate=1000., nfreqs=4096):\n",
        "        \"\"\"Compute the filter frequency response.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        b : array\n",
        "            Numerator coefficients.\n",
        "        a : array\n",
        "            Denominator coefficients.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        nfreqs : int, optional\n",
        "            Number of frequency points to compute.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        freqs : array\n",
        "            Array of frequencies (Hz) at which the response was computed.\n",
        "        resp : array\n",
        "            Frequency response.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        w, resp = ss.freqz(b, a, nfreqs)\n",
        "\n",
        "        # convert frequencies\n",
        "        freqs = w * sampling_rate / (2. * np.pi)\n",
        "\n",
        "        return freqs, resp\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_window(kernel, size, **kwargs):\n",
        "        \"\"\"Return a window with the specified parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        kernel : str\n",
        "            Type of window to create.\n",
        "        size : int\n",
        "            Size of the window.\n",
        "        ``**kwargs`` : dict, optional\n",
        "            Additional keyword arguments are passed to the underlying\n",
        "            scipy.signal.windows function.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        window : array\n",
        "            Created window.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # mimics scipy.signal.get_window\n",
        "        if kernel in ['blackman', 'black', 'blk']:\n",
        "            winfunc = ss.blackman\n",
        "        elif kernel in ['triangle', 'triang', 'tri']:\n",
        "            winfunc = ss.triang\n",
        "        elif kernel in ['hamming', 'hamm', 'ham']:\n",
        "            winfunc = ss.windows.hamming\n",
        "        elif kernel in ['bartlett', 'bart', 'brt']:\n",
        "            winfunc = ss.bartlett\n",
        "        elif kernel in ['hanning', 'hann', 'han']:\n",
        "            winfunc = ss.hann\n",
        "        elif kernel in ['blackmanharris', 'blackharr', 'bkh']:\n",
        "            winfunc = ss.blackmanharris\n",
        "        elif kernel in ['parzen', 'parz', 'par']:\n",
        "            winfunc = ss.parzen\n",
        "        elif kernel in ['bohman', 'bman', 'bmn']:\n",
        "            winfunc = ss.bohman\n",
        "        elif kernel in ['nuttall', 'nutl', 'nut']:\n",
        "            winfunc = ss.nuttall\n",
        "        elif kernel in ['barthann', 'brthan', 'bth']:\n",
        "            winfunc = ss.barthann\n",
        "        elif kernel in ['flattop', 'flat', 'flt']:\n",
        "            winfunc = ss.flattop\n",
        "        elif kernel in ['kaiser', 'ksr']:\n",
        "            winfunc = ss.kaiser\n",
        "        elif kernel in ['gaussian', 'gauss', 'gss']:\n",
        "            winfunc = ss.gaussian\n",
        "        elif kernel in ['general gaussian', 'general_gaussian', 'general gauss',\n",
        "                        'general_gauss', 'ggs']:\n",
        "            winfunc = ss.general_gaussian\n",
        "        elif kernel in ['boxcar', 'box', 'ones', 'rect', 'rectangular']:\n",
        "            winfunc = ss.boxcar\n",
        "        elif kernel in ['slepian', 'slep', 'optimal', 'dpss', 'dss']:\n",
        "            winfunc = ss.slepian\n",
        "        elif kernel in ['cosine', 'halfcosine']:\n",
        "            winfunc = ss.cosine\n",
        "        elif kernel in ['chebwin', 'cheb']:\n",
        "            winfunc = ss.chebwin\n",
        "        else:\n",
        "            raise ValueError(\"Unknown window type.\")\n",
        "\n",
        "        try:\n",
        "            window = winfunc(size, **kwargs)\n",
        "        except TypeError as e:\n",
        "            raise TypeError(\"Invalid window arguments: %s.\" % e)\n",
        "\n",
        "        return window\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filter(ftype='FIR',\n",
        "                   band='lowpass',\n",
        "                   order=None,\n",
        "                   frequency=None,\n",
        "                   sampling_rate=1000., **kwargs):\n",
        "        \"\"\"Compute digital (FIR or IIR) filter coefficients with the given\n",
        "        parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ftype : str\n",
        "            Filter type:\n",
        "                * Finite Impulse Response filter ('FIR');\n",
        "                * Butterworth filter ('butter');\n",
        "                * Chebyshev filters ('cheby1', 'cheby2');\n",
        "                * Elliptic filter ('ellip');\n",
        "                * Bessel filter ('bessel').\n",
        "        band : str\n",
        "            Band type:\n",
        "                * Low-pass filter ('lowpass');\n",
        "                * High-pass filter ('highpass');\n",
        "                * Band-pass filter ('bandpass');\n",
        "                * Band-stop filter ('bandstop').\n",
        "        order : int\n",
        "            Order of the filter.\n",
        "        frequency : int, float, list, array\n",
        "            Cutoff frequencies; format depends on type of band:\n",
        "                * 'lowpass' or 'bandpass': single frequency;\n",
        "                * 'bandpass' or 'bandstop': pair of frequencies.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        ``**kwargs`` : dict, optional\n",
        "            Additional keyword arguments are passed to the underlying\n",
        "            scipy.signal function.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        b : array\n",
        "            Numerator coefficients.\n",
        "        a : array\n",
        "            Denominator coefficients.\n",
        "\n",
        "        See Also:\n",
        "            scipy.signal\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if order is None:\n",
        "            raise TypeError(\"Please specify the filter order.\")\n",
        "        if frequency is None:\n",
        "            raise TypeError(\"Please specify the cutoff frequency.\")\n",
        "        if band not in ['lowpass', 'highpass', 'bandpass', 'bandstop']:\n",
        "            raise ValueError(\n",
        "                \"Unknown filter type '%r'; choose 'lowpass', 'highpass', \\\n",
        "                'bandpass', or 'bandstop'.\"\n",
        "                % band)\n",
        "\n",
        "        # convert frequencies\n",
        "        frequency = st._norm_freq(frequency, sampling_rate)\n",
        "\n",
        "        # get coeffs\n",
        "        b, a = [], []\n",
        "        if ftype == 'FIR':\n",
        "            # FIR filter\n",
        "            if order % 2 == 0:\n",
        "                order += 1\n",
        "            a = np.array([1])\n",
        "            if band in ['lowpass', 'bandstop']:\n",
        "                b = ss.firwin(numtaps=order,\n",
        "                              cutoff=frequency,\n",
        "                              pass_zero=True, **kwargs)\n",
        "            elif band in ['highpass', 'bandpass']:\n",
        "                b = ss.firwin(numtaps=order,\n",
        "                              cutoff=frequency,\n",
        "                              pass_zero=False, **kwargs)\n",
        "        elif ftype == 'butter':\n",
        "            # Butterworth filter\n",
        "            b, a = ss.butter(N=order,\n",
        "                             Wn=frequency,\n",
        "                             btype=band,\n",
        "                             analog=False,\n",
        "                             output='ba', **kwargs)\n",
        "        elif ftype == 'cheby1':\n",
        "            # Chebyshev type I filter\n",
        "            b, a = ss.cheby1(N=order,\n",
        "                             Wn=frequency,\n",
        "                             btype=band,\n",
        "                             analog=False,\n",
        "                             output='ba', **kwargs)\n",
        "        elif ftype == 'cheby2':\n",
        "            # chevyshev type II filter\n",
        "            b, a = ss.cheby2(N=order,\n",
        "                             Wn=frequency,\n",
        "                             btype=band,\n",
        "                             analog=False,\n",
        "                             output='ba', **kwargs)\n",
        "        elif ftype == 'ellip':\n",
        "            # Elliptic filter\n",
        "            b, a = ss.ellip(N=order,\n",
        "                            Wn=frequency,\n",
        "                            btype=band,\n",
        "                            analog=False,\n",
        "                            output='ba', **kwargs)\n",
        "        elif ftype == 'bessel':\n",
        "            # Bessel filter\n",
        "            b, a = ss.bessel(N=order,\n",
        "                             Wn=frequency,\n",
        "                             btype=band,\n",
        "                             analog=False,\n",
        "                             output='ba', **kwargs)\n",
        "\n",
        "        return utils.ReturnTuple((b, a), ('b', 'a'))\n",
        "\n",
        "    @staticmethod\n",
        "    def filter_signal(signal=None,\n",
        "                      ftype='FIR',\n",
        "                      band='lowpass',\n",
        "                      order=None,\n",
        "                      frequency=None,\n",
        "                      sampling_rate=1000., **kwargs):\n",
        "        \"\"\"Filter a signal according to the given parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Signal to filter.\n",
        "        ftype : str\n",
        "            Filter type:\n",
        "                * Finite Impulse Response filter ('FIR');\n",
        "                * Butterworth filter ('butter');\n",
        "                * Chebyshev filters ('cheby1', 'cheby2');\n",
        "                * Elliptic filter ('ellip');\n",
        "                * Bessel filter ('bessel').\n",
        "        band : str\n",
        "            Band type:\n",
        "                * Low-pass filter ('lowpass');\n",
        "                * High-pass filter ('highpass');\n",
        "                * Band-pass filter ('bandpass');\n",
        "                * Band-stop filter ('bandstop').\n",
        "        order : int\n",
        "            Order of the filter.\n",
        "        frequency : int, float, list, array\n",
        "            Cutoff frequencies; format depends on type of band:\n",
        "                * 'lowpass' or 'bandpass': single frequency;\n",
        "                * 'bandpass' or 'bandstop': pair of frequencies.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        ``**kwargs`` : dict, optional\n",
        "            Additional keyword arguments are passed to the underlying\n",
        "            scipy.signal function.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        signal : array\n",
        "            Filtered signal.\n",
        "        sampling_rate : float\n",
        "            Sampling frequency (Hz).\n",
        "        params : dict\n",
        "            Filter parameters.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Uses a forward-backward filter implementation. Therefore, the combined\n",
        "          filter has linear phase.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify a signal to filter.\")\n",
        "\n",
        "        # get filter\n",
        "        b, a = st.get_filter(ftype=ftype,\n",
        "                          order=order,\n",
        "                          frequency=frequency,\n",
        "                          sampling_rate=sampling_rate,\n",
        "                          band=band, **kwargs)\n",
        "\n",
        "        # filter\n",
        "        filtered, _ = st._filter_signal(b, a, signal, check_phase=True)\n",
        "\n",
        "        # output\n",
        "        params = {\n",
        "            'ftype': ftype,\n",
        "            'order': order,\n",
        "            'frequency': frequency,\n",
        "            'band': band,\n",
        "        }\n",
        "        params.update(kwargs)\n",
        "\n",
        "        args = (filtered, sampling_rate, params)\n",
        "        names = ('signal', 'sampling_rate', 'params')\n",
        "\n",
        "        return utils.ReturnTuple(args, names)\n",
        "\n",
        "    class OnlineFilter(object):\n",
        "        \"\"\"Online filtering.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        b : array\n",
        "            Numerator coefficients.\n",
        "        a : array\n",
        "            Denominator coefficients.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, b=None, a=None):\n",
        "            # check inputs\n",
        "            if b is None:\n",
        "                raise TypeError('Please specify the numerator coefficients.')\n",
        "\n",
        "            if a is None:\n",
        "                raise TypeError('Please specify the denominator coefficients.')\n",
        "\n",
        "            # self things\n",
        "            self.b = b\n",
        "            self.a = a\n",
        "\n",
        "            # reset\n",
        "            self.reset()\n",
        "\n",
        "        def reset(self):\n",
        "            \"\"\"Reset the filter state.\"\"\"\n",
        "\n",
        "            self.zi = ss.lfilter_zi(self.b, self.a)\n",
        "\n",
        "        def filter(self, signal=None):\n",
        "            \"\"\"Filter a signal segment.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            signal : array\n",
        "                Signal segment to filter.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            filtered : array\n",
        "                Filtered signal segment.\n",
        "\n",
        "            \"\"\"\n",
        "\n",
        "            # check input\n",
        "            if signal is None:\n",
        "                raise TypeError('Please specify the input signal.')\n",
        "\n",
        "            filtered, zf = ss.lfilter(self.b, self.a, signal, zi=self.zi)\n",
        "            self.zi = zf\n",
        "\n",
        "            return utils.ReturnTuple((filtered, ), ('filtered', ))\n",
        "\n",
        "    @staticmethod\n",
        "    def smoother(signal=None, kernel='boxzen', size=10, mirror=True, **kwargs):\n",
        "        \"\"\"Smooth a signal using an N-point moving average [MAvg]_ filter.\n",
        "\n",
        "        This implementation uses the convolution of a filter kernel with the input\n",
        "        signal to compute the smoothed signal [Smit97]_.\n",
        "\n",
        "        Availabel kernels: median, boxzen, boxcar, triang, blackman, hamming, hann,\n",
        "        bartlett, flattop, parzen, bohman, blackmanharris, nuttall, barthann,\n",
        "        kaiser (needs beta), gaussian (needs std), general_gaussian (needs power,\n",
        "        width), slepian (needs width), chebwin (needs attenuation).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Signal to smooth.\n",
        "        kernel : str, array, optional\n",
        "            Type of kernel to use; if array, use directly as the kernel.\n",
        "        size : int, optional\n",
        "            Size of the kernel; ignored if kernel is an array.\n",
        "        mirror : bool, optional\n",
        "            If True, signal edges are extended to avoid boundary effects.\n",
        "        ``**kwargs`` : dict, optional\n",
        "            Additional keyword arguments are passed to the underlying\n",
        "            scipy.signal.windows function.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        signal : array\n",
        "            Smoothed signal.\n",
        "        params : dict\n",
        "            Smoother parameters.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * When the kernel is 'median', mirror is ignored.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [MAvg] Wikipedia, \"Moving Average\",\n",
        "           http://en.wikipedia.org/wiki/Moving_average\n",
        "        .. [Smit97] S. W. Smith, \"Moving Average Filters - Implementation by\n",
        "           Convolution\", http://www.dspguide.com/ch15/1.htm, 1997\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify a signal to smooth.\")\n",
        "\n",
        "        length = len(signal)\n",
        "\n",
        "        if isinstance(kernel, six.string_types):\n",
        "            # check length\n",
        "            if size > length:\n",
        "                size = length - 1\n",
        "\n",
        "            if size < 1:\n",
        "                size = 1\n",
        "\n",
        "            if kernel == 'boxzen':\n",
        "                # hybrid method\n",
        "                # 1st pass - boxcar kernel\n",
        "                aux, _ = smoother(signal,\n",
        "                                  kernel='boxcar',\n",
        "                                  size=size,\n",
        "                                  mirror=mirror)\n",
        "\n",
        "                # 2nd pass - parzen kernel\n",
        "                smoothed, _ = smoother(aux,\n",
        "                                       kernel='parzen',\n",
        "                                       size=size,\n",
        "                                       mirror=mirror)\n",
        "\n",
        "                params = {'kernel': kernel, 'size': size, 'mirror': mirror}\n",
        "\n",
        "                args = (smoothed, params)\n",
        "                names = ('signal', 'params')\n",
        "\n",
        "                return utils.ReturnTuple(args, names)\n",
        "\n",
        "            elif kernel == 'median':\n",
        "                # median filter\n",
        "                if size % 2 == 0:\n",
        "                    raise ValueError(\n",
        "                        \"When the kernel is 'median', size must be odd.\")\n",
        "\n",
        "                smoothed = ss.medfilt(signal, kernel_size=size)\n",
        "\n",
        "                params = {'kernel': kernel, 'size': size, 'mirror': mirror}\n",
        "\n",
        "                args = (smoothed, params)\n",
        "                names = ('signal', 'params')\n",
        "\n",
        "                return utils.ReturnTuple(args, names)\n",
        "\n",
        "            else:\n",
        "                win = st._get_window(kernel, size, **kwargs)\n",
        "\n",
        "        elif isinstance(kernel, np.ndarray):\n",
        "            win = kernel\n",
        "            size = len(win)\n",
        "\n",
        "            # check length\n",
        "            if size > length:\n",
        "                raise ValueError(\"Kernel size is bigger than signal length.\")\n",
        "\n",
        "            if size < 1:\n",
        "                raise ValueError(\"Kernel size is smaller than 1.\")\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Unknown kernel type.\")\n",
        "\n",
        "        # convolve\n",
        "        w = win / win.sum()\n",
        "        if mirror:\n",
        "            aux = np.concatenate(\n",
        "                (signal[0] * np.ones(size), signal, signal[-1] * np.ones(size)))\n",
        "            smoothed = np.convolve(w, aux, mode='same')\n",
        "            smoothed = smoothed[size:-size]\n",
        "        else:\n",
        "            smoothed = np.convolve(w, signal, mode='same')\n",
        "\n",
        "        # output\n",
        "        params = {'kernel': kernel, 'size': size, 'mirror': mirror}\n",
        "        params.update(kwargs)\n",
        "\n",
        "        args = (smoothed, params)\n",
        "        names = ('signal', 'params')\n",
        "\n",
        "        return utils.ReturnTuple(args, names)\n",
        "\n",
        "    @staticmethod\n",
        "    def analytic_signal(signal=None, N=None):\n",
        "        \"\"\"Compute analytic signal, using the Hilbert Transform.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        N : int, optional\n",
        "            Number of Fourier components; default is `len(signal)`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        amplitude : array\n",
        "            Amplitude envelope of the analytic signal.\n",
        "        phase : array\n",
        "            Instantaneous phase component of the analystic signal.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        # hilbert transform\n",
        "        asig = ss.hilbert(signal, N=N)\n",
        "\n",
        "        # amplitude envelope\n",
        "        amp = np.absolute(asig)\n",
        "\n",
        "        # instantaneous\n",
        "        phase = np.angle(asig)\n",
        "\n",
        "        return utils.ReturnTuple((amp, phase), ('amplitude', 'phase'))\n",
        "\n",
        "    @staticmethod\n",
        "    def phase_locking(signal1=None, signal2=None, N=None):\n",
        "        \"\"\"Compute the Phase-Locking Factor (PLF) between two signals.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal1 : array\n",
        "            First input signal.\n",
        "        signal2 : array\n",
        "            Second input signal.\n",
        "        N : int, optional\n",
        "            Number of Fourier components.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        plf : float\n",
        "            The PLF between the two signals.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal1 is None:\n",
        "            raise TypeError(\"Please specify the first input signal.\")\n",
        "\n",
        "        if signal2 is None:\n",
        "            raise TypeError(\"Please specify the second input signal.\")\n",
        "\n",
        "        if len(signal1) != len(signal2):\n",
        "            raise ValueError(\"The input signals must have the same length.\")\n",
        "\n",
        "        # compute analytic signal\n",
        "        asig1 = ss.hilbert(signal1, N=N)\n",
        "        phase1 = np.angle(asig1)\n",
        "\n",
        "        asig2 = ss.hilbert(signal2, N=N)\n",
        "        phase2 = np.angle(asig2)\n",
        "\n",
        "        # compute PLF\n",
        "        plf = np.absolute(np.mean(np.exp(1j * (phase1 - phase2))))\n",
        "\n",
        "        return utils.ReturnTuple((plf,), ('plf',))\n",
        "\n",
        "    @staticmethod\n",
        "    def power_spectrum(signal=None,\n",
        "                       sampling_rate=1000.,\n",
        "                       pad=None,\n",
        "                       pow2=False,\n",
        "                       decibel=True):\n",
        "        \"\"\"Compute the power spectrum of a signal (one-sided).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        pad : int, optional\n",
        "            Padding for the Fourier Transform (number of zeros added);\n",
        "            defaults to no padding..\n",
        "        pow2 : bool, optional\n",
        "            If True, rounds the number of points `N = len(signal) + pad` to the\n",
        "            nearest power of 2 greater than N.\n",
        "        decibel : bool, optional\n",
        "            If True, returns the power in decibels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        freqs : array\n",
        "            Array of frequencies (Hz) at which the power was computed.\n",
        "        power : array\n",
        "            Power spectrum.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        npoints = len(signal)\n",
        "\n",
        "        if pad is not None:\n",
        "            if pad >= 0:\n",
        "                npoints += pad\n",
        "            else:\n",
        "                raise ValueError(\"Padding must be a positive integer.\")\n",
        "\n",
        "        # power of 2\n",
        "        if pow2:\n",
        "            npoints = 2 ** (np.ceil(np.log2(npoints)))\n",
        "\n",
        "        Nyq = float(sampling_rate) / 2\n",
        "        hpoints = npoints // 2\n",
        "\n",
        "        freqs = np.linspace(0, Nyq, hpoints)\n",
        "        power = np.abs(np.fft.fft(signal, npoints)) / npoints\n",
        "\n",
        "        # one-sided\n",
        "        power = power[:hpoints]\n",
        "        power[1:] *= 2\n",
        "        power = np.power(power, 2)\n",
        "\n",
        "        if decibel:\n",
        "            power = 10. * np.log10(power)\n",
        "\n",
        "        return utils.ReturnTuple((freqs, power), ('freqs', 'power'))\n",
        "\n",
        "    @staticmethod\n",
        "    def welch_spectrum(signal=None,\n",
        "                       sampling_rate=1000.,\n",
        "                       size=None,\n",
        "                       overlap=None,\n",
        "                       window='hanning',\n",
        "                       window_kwargs=None,\n",
        "                       pad=None,\n",
        "                       decibel=True):\n",
        "        \"\"\"Compute the power spectrum of a signal using Welch's method (one-sided).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        size : int, optional\n",
        "            Number of points in each Welch segment;\n",
        "            defaults to the equivalent of 1 second;\n",
        "            ignored when 'window' is an array.\n",
        "        overlap : int, optional\n",
        "            Number of points to overlap between segments; defaults to `size / 2`.\n",
        "        window : str, array, optional\n",
        "            Type of window to use.\n",
        "        window_kwargs : dict, optional\n",
        "            Additional keyword arguments to pass on window creation; ignored if\n",
        "            'window' is an array.\n",
        "        pad : int, optional\n",
        "            Padding for the Fourier Transform (number of zeros added);\n",
        "            defaults to no padding.\n",
        "        decibel : bool, optional\n",
        "            If True, returns the power in decibels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        freqs : array\n",
        "            Array of frequencies (Hz) at which the power was computed.\n",
        "        power : array\n",
        "            Power spectrum.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Detrends each Welch segment by removing the mean.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        length = len(signal)\n",
        "        sampling_rate = float(sampling_rate)\n",
        "\n",
        "        if size is None:\n",
        "            size = int(sampling_rate)\n",
        "\n",
        "        if window_kwargs is None:\n",
        "            window_kwargs = {}\n",
        "\n",
        "        if isinstance(window, six.string_types):\n",
        "            win = _get_window(window, size, **window_kwargs)\n",
        "        elif isinstance(window, np.ndarray):\n",
        "            win = window\n",
        "            size = len(win)\n",
        "\n",
        "        if size > length:\n",
        "            raise ValueError('Segment size must be smaller than signal length.')\n",
        "\n",
        "        if overlap is None:\n",
        "            overlap = size // 2\n",
        "        elif overlap > size:\n",
        "            raise ValueError('Overlap must be smaller than segment size.')\n",
        "\n",
        "        nfft = size\n",
        "        if pad is not None:\n",
        "            if pad >= 0:\n",
        "                nfft += pad\n",
        "            else:\n",
        "                raise ValueError(\"Padding must be a positive integer.\")\n",
        "\n",
        "        freqs, power = ss.welch(\n",
        "            signal,\n",
        "            fs=sampling_rate,\n",
        "            window=win,\n",
        "            nperseg=size,\n",
        "            noverlap=overlap,\n",
        "            nfft=nfft,\n",
        "            detrend='constant',\n",
        "            return_onesided=True,\n",
        "            scaling='spectrum',\n",
        "        )\n",
        "\n",
        "        # compensate one-sided\n",
        "        power *= 2\n",
        "\n",
        "        if decibel:\n",
        "            power = 10. * np.log10(power)\n",
        "\n",
        "        return utils.ReturnTuple((freqs, power), ('freqs', 'power'))\n",
        "\n",
        "    @staticmethod\n",
        "    def band_power(freqs=None, power=None, frequency=None, decibel=True):\n",
        "        \"\"\"Compute the avearge power in a frequency band.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        freqs : array\n",
        "            Array of frequencies (Hz) at which the power was computed.\n",
        "        power : array\n",
        "            Input power spectrum.\n",
        "        frequency : list, array\n",
        "            Pair of frequencies defining the band.\n",
        "        decibel : bool, optional\n",
        "            If True, input power is in decibels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        avg_power : float\n",
        "            The average power in the band.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if freqs is None:\n",
        "            raise TypeError(\"Please specify the 'freqs' array.\")\n",
        "\n",
        "        if power is None:\n",
        "            raise TypeError(\"Please specify the input power spectrum.\")\n",
        "\n",
        "        if len(freqs) != len(power):\n",
        "            raise ValueError(\n",
        "                \"The input 'freqs' and 'power' arrays must have the same length.\")\n",
        "\n",
        "        if frequency is None:\n",
        "            raise TypeError(\"Please specify the band frequencies.\")\n",
        "\n",
        "        try:\n",
        "            f1, f2 = frequency\n",
        "        except ValueError:\n",
        "            raise ValueError(\"Input 'frequency' must be a pair of frequencies.\")\n",
        "\n",
        "        # make frequencies sane\n",
        "        if f1 > f2:\n",
        "            f1, f2 = f2, f1\n",
        "\n",
        "        if f1 < freqs[0]:\n",
        "            f1 = freqs[0]\n",
        "        if f2 > freqs[-1]:\n",
        "            f2 = freqs[-1]\n",
        "\n",
        "        # average\n",
        "        sel = np.nonzero(np.logical_and(f1 <= freqs, freqs <= f2))[0]\n",
        "\n",
        "        if decibel:\n",
        "            aux = 10 ** (power / 10.)\n",
        "            avg = np.mean(aux[sel])\n",
        "            avg = 10. * np.log10(avg)\n",
        "        else:\n",
        "            avg = np.mean(power[sel])\n",
        "\n",
        "        return utils.ReturnTuple((avg,), ('avg_power',))\n",
        "\n",
        "    @staticmethod\n",
        "    def signal_stats(signal=None):\n",
        "        \"\"\"Compute various metrics describing the signal.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        mean : float\n",
        "            Mean of the signal.\n",
        "        median : float\n",
        "            Median of the signal.\n",
        "        max : float\n",
        "            Maximum signal amplitude.\n",
        "        var : float\n",
        "            Signal variance (unbiased).\n",
        "        std_dev : float\n",
        "            Standard signal deviation (unbiased).\n",
        "        abs_dev : float\n",
        "            Absolute signal deviation.\n",
        "        kurtosis : float\n",
        "            Signal kurtosis (unbiased).\n",
        "        skew : float\n",
        "            Signal skewness (unbiased).\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        signal = np.array(signal)\n",
        "\n",
        "        # mean\n",
        "        mean = np.mean(signal)\n",
        "\n",
        "        # median\n",
        "        median = np.median(signal)\n",
        "\n",
        "        # maximum amplitude\n",
        "        maxAmp = np.abs(signal - mean).max()\n",
        "\n",
        "        # variance\n",
        "        sigma2 = signal.var(ddof=1)\n",
        "\n",
        "        # standard deviation\n",
        "        sigma = signal.std(ddof=1)\n",
        "\n",
        "        # absolute deviation\n",
        "        ad = np.sum(np.abs(signal - median))\n",
        "\n",
        "        # kurtosis\n",
        "        kurt = stats.kurtosis(signal, bias=False)\n",
        "\n",
        "        # skweness\n",
        "        skew = stats.skew(signal, bias=False)\n",
        "\n",
        "        # output\n",
        "        args = (mean, median, maxAmp, sigma2, sigma, ad, kurt, skew)\n",
        "        names = ('mean', 'median', 'max', 'var', 'std_dev', 'abs_dev', 'kurtosis',\n",
        "                 'skewness')\n",
        "\n",
        "        return utils.ReturnTuple(args, names)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize(signal=None, ddof=1):\n",
        "        \"\"\"Normalize a signal to zero mean and unitary standard deviation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        ddof : int, optional\n",
        "            Delta degrees of freedom for standard deviation computation;\n",
        "            the divisor is `N - ddof`, where `N` is the number of elements;\n",
        "            default is one.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        signal : array\n",
        "            Normalized signal.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        signal = np.array(signal)\n",
        "\n",
        "        normalized = signal - signal.mean()\n",
        "        normalized /= normalized.std(ddof=ddof)\n",
        "\n",
        "        return utils.ReturnTuple((normalized,), ('signal',))\n",
        "\n",
        "    @staticmethod\n",
        "    def zero_cross(signal=None, detrend=False):\n",
        "        \"\"\"Locate the indices where the signal crosses zero.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        detrend : bool, optional\n",
        "            If True, remove signal mean before computation.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        zeros : array\n",
        "            Indices of zero crossings.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * When the signal crosses zero between samples, the first index\n",
        "          is returned.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        if detrend:\n",
        "            signal = signal - np.mean(signal)\n",
        "\n",
        "        # zeros\n",
        "        df = np.diff(np.sign(signal))\n",
        "        zeros = np.nonzero(np.abs(df) > 0)[0]\n",
        "\n",
        "        return utils.ReturnTuple((zeros,), ('zeros',))\n",
        "\n",
        "    @staticmethod\n",
        "    def find_extrema(signal=None, mode='both'):\n",
        "        \"\"\"Locate local extrema points in a signal.\n",
        "\n",
        "        Based on Fermat's Theorem [Ferm]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        mode : str, optional\n",
        "            Whether to find maxima ('max'), minima ('min'), or both ('both').\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        extrema : array\n",
        "            Indices of the extrama points.\n",
        "        values : array\n",
        "            Signal values at the extrema points.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [Ferm] Wikipedia, \"Fermat's theorem (stationary points)\",\n",
        "           https://en.wikipedia.org/wiki/Fermat%27s_theorem_(stationary_points)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        if mode not in ['max', 'min', 'both']:\n",
        "            raise ValueError(\"Unknwon mode %r.\" % mode)\n",
        "\n",
        "        aux = np.diff(np.sign(np.diff(signal)))\n",
        "\n",
        "        if mode == 'both':\n",
        "            aux = np.abs(aux)\n",
        "            extrema = np.nonzero(aux > 0)[0] + 1\n",
        "        elif mode == 'max':\n",
        "            extrema = np.nonzero(aux < 0)[0] + 1\n",
        "        elif mode == 'min':\n",
        "            extrema = np.nonzero(aux > 0)[0] + 1\n",
        "\n",
        "        values = signal[extrema]\n",
        "\n",
        "        return utils.ReturnTuple((extrema, values), ('extrema', 'values'))\n",
        "\n",
        "    @staticmethod\n",
        "    def windower(signal=None,\n",
        "                 size=None,\n",
        "                 step=None,\n",
        "                 fcn=None,\n",
        "                 fcn_kwargs=None,\n",
        "                 kernel='boxcar',\n",
        "                 kernel_kwargs=None):\n",
        "        \"\"\"Apply a function to a signal in sequential windows, with optional overlap.\n",
        "\n",
        "        Availabel window kernels: boxcar, triang, blackman, hamming, hann,\n",
        "        bartlett, flattop, parzen, bohman, blackmanharris, nuttall, barthann,\n",
        "        kaiser (needs beta), gaussian (needs std), general_gaussian (needs power,\n",
        "        width), slepian (needs width), chebwin (needs attenuation).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        size : int\n",
        "            Size of the signal window.\n",
        "        step : int, optional\n",
        "            Size of window shift; if None, there is no overlap.\n",
        "        fcn : callable\n",
        "            Function to apply to each window.\n",
        "        fcn_kwargs : dict, optional\n",
        "            Additional keyword arguments to pass to 'fcn'.\n",
        "        kernel : str, array, optional\n",
        "            Type of kernel to use; if array, use directly as the kernel.\n",
        "        kernel_kwargs : dict, optional\n",
        "            Additional keyword arguments to pass on window creation; ignored if\n",
        "            'kernel' is an array.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        index : array\n",
        "            Indices characterizing window locations (start of the window).\n",
        "        values : array\n",
        "            Concatenated output of calling 'fcn' on each window.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "        if fcn is None:\n",
        "            raise TypeError(\"Please specify a function to apply to each window.\")\n",
        "\n",
        "        if fcn_kwargs is None:\n",
        "            fcn_kwargs = {}\n",
        "\n",
        "        if kernel_kwargs is None:\n",
        "            kernel_kwargs = {}\n",
        "\n",
        "        length = len(signal)\n",
        "\n",
        "        if isinstance(kernel, six.string_types):\n",
        "            # check size\n",
        "            if size > length:\n",
        "                raise ValueError(\"Window size must be smaller than signal length.\")\n",
        "\n",
        "            win = _get_window(kernel, size, **kernel_kwargs)\n",
        "        elif isinstance(kernel, np.ndarray):\n",
        "            win = kernel\n",
        "            size = len(win)\n",
        "\n",
        "            # check size\n",
        "            if size > length:\n",
        "                raise ValueError(\"Window size must be smaller than signal length.\")\n",
        "\n",
        "        if step is None:\n",
        "            step = size\n",
        "\n",
        "        if step <= 0:\n",
        "            raise ValueError(\"Step size must be at least 1.\")\n",
        "\n",
        "        # number of windows\n",
        "        nb = 1 + (length - size) // step\n",
        "\n",
        "        # check signal dimensionality\n",
        "        if np.ndim(signal) == 2:\n",
        "            # time along 1st dim, tile window\n",
        "            nch = np.shape(signal)[1]\n",
        "            win = np.tile(np.reshape(win, (size, 1)), nch)\n",
        "\n",
        "        index = []\n",
        "        values = []\n",
        "        for i in range(nb):\n",
        "            start = i * step\n",
        "            stop = start + size\n",
        "            index.append(start)\n",
        "\n",
        "            aux = signal[start:stop] * win\n",
        "\n",
        "            # apply function\n",
        "            out = fcn(aux, **fcn_kwargs)\n",
        "            values.append(out)\n",
        "\n",
        "        # transform to numpy\n",
        "        index = np.array(index, dtype='int')\n",
        "        values = np.array(values)\n",
        "\n",
        "        return utils.ReturnTuple((index, values), ('index', 'values'))\n",
        "\n",
        "    @staticmethod\n",
        "    def synchronize(x=None, y=None, detrend=True):\n",
        "        \"\"\"Align two signals based on cross-correlation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array\n",
        "            First input signal.\n",
        "        y : array\n",
        "            Second input signal.\n",
        "        detrend : bool, optional\n",
        "            If True, remove signal means before computation.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        delay : int\n",
        "            Delay (number of samples) of 'x' in relation to 'y';\n",
        "            if 'delay' < 0 , 'x' is ahead in relation to 'y';\n",
        "            if 'delay' > 0 , 'x' is delayed in relation to 'y'.\n",
        "        corr : float\n",
        "            Value of maximum correlation.\n",
        "        synch_x : array\n",
        "            Biggest possible portion of 'x' in synchronization.\n",
        "        synch_y : array\n",
        "            Biggest possible portion of 'y' in synchronization.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if x is None:\n",
        "            raise TypeError(\"Please specify the first input signal.\")\n",
        "\n",
        "        if y is None:\n",
        "            raise TypeError(\"Please specify the second input signal.\")\n",
        "\n",
        "        n1 = len(x)\n",
        "        n2 = len(y)\n",
        "\n",
        "        if detrend:\n",
        "            x = x - np.mean(x)\n",
        "            y = y - np.mean(y)\n",
        "\n",
        "        # correlate\n",
        "        corr = np.correlate(x, y, mode='full')\n",
        "        d = np.arange(-n2 + 1, n1, dtype='int')\n",
        "        ind = np.argmax(corr)\n",
        "\n",
        "        delay = d[ind]\n",
        "        maxCorr = corr[ind]\n",
        "\n",
        "        # get synchronization overlap\n",
        "        if delay < 0:\n",
        "            c = min([n1, len(y[-delay:])])\n",
        "            synch_x = x[:c]\n",
        "            synch_y = y[-delay:-delay + c]\n",
        "        elif delay > 0:\n",
        "            c = min([n2, len(x[delay:])])\n",
        "            synch_x = x[delay:delay + c]\n",
        "            synch_y = y[:c]\n",
        "        else:\n",
        "            c = min([n1, n2])\n",
        "            synch_x = x[:c]\n",
        "            synch_y = y[:c]\n",
        "\n",
        "        # output\n",
        "        args = (delay, maxCorr, synch_x, synch_y)\n",
        "        names = ('delay', 'corr', 'synch_x', 'synch_y')\n",
        "\n",
        "        return utils.ReturnTuple(args, names)\n",
        "\n",
        "    @staticmethod\n",
        "    def pearson_correlation(x=None, y=None):\n",
        "        \"\"\"Compute the Pearson Correlation Coefficient bertween two signals.\n",
        "\n",
        "        The coefficient is given by:\n",
        "\n",
        "        .. math::\n",
        "\n",
        "            r_{xy} = \\\\frac{E[(X - \\\\mu_X) (Y - \\\\mu_Y)]}{\\\\sigma_X \\\\sigma_Y}\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array\n",
        "            First input signal.\n",
        "        y : array\n",
        "            Second input signal.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        rxy : float\n",
        "            Pearson correlation coefficient, ranging between -1 and +1.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the input signals do not have the same length.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if x is None:\n",
        "            raise TypeError(\"Please specify the first input signal.\")\n",
        "\n",
        "        if y is None:\n",
        "            raise TypeError(\"Please specify the second input signal.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "\n",
        "        n = len(x)\n",
        "\n",
        "        if n != len(y):\n",
        "            raise ValueError('Input signals must have the same length.')\n",
        "\n",
        "        mx = np.mean(x)\n",
        "        my = np.mean(y)\n",
        "\n",
        "        Sxy = np.sum(x * y) - n*mx*my\n",
        "        Sxx = np.sum(np.power(x, 2)) - n * mx**2\n",
        "        Syy = np.sum(np.power(x, 2)) - n * my**2\n",
        "\n",
        "        rxy = Sxy / (np.sqrt(Sxx) * np.sqrt(Syy))\n",
        "\n",
        "        # avoid propagation of numerical errors\n",
        "        if rxy > 1.0:\n",
        "            rxy = 1.0\n",
        "        elif rxy < -1.0:\n",
        "            rxy = -1.0\n",
        "\n",
        "        return utils.ReturnTuple((rxy, ), ('rxy', ))\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_error(x=None, y=None):\n",
        "        \"\"\"Compute the Root-Mean-Square Error between two signals.\n",
        "\n",
        "        The error is given by:\n",
        "\n",
        "        .. math::\n",
        "\n",
        "            rmse = \\\\sqrt{E[(X - Y)^2]}\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array\n",
        "            First input signal.\n",
        "        y : array\n",
        "            Second input signal.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        rmse : float\n",
        "            Root-mean-square error.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the input signals do not have the same length.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if x is None:\n",
        "            raise TypeError(\"Please specify the first input signal.\")\n",
        "\n",
        "        if y is None:\n",
        "            raise TypeError(\"Please specify the second input signal.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        x = np.array(x)\n",
        "        y = np.array(y)\n",
        "\n",
        "        n = len(x)\n",
        "\n",
        "        if n != len(y):\n",
        "            raise ValueError('Input signals must have the same length.')\n",
        "\n",
        "        rmse = np.sqrt(np.mean(np.power(x - y, 2)))\n",
        "\n",
        "        return utils.ReturnTuple((rmse, ), ('rmse', ))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_heart_rate(beats=None, sampling_rate=1000., smooth=False, size=3):\n",
        "        \"\"\"Compute instantaneous heart rate from an array of beat indices.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        beats : array\n",
        "            Beat location indices.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        smooth : bool, optional\n",
        "            If True, perform smoothing on the resulting heart rate.\n",
        "        size : int, optional\n",
        "            Size of smoothing window; ignored if `smooth` is False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        index : array\n",
        "            Heart rate location indices.\n",
        "        heart_rate : array\n",
        "            Instantaneous heart rate (bpm).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Assumes normal human heart rate to be between 40 and 200 bpm.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if beats is None:\n",
        "            raise TypeError(\"Please specify the input beat indices.\")\n",
        "\n",
        "        if len(beats) < 2:\n",
        "            raise ValueError(\"Not enough beats to compute heart rate.\")\n",
        "\n",
        "        # compute heart rate\n",
        "        ts = beats[1:]\n",
        "        hr = sampling_rate * (60. / np.diff(beats))\n",
        "\n",
        "        # physiological limits\n",
        "        indx = np.nonzero(np.logical_and(hr >= 40, hr <= 200))\n",
        "        ts = ts[indx]\n",
        "        hr = hr[indx]\n",
        "\n",
        "        # smooth with moving average\n",
        "        if smooth and (len(hr) > 1):\n",
        "            hr, _ = smoother(signal=hr, kernel='boxcar', size=size, mirror=True)\n",
        "\n",
        "        return utils.ReturnTuple((ts, hr), ('index', 'heart_rate'))\n",
        "\n",
        "    @staticmethod\n",
        "    def _pdiff(x, p1, p2):\n",
        "        \"\"\"Compute the squared difference between two interpolators, given the\n",
        "        x-coordinates.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array\n",
        "            Array of x-coordinates.\n",
        "        p1 : object\n",
        "            First interpolator.\n",
        "        p2 : object\n",
        "            Second interpolator.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        diff : array\n",
        "            Squared differences.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        diff = (p1(x) - p2(x)) ** 2\n",
        "\n",
        "        return diff\n",
        "\n",
        "    @staticmethod\n",
        "    def find_intersection(x1=None,\n",
        "                          y1=None,\n",
        "                          x2=None,\n",
        "                          y2=None,\n",
        "                          alpha=1.5,\n",
        "                          xtol=1e-6,\n",
        "                          ytol=1e-6):\n",
        "        \"\"\"Find the intersection points between two lines using piecewise\n",
        "        polynomial interpolation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x1 : array\n",
        "            Array of x-coordinates of the first line.\n",
        "        y1 : array\n",
        "            Array of y-coordinates of the first line.\n",
        "        x2 : array\n",
        "            Array of x-coordinates of the second line.\n",
        "        y2 : array\n",
        "            Array of y-coordinates of the second line.\n",
        "        alpha : float, optional\n",
        "            Resolution factor for the x-axis; fraction of total number of\n",
        "            x-coordinates.\n",
        "        xtol : float, optional\n",
        "            Tolerance for the x-axis.\n",
        "        ytol : float, optional\n",
        "            Tolerance for the y-axis.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        roots : array\n",
        "            Array of x-coordinates of found intersection points.\n",
        "        values : array\n",
        "            Array of y-coordinates of found intersection points.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * If no intersection is found, returns the closest point.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if x1 is None:\n",
        "            raise TypeError(\"Please specify the x-coordinates of the first line.\")\n",
        "        if y1 is None:\n",
        "            raise TypeError(\"Please specify the y-coordinates of the first line.\")\n",
        "        if x2 is None:\n",
        "            raise TypeError(\"Please specify the x-coordinates of the second line.\")\n",
        "        if y2 is None:\n",
        "            raise TypeError(\"Please specify the y-coordinates of the second line.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        x1 = np.array(x1)\n",
        "        y1 = np.array(y1)\n",
        "        x2 = np.array(x2)\n",
        "        y2 = np.array(y2)\n",
        "\n",
        "        if x1.shape != y1.shape:\n",
        "            raise ValueError(\n",
        "                \"Input coordinates for the first line must have the same shape.\")\n",
        "        if x2.shape != y2.shape:\n",
        "            raise ValueError(\n",
        "                \"Input coordinates for the second line must have the same shape.\")\n",
        "\n",
        "        # interpolate\n",
        "        p1 = interpolate.BPoly.from_derivatives(x1, y1[:, np.newaxis])\n",
        "        p2 = interpolate.BPoly.from_derivatives(x2, y2[:, np.newaxis])\n",
        "\n",
        "        # combine x intervals\n",
        "        x = np.r_[x1, x2]\n",
        "        x_min = x.min()\n",
        "        x_max = x.max()\n",
        "        npoints = int(len(np.unique(x)) * alpha)\n",
        "        x = np.linspace(x_min, x_max, npoints)\n",
        "\n",
        "        # initial estimates\n",
        "        pd = p1(x) - p2(x)\n",
        "        zerocs, = zero_cross(pd)\n",
        "\n",
        "        pd_abs = np.abs(pd)\n",
        "        zeros = np.nonzero(pd_abs < ytol)[0]\n",
        "\n",
        "        ind = np.unique(np.concatenate((zerocs, zeros)))\n",
        "        xi = x[ind]\n",
        "\n",
        "        # search for solutions\n",
        "        roots = set()\n",
        "        for v in xi:\n",
        "            root, _, ier, _ = optimize.fsolve(\n",
        "                _pdiff,\n",
        "                v,\n",
        "                args=(p1, p2),\n",
        "                full_output=True,\n",
        "                xtol=xtol,\n",
        "            )\n",
        "            if ier == 1 and x_min <= root <= x_max:\n",
        "                roots.add(root[0])\n",
        "\n",
        "        if len(roots) == 0:\n",
        "            # no solution was found => give the best from the initial estimates\n",
        "            aux = np.abs(pd)\n",
        "            bux = aux.min() * np.ones(npoints, dtype='float')\n",
        "            roots, _ = find_intersection(x, aux, x, bux,\n",
        "                                         alpha=1.,\n",
        "                                         xtol=xtol,\n",
        "                                         ytol=ytol)\n",
        "\n",
        "        # compute values\n",
        "        roots = list(roots)\n",
        "        roots.sort()\n",
        "        roots = np.array(roots)\n",
        "        values = np.mean(np.vstack((p1(roots), p2(roots))), axis=0)\n",
        "\n",
        "        return utils.ReturnTuple((roots, values), ('roots', 'values'))\n",
        "\n",
        "    @staticmethod\n",
        "    def finite_difference(signal=None, weights=None):\n",
        "        \"\"\"Apply the Finite Difference method to compute derivatives.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Signal to differentiate.\n",
        "        weights : list, array\n",
        "            Finite difference weight coefficients.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        index : array\n",
        "            Indices from `signal` for which the derivative was computed.\n",
        "        derivative : array\n",
        "            Computed derivative.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * The method assumes central differences weights.\n",
        "        * The method accounts for the delay introduced by the algorithm.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the number of weights is not odd.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify a signal to differentiate.\")\n",
        "\n",
        "        if weights is None:\n",
        "            raise TypeError(\"Please specify the weight coefficients.\")\n",
        "\n",
        "        N = len(weights)\n",
        "        if N % 2 == 0:\n",
        "            raise ValueError(\"Number of weights must be odd.\")\n",
        "\n",
        "        # diff\n",
        "        weights = weights[::-1]\n",
        "        derivative = ss.lfilter(weights, [1], signal)\n",
        "\n",
        "        # trim delay\n",
        "        D = N - 1\n",
        "        D2 = D // 2\n",
        "\n",
        "        index = np.arange(D2, len(signal) - D2, dtype='int')\n",
        "        derivative = derivative[D:]\n",
        "\n",
        "        return utils.ReturnTuple((index, derivative), ('index', 'derivative'))\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_dist_profile(m, n, signal):\n",
        "        \"\"\"Compute initial time series signal statistics for distance profile.\n",
        "\n",
        "        Implements the algorithm described in [Mueen2014]_, using the notation\n",
        "        from [Yeh2016_a]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        m : int\n",
        "            Sub-sequence length.\n",
        "        n : int\n",
        "            Target signal length.\n",
        "        signal : array\n",
        "            Target signal.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X : array\n",
        "            Fourier Transform (FFT) of the signal.\n",
        "        sigma : array\n",
        "            Moving standard deviation in windows of length `m`.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [Mueen2014] Abdullah Mueen, Hossein Hamooni, \"Trilce Estrada: Time\n",
        "           Series Join on Subsequence Correlation\", ICDM 2014: 450-459\n",
        "        .. [Yeh2016_a] Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova,\n",
        "           Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva,\n",
        "           Abdullah Mueen, Eamonn Keogh, \"Matrix Profile I: All Pairs Similarity\n",
        "           Joins for Time Series: A Unifying View that Includes Motifs, Discords\n",
        "           and Shapelets\", IEEE ICDM 2016\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # compute signal stats\n",
        "        csumx = np.zeros(n+1, dtype='float')\n",
        "        csumx[1:] = np.cumsum(signal)\n",
        "        sumx = csumx[m:] - csumx[:-m]\n",
        "\n",
        "        csumx2 = np.zeros(n+1, dtype='float')\n",
        "        csumx2[1:] = np.cumsum(np.power(signal, 2))\n",
        "        sumx2 = csumx2[m:] - csumx2[:-m]\n",
        "\n",
        "        meanx = sumx / m\n",
        "        sigmax2 = (sumx2 / m) - np.power(meanx, 2)\n",
        "        sigma = np.sqrt(sigmax2)\n",
        "\n",
        "        # append zeros\n",
        "        x = np.concatenate((signal, np.zeros(n, dtype='float')))\n",
        "\n",
        "        # compute FFT\n",
        "        X = np.fft.fft(x)\n",
        "\n",
        "        return X, sigma\n",
        "\n",
        "    @staticmethod\n",
        "    def _ditance_profile(m, n, query, X, sigma):\n",
        "        \"\"\"Compute the distance profile of a query sequence against a signal.\n",
        "\n",
        "        Implements the algorithm described in [Mueen2014]_, using the notation\n",
        "        from [Yeh2016]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        m : int\n",
        "            Query sub-sequence length.\n",
        "        n : int\n",
        "            Target time series length.\n",
        "        query : array\n",
        "            Query sub-sequence.\n",
        "        X : array\n",
        "            Target time series Fourier Transform (FFT).\n",
        "        sigma : array\n",
        "            Moving standard deviation in windows of length `m`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dist : array\n",
        "            Distance profile (squared).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Computes distances on z-normalized data.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [Mueen2014] Abdullah Mueen, Hossein Hamooni, \"Trilce Estrada: Time\n",
        "           Series Join on Subsequence Correlation\", ICDM 2014: 450-459\n",
        "        .. [Yeh2016] Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova,\n",
        "           Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva,\n",
        "           Abdullah Mueen, Eamonn Keogh, \"Matrix Profile I: All Pairs Similarity\n",
        "           Joins for Time Series: A Unifying View that Includes Motifs, Discords\n",
        "           and Shapelets\", IEEE ICDM 2016\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # normalize query\n",
        "        q = (query - np.mean(query)) / np.std(query)\n",
        "\n",
        "        # reverse query and append zeros\n",
        "        y = np.concatenate((q[::-1], np.zeros(2*n - m, dtype='float')))\n",
        "\n",
        "        # compute dot products fast\n",
        "        Y = np.fft.fft(y)\n",
        "        Z = X * Y\n",
        "        z = np.fft.ifft(Z)\n",
        "        z = z[m-1:n]\n",
        "\n",
        "        # compute distances (z-normalized squared euclidean distance)\n",
        "        dist = 2 * m * (1 - z / (m * sigma))\n",
        "\n",
        "        return dist\n",
        "\n",
        "    @staticmethod\n",
        "    def distance_profile(query=None, signal=None, metric='euclidean'):\n",
        "        \"\"\"Compute the distance profile of a query sequence against a signal.\n",
        "\n",
        "        Implements the algorithm described in [Mueen2014]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        query : array\n",
        "            Input query signal sequence.\n",
        "        signal : array\n",
        "            Input target time series signal.\n",
        "        metric : str, optional\n",
        "            The distance metric to use; one of 'euclidean' or 'pearson'; default\n",
        "            is 'euclidean'.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dist : array\n",
        "            Distance of the query sequence to every sub-sequnce in the signal.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Computes distances on z-normalized data.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [Mueen2014] Abdullah Mueen, Hossein Hamooni, \"Trilce Estrada: Time\n",
        "           Series Join on Subsequence Correlation\", ICDM 2014: 450-459\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if query is None:\n",
        "            raise TypeError(\"Please specify the input query sequence.\")\n",
        "\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify the input time series signal.\")\n",
        "\n",
        "        if metric not in ['euclidean', 'pearson']:\n",
        "            raise ValueError(\"Unknown distance metric.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        query = np.array(query)\n",
        "        signal = np.array(signal)\n",
        "\n",
        "        m = len(query)\n",
        "        n = len(signal)\n",
        "        if m > n/2:\n",
        "            raise ValueError(\"Time series signal is too short relative to\"\n",
        "                             \" query length.\")\n",
        "\n",
        "        # get initial signal stats\n",
        "        X, sigma = _init_dist_profile(m, n, signal)\n",
        "\n",
        "        # compute distance profile\n",
        "        dist = _ditance_profile(m, n, query, X, sigma)\n",
        "\n",
        "        if metric == 'pearson':\n",
        "            dist = 1 - np.abs(dist) / (2 * m)\n",
        "        elif metric == 'euclidean':\n",
        "            dist = np.abs(np.sqrt(dist))\n",
        "\n",
        "        return utils.ReturnTuple((dist, ), ('dist', ))\n",
        "\n",
        "    @staticmethod\n",
        "    def signal_self_join(signal=None, size=None, index=None, limit=None):\n",
        "        \"\"\"Compute the matrix profile for a self-similarity join of a time series.\n",
        "\n",
        "        Implements the algorithm described in [Yeh2016_b]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input target time series signal.\n",
        "        size : int\n",
        "            Size of the query sub-sequences.\n",
        "        index : list, array, optional\n",
        "            Starting indices for query sub-sequences; the default is to search all\n",
        "            sub-sequences.\n",
        "        limit : int, optional\n",
        "            Upper limit for the number of query sub-sequences; the default is to\n",
        "            search all sub-sequences.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        matrix_index : array\n",
        "            Matric profile index.\n",
        "        matrix_profile : array\n",
        "            Computed matrix profile (distances).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Computes euclidean distances on z-normalized data.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [Yeh2016_b] Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova,\n",
        "           Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva,\n",
        "           Abdullah Mueen, Eamonn Keogh, \"Matrix Profile I: All Pairs Similarity\n",
        "           Joins for Time Series: A Unifying View that Includes Motifs, Discords\n",
        "           and Shapelets\", IEEE ICDM 2016\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal is None:\n",
        "            raise TypeError(\"Please specify the input time series signal.\")\n",
        "\n",
        "        if size is None:\n",
        "            raise TypeError(\"Please specify the sub-sequence size.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        signal = np.array(signal)\n",
        "\n",
        "        n = len(signal)\n",
        "        if size > n/2:\n",
        "            raise ValueError(\"Time series signal is too short relative to desired\"\n",
        "                             \" sub-sequence length.\")\n",
        "\n",
        "        if size < 4:\n",
        "            raise ValueError(\"Sub-sequence length must be at least 4.\")\n",
        "\n",
        "        # matrix profile length\n",
        "        nb = n - size + 1\n",
        "\n",
        "        # get search index\n",
        "        if index is None:\n",
        "            index = np.random.permutation(np.arange(nb, dtype='int'))\n",
        "        else:\n",
        "            index = np.array(index)\n",
        "            if not np.all(index < nb):\n",
        "                raise ValueError(\"Provided `index` exceeds allowable sub-sequences.\")\n",
        "\n",
        "        # limit search\n",
        "        if limit is not None:\n",
        "            if limit < 1:\n",
        "                raise ValueError(\"Search limit must be at least 1.\")\n",
        "\n",
        "            index = index[:limit]\n",
        "\n",
        "        # exclusion zone (to avoid query self-matches)\n",
        "        ezone = int(round(size / 4))\n",
        "\n",
        "        # initialization\n",
        "        matrix_profile = np.inf * np.ones(nb, dtype='float')\n",
        "        matrix_index = np.zeros(nb, dtype='int')\n",
        "\n",
        "        X, sigma = _init_dist_profile(size, n, signal)\n",
        "\n",
        "        # compute matrix profile\n",
        "        for idx in index:\n",
        "            # compute distance profile\n",
        "            query = signal[idx:idx+size]\n",
        "            dist = _ditance_profile(size, n, query, X, sigma)\n",
        "            dist = np.abs(np.sqrt(dist)) # to have euclidean distance\n",
        "\n",
        "            # apply exlusion zone\n",
        "            a = max([0, idx-ezone])\n",
        "            b = min([nb, idx+ezone+1])\n",
        "            dist[a:b] = np.inf\n",
        "\n",
        "            # find nearest neighbors\n",
        "            pos = dist < matrix_profile\n",
        "            matrix_profile[pos] = dist[pos]\n",
        "            matrix_index[pos] = idx\n",
        "\n",
        "            # account for exlusion zone\n",
        "            neighbor = np.argmin(dist)\n",
        "            matrix_profile[idx] = dist[neighbor]\n",
        "            matrix_index[idx] = neighbor\n",
        "\n",
        "        # output\n",
        "        args = (matrix_index, matrix_profile)\n",
        "        names = ('matrix_index', 'matrix_profile')\n",
        "\n",
        "        return utils.ReturnTuple(args, names)\n",
        "\n",
        "    @staticmethod\n",
        "    def signal_cross_join(signal1=None,\n",
        "                          signal2=None,\n",
        "                          size=None,\n",
        "                          index=None,\n",
        "                          limit=None):\n",
        "        \"\"\"Compute the matrix profile for a similarity join of two time series.\n",
        "\n",
        "        Computes the nearest sub-sequence in `signal2` for each sub-sequence in\n",
        "        `signal1`. Implements the algorithm described in [Yeh2016_c]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal1 : array\n",
        "            Fisrt input time series signal.\n",
        "        signal2 : array\n",
        "            Second input time series signal.\n",
        "        size : int\n",
        "            Size of the query sub-sequences.\n",
        "        index : list, array, optional\n",
        "            Starting indices for query sub-sequences; the default is to search all\n",
        "            sub-sequences.\n",
        "        limit : int, optional\n",
        "            Upper limit for the number of query sub-sequences; the default is to\n",
        "            search all sub-sequences.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        matrix_index : array\n",
        "            Matric profile index.\n",
        "        matrix_profile : array\n",
        "            Computed matrix profile (distances).\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Computes euclidean distances on z-normalized data.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. [Yeh2016_c] Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova,\n",
        "           Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva,\n",
        "           Abdullah Mueen, Eamonn Keogh, \"Matrix Profile I: All Pairs Similarity\n",
        "           Joins for Time Series: A Unifying View that Includes Motifs, Discords\n",
        "           and Shapelets\", IEEE ICDM 2016\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if signal1 is None:\n",
        "            raise TypeError(\"Please specify the first input time series signal.\")\n",
        "\n",
        "        if signal2 is None:\n",
        "            raise TypeError(\"Please specify the second input time series signal.\")\n",
        "\n",
        "        if size is None:\n",
        "            raise TypeError(\"Please specify the sub-sequence size.\")\n",
        "\n",
        "        # ensure numpy\n",
        "        signal1 = np.array(signal1)\n",
        "        signal2 = np.array(signal2)\n",
        "\n",
        "        n1 = len(signal1)\n",
        "        if size > n1/2:\n",
        "            raise ValueError(\"First time series signal is too short relative to\"\n",
        "                             \" desired sub-sequence length.\")\n",
        "\n",
        "        n2 = len(signal2)\n",
        "        if size > n2/2:\n",
        "            raise ValueError(\"Second time series signal is too short relative to\"\n",
        "                             \" desired sub-sequence length.\")\n",
        "\n",
        "        if size < 4:\n",
        "            raise ValueError(\"Sub-sequence length must be at least 4.\")\n",
        "\n",
        "        # matrix profile length\n",
        "        nb1 = n1 - size + 1\n",
        "        nb2 = n2 - size + 1\n",
        "\n",
        "        # get search index\n",
        "        if index is None:\n",
        "            index = np.random.permutation(np.arange(nb2, dtype='int'))\n",
        "        else:\n",
        "            index = np.array(index)\n",
        "            if not np.all(index < nb2):\n",
        "                raise ValueError(\"Provided `index` exceeds allowable `signal2`\"\n",
        "                                 \" sub-sequences.\")\n",
        "\n",
        "        # limit search\n",
        "        if limit is not None:\n",
        "            if limit < 1:\n",
        "                raise ValueError(\"Search limit must be at least 1.\")\n",
        "\n",
        "            index = index[:limit]\n",
        "\n",
        "        # initialization\n",
        "        matrix_profile = np.inf * np.ones(nb1, dtype='float')\n",
        "        matrix_index = np.zeros(nb1, dtype='int')\n",
        "\n",
        "        X, sigma = _init_dist_profile(size, n1, signal1)\n",
        "\n",
        "        # compute matrix profile\n",
        "        for idx in index:\n",
        "            # compute distance profile\n",
        "            query = signal2[idx:idx+size]\n",
        "            dist = _ditance_profile(size, n1, query, X, sigma)\n",
        "            dist = np.abs(np.sqrt(dist)) # to have euclidean distance\n",
        "\n",
        "            # find nearest neighbor\n",
        "            pos = dist <= matrix_profile\n",
        "            matrix_profile[pos] = dist[pos]\n",
        "            matrix_index[pos] = idx\n",
        "\n",
        "        # output\n",
        "        args = (matrix_index, matrix_profile)\n",
        "        names = ('matrix_index', 'matrix_profile')\n",
        "\n",
        "        return utils.ReturnTuple(args, names)\n",
        "\n",
        "\n",
        "    def mean_waves(data=None, size=None, step=None):\n",
        "        \"\"\"Extract mean samples from a data set.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : array\n",
        "            An m by n array of m data samples in an n-dimensional space.\n",
        "        size : int\n",
        "            Number of samples to use for each mean sample.\n",
        "        step : int, optional\n",
        "            Number of samples to jump, controlling overlap; default is equal to\n",
        "            `size` (no overlap).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        waves : array\n",
        "            An k by n array of mean samples.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Discards trailing samples if they are not enough to satify the `size`\n",
        "          parameter.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If `step` is an invalid value.\n",
        "        ValueError\n",
        "            If there are not enough samples for the given `size`.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if data is None:\n",
        "            raise TypeError(\"Please specify an input data set.\")\n",
        "\n",
        "        if size is None:\n",
        "            raise TypeError(\"Please specify the number of samples for the mean.\")\n",
        "\n",
        "        if step is None:\n",
        "            step = size\n",
        "\n",
        "        if step < 0:\n",
        "            raise ValueError(\"The step must be a positive integer.\")\n",
        "\n",
        "        # number of waves\n",
        "        L = len(data) - size\n",
        "        nb = 1 + L // step\n",
        "        if nb <= 0:\n",
        "            raise ValueError(\"Not enough samples for the given `size`.\")\n",
        "\n",
        "        # compute\n",
        "        waves = [np.mean(data[i:i+size], axis=0) for i in range(0, L+1, step)]\n",
        "        waves = np.array(waves)\n",
        "\n",
        "        return utils.ReturnTuple((waves, ), ('waves', ))\n",
        "\n",
        "\n",
        "    def median_waves(data=None, size=None, step=None):\n",
        "        \"\"\"Extract median samples from a data set.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : array\n",
        "            An m by n array of m data samples in an n-dimensional space.\n",
        "        size : int\n",
        "            Number of samples to use for each median sample.\n",
        "        step : int, optional\n",
        "            Number of samples to jump, controlling overlap; default is equal to\n",
        "            `size` (no overlap).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        waves : array\n",
        "            An k by n array of median samples.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        * Discards trailing samples if they are not enough to satify the `size`\n",
        "          parameter.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If `step` is an invalid value.\n",
        "        ValueError\n",
        "            If there are not enough samples for the given `size`.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # check inputs\n",
        "        if data is None:\n",
        "            raise TypeError(\"Please specify an input data set.\")\n",
        "\n",
        "        if size is None:\n",
        "            raise TypeError(\"Please specify the number of samples for the median.\")\n",
        "\n",
        "        if step is None:\n",
        "            step = size\n",
        "\n",
        "        if step < 0:\n",
        "            raise ValueError(\"The step must be a positive integer.\")\n",
        "\n",
        "        # number of waves\n",
        "        L = len(data) - size\n",
        "        nb = 1 + L // step\n",
        "        if nb <= 0:\n",
        "            raise ValueError(\"Not enough samples for the given `size`.\")\n",
        "\n",
        "        # compute\n",
        "        waves = [np.median(data[i:i+size], axis=0) for i in range(0, L+1, step)]\n",
        "        waves = np.array(waves)\n",
        "\n",
        "        return utils.ReturnTuple((waves, ), ('waves', ))\n",
        "\n",
        "st = Tools\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "biosppy.plotting\n",
        "----------------\n",
        "\n",
        "This module provides utilities to plot data.\n",
        "\n",
        ":copyright: (c) 2015-2018 by Instituto de Telecomunicacoes\n",
        ":license: BSD 3-clause, see LICENSE for more details.\n",
        "\"\"\"\n",
        "# Globals\n",
        "MAJOR_LW = 2.5\n",
        "MINOR_LW = 1.5\n",
        "MAX_ROWS = 10\n",
        "\n",
        "\n",
        "class Plotting:\n",
        "    @staticmethod\n",
        "    def _plot_filter(b, a, sampling_rate=1000., nfreqs=4096, ax=None):\n",
        "        \"\"\"Compute and plot the frequency response of a digital filter.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        b : array\n",
        "            Numerator coefficients.\n",
        "        a : array\n",
        "            Denominator coefficients.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        nfreqs : int, optional\n",
        "            Number of frequency points to compute.\n",
        "        ax : axis, optional\n",
        "            Plot Axis to use.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        fig : Figure\n",
        "            Figure object.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # compute frequency response\n",
        "        freqs, resp = st._filter_resp(b, a,\n",
        "                                      sampling_rate=sampling_rate,\n",
        "                                      nfreqs=nfreqs)\n",
        "\n",
        "        # plot\n",
        "        if ax is None:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111)\n",
        "        else:\n",
        "            fig = ax.figure\n",
        "\n",
        "        # amplitude\n",
        "        pwr = 20. * np.log10(np.abs(resp))\n",
        "        ax.semilogx(freqs, pwr, 'b', linewidth=MAJOR_LW)\n",
        "        ax.set_ylabel('Amplitude (dB)', color='b')\n",
        "        ax.set_xlabel('Frequency (Hz)')\n",
        "\n",
        "        # phase\n",
        "        angles = np.unwrap(np.angle(resp))\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.semilogx(freqs, angles, 'g', linewidth=MAJOR_LW)\n",
        "        ax2.set_ylabel('Angle (radians)', color='g')\n",
        "\n",
        "        ax.grid()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_filter(ftype='FIR',\n",
        "                    band='lowpass',\n",
        "                    order=None,\n",
        "                    frequency=None,\n",
        "                    sampling_rate=1000.,\n",
        "                    path=None,\n",
        "                    show=True, **kwargs):\n",
        "        \"\"\"Plot the frequency response of the filter specified with the given\n",
        "        parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ftype : str\n",
        "            Filter type:\n",
        "                * Finite Impulse Response filter ('FIR');\n",
        "                * Butterworth filter ('butter');\n",
        "                * Chebyshev filters ('cheby1', 'cheby2');\n",
        "                * Elliptic filter ('ellip');\n",
        "                * Bessel filter ('bessel').\n",
        "        band : str\n",
        "            Band type:\n",
        "                * Low-pass filter ('lowpass');\n",
        "                * High-pass filter ('highpass');\n",
        "                * Band-pass filter ('bandpass');\n",
        "                * Band-stop filter ('bandstop').\n",
        "        order : int\n",
        "            Order of the filter.\n",
        "        frequency : int, float, list, array\n",
        "            Cutoff frequencies; format depends on type of band:\n",
        "                * 'lowpass' or 'bandpass': single frequency;\n",
        "                * 'bandpass' or 'bandstop': pair of frequencies.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "        ``**kwargs`` : dict, optional\n",
        "            Additional keyword arguments are passed to the underlying\n",
        "            scipy.signal function.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # get filter\n",
        "        b, a = st.get_filter(ftype=ftype,\n",
        "                             band=band,\n",
        "                             order=order,\n",
        "                             frequency=frequency,\n",
        "                             sampling_rate=sampling_rate, **kwargs)\n",
        "\n",
        "        # plot\n",
        "        fig = _plot_filter(b, a, sampling_rate)\n",
        "\n",
        "        # make layout tight\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_spectrum(signal=None, sampling_rate=1000., path=None, show=True):\n",
        "        \"\"\"Plot the power spectrum of a signal (one-sided).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        sampling_rate : int, float, optional\n",
        "            Sampling frequency (Hz).\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        freqs, power = st.power_spectrum(signal, sampling_rate,\n",
        "                                         pad=0,\n",
        "                                         pow2=False,\n",
        "                                         decibel=True)\n",
        "\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "\n",
        "        ax.plot(freqs, power, linewidth=MAJOR_LW)\n",
        "        ax.set_xlabel('Frequency (Hz)')\n",
        "        ax.set_ylabel('Power (dB)')\n",
        "        ax.grid()\n",
        "\n",
        "        # make layout tight\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_bvp(ts=None,\n",
        "                 raw=None,\n",
        "                 filtered=None,\n",
        "                 onsets=None,\n",
        "                 heart_rate_ts=None,\n",
        "                 heart_rate=None,\n",
        "                 path=None,\n",
        "                 show=False):\n",
        "        \"\"\"Create a summary plot from the output of signals.bvp.bvp.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ts : array\n",
        "            Signal time axis reference (seconds).\n",
        "        raw : array\n",
        "            Raw BVP signal.\n",
        "        filtered : array\n",
        "            Filtered BVP signal.\n",
        "        onsets : array\n",
        "            Indices of BVP pulse onsets.\n",
        "        heart_rate_ts : array\n",
        "            Heart rate time axis reference (seconds).\n",
        "        heart_rate : array\n",
        "            Instantaneous heart rate (bpm).\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        fig = plt.figure()\n",
        "        fig.suptitle('BVP Summary')\n",
        "\n",
        "        # raw signal\n",
        "        ax1 = fig.add_subplot(311)\n",
        "\n",
        "        ax1.plot(ts, raw, linewidth=MAJOR_LW, label='Raw')\n",
        "\n",
        "        ax1.set_ylabel('Amplitude')\n",
        "        ax1.legend()\n",
        "        ax1.grid()\n",
        "\n",
        "        # filtered signal with onsets\n",
        "        ax2 = fig.add_subplot(312, sharex=ax1)\n",
        "\n",
        "        ymin = np.min(filtered)\n",
        "        ymax = np.max(filtered)\n",
        "        alpha = 0.1 * (ymax - ymin)\n",
        "        ymax += alpha\n",
        "        ymin -= alpha\n",
        "\n",
        "        ax2.plot(ts, filtered, linewidth=MAJOR_LW, label='Filtered')\n",
        "        ax2.vlines(ts[onsets], ymin, ymax,\n",
        "                   color='m',\n",
        "                   linewidth=MINOR_LW,\n",
        "                   label='Onsets')\n",
        "\n",
        "        ax2.set_ylabel('Amplitude')\n",
        "        ax2.legend()\n",
        "        ax2.grid()\n",
        "\n",
        "        # heart rate\n",
        "        ax3 = fig.add_subplot(313, sharex=ax1)\n",
        "\n",
        "        ax3.plot(heart_rate_ts, heart_rate, linewidth=MAJOR_LW, label='Heart Rate')\n",
        "\n",
        "        ax3.set_xlabel('Time (s)')\n",
        "        ax3.set_ylabel('Heart Rate (bpm)')\n",
        "        ax3.legend()\n",
        "        ax3.grid()\n",
        "\n",
        "        # make layout tight\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_eda(ts=None,\n",
        "                 raw=None,\n",
        "                 filtered=None,\n",
        "                 onsets=None,\n",
        "                 peaks=None,\n",
        "                 amplitudes=None,\n",
        "                 path=None,\n",
        "                 show=False):\n",
        "        \"\"\"Create a summary plot from the output of signals.eda.eda.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ts : array\n",
        "            Signal time axis reference (seconds).\n",
        "        raw : array\n",
        "            Raw EDA signal.\n",
        "        filtered : array\n",
        "            Filtered EDA signal.\n",
        "        onsets : array\n",
        "            Indices of SCR pulse onsets.\n",
        "        peaks : array\n",
        "            Indices of the SCR peaks.\n",
        "        amplitudes : array\n",
        "            SCR pulse amplitudes.\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        fig = plt.figure()\n",
        "        fig.suptitle('EDA Summary')\n",
        "\n",
        "        # raw signal\n",
        "        ax1 = fig.add_subplot(311)\n",
        "\n",
        "        ax1.plot(ts, raw, linewidth=MAJOR_LW, label='raw')\n",
        "\n",
        "        ax1.set_ylabel('Amplitude')\n",
        "        ax1.legend()\n",
        "        ax1.grid()\n",
        "\n",
        "        # filtered signal with onsets, peaks\n",
        "        ax2 = fig.add_subplot(312, sharex=ax1)\n",
        "\n",
        "        ymin = np.min(filtered)\n",
        "        ymax = np.max(filtered)\n",
        "        alpha = 0.1 * (ymax - ymin)\n",
        "        ymax += alpha\n",
        "        ymin -= alpha\n",
        "\n",
        "        ax2.plot(ts, filtered, linewidth=MAJOR_LW, label='Filtered')\n",
        "        ax2.vlines(ts[onsets], ymin, ymax,\n",
        "                   color='m',\n",
        "                   linewidth=MINOR_LW,\n",
        "                   label='Onsets')\n",
        "        ax2.vlines(ts[peaks], ymin, ymax,\n",
        "                   color='g',\n",
        "                   linewidth=MINOR_LW,\n",
        "                   label='Peaks')\n",
        "\n",
        "        ax2.set_ylabel('Amplitude')\n",
        "        ax2.legend()\n",
        "        ax2.grid()\n",
        "\n",
        "        # amplitudes\n",
        "        ax3 = fig.add_subplot(313, sharex=ax1)\n",
        "\n",
        "        ax3.plot(ts[onsets], amplitudes, linewidth=MAJOR_LW, label='Amplitudes')\n",
        "\n",
        "        ax3.set_xlabel('Time (s)')\n",
        "        ax3.set_ylabel('Amplitude')\n",
        "        ax3.legend()\n",
        "        ax3.grid()\n",
        "\n",
        "        # make layout tight\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_emg(ts=None,\n",
        "                 sampling_rate=None,\n",
        "                 raw=None,\n",
        "                 filtered=None,\n",
        "                 onsets=None,\n",
        "                 processed=None,\n",
        "                 path=None,\n",
        "                 show=False):\n",
        "        \"\"\"Create a summary plot from the output of signals.emg.emg.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ts : array\n",
        "            Signal time axis reference (seconds).\n",
        "        sampling_rate : int, float\n",
        "            Sampling frequency (Hz).\n",
        "        raw : array\n",
        "            Raw EMG signal.\n",
        "        filtered : array\n",
        "            Filtered EMG signal.\n",
        "        onsets : array\n",
        "            Indices of EMG pulse onsets.\n",
        "        processed : array, optional\n",
        "            Processed EMG signal according to the chosen onset detector.\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        fig = plt.figure()\n",
        "        fig.suptitle('EMG Summary')\n",
        "\n",
        "        if processed is not None:\n",
        "            ax1 = fig.add_subplot(311)\n",
        "            ax2 = fig.add_subplot(312, sharex=ax1)\n",
        "            ax3 = fig.add_subplot(313)\n",
        "\n",
        "            # processed signal\n",
        "            L = len(processed)\n",
        "            T = (L - 1) / sampling_rate\n",
        "            ts_processed = np.linspace(0, T, L, endpoint=False)\n",
        "            ax3.plot(ts_processed, processed,\n",
        "                     linewidth=MAJOR_LW,\n",
        "                     label='Processed')\n",
        "            ax3.set_xlabel('Time (s)')\n",
        "            ax3.set_ylabel('Amplitude')\n",
        "            ax3.legend()\n",
        "            ax3.grid()\n",
        "        else:\n",
        "            ax1 = fig.add_subplot(211)\n",
        "            ax2 = fig.add_subplot(212, sharex=ax1)\n",
        "\n",
        "        # raw signal\n",
        "        ax1.plot(ts, raw, linewidth=MAJOR_LW, label='Raw')\n",
        "\n",
        "        ax1.set_ylabel('Amplitude')\n",
        "        ax1.legend()\n",
        "        ax1.grid()\n",
        "\n",
        "        # filtered signal with onsets\n",
        "        ymin = np.min(filtered)\n",
        "        ymax = np.max(filtered)\n",
        "        alpha = 0.1 * (ymax - ymin)\n",
        "        ymax += alpha\n",
        "        ymin -= alpha\n",
        "\n",
        "        ax2.plot(ts, filtered, linewidth=MAJOR_LW, label='Filtered')\n",
        "        ax2.vlines(ts[onsets], ymin, ymax,\n",
        "                   color='m',\n",
        "                   linewidth=MINOR_LW,\n",
        "                   label='Onsets')\n",
        "\n",
        "        ax2.set_xlabel('Time (s)')\n",
        "        ax2.set_ylabel('Amplitude')\n",
        "        ax2.legend()\n",
        "        ax2.grid()\n",
        "\n",
        "        # make layout tight\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_resp(ts=None,\n",
        "                  raw=None,\n",
        "                  filtered=None,\n",
        "                  zeros=None,\n",
        "                  resp_rate_ts=None,\n",
        "                  resp_rate=None,\n",
        "                  path=None,\n",
        "                  show=False):\n",
        "        \"\"\"Create a summary plot from the output of signals.bvp.bvp.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ts : array\n",
        "            Signal time axis reference (seconds).\n",
        "        raw : array\n",
        "            Raw Resp signal.\n",
        "        filtered : array\n",
        "            Filtered Resp signal.\n",
        "        zeros : array\n",
        "            Indices of Respiration zero crossings.\n",
        "        resp_rate_ts : array\n",
        "            Respiration rate time axis reference (seconds).\n",
        "        resp_rate : array\n",
        "            Instantaneous respiration rate (Hz).\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        fig = plt.figure()\n",
        "        fig.suptitle('Respiration Summary')\n",
        "\n",
        "        # raw signal\n",
        "        ax1 = fig.add_subplot(311)\n",
        "\n",
        "        ax1.plot(ts, raw, linewidth=MAJOR_LW, label='Raw')\n",
        "\n",
        "        ax1.set_ylabel('Amplitude')\n",
        "        ax1.legend()\n",
        "        ax1.grid()\n",
        "\n",
        "        # filtered signal with zeros\n",
        "        ax2 = fig.add_subplot(312, sharex=ax1)\n",
        "\n",
        "        ymin = np.min(filtered)\n",
        "        ymax = np.max(filtered)\n",
        "        alpha = 0.1 * (ymax - ymin)\n",
        "        ymax += alpha\n",
        "        ymin -= alpha\n",
        "\n",
        "        ax2.plot(ts, filtered, linewidth=MAJOR_LW, label='Filtered')\n",
        "        ax2.vlines(ts[zeros], ymin, ymax,\n",
        "                   color='m',\n",
        "                   linewidth=MINOR_LW,\n",
        "                   label='Zero crossings')\n",
        "\n",
        "        ax2.set_ylabel('Amplitude')\n",
        "        ax2.legend()\n",
        "        ax2.grid()\n",
        "\n",
        "        # heart rate\n",
        "        ax3 = fig.add_subplot(313, sharex=ax1)\n",
        "\n",
        "        ax3.plot(resp_rate_ts, resp_rate,\n",
        "                 linewidth=MAJOR_LW,\n",
        "                 label='Respiration Rate')\n",
        "\n",
        "        ax3.set_xlabel('Time (s)')\n",
        "        ax3.set_ylabel('Respiration Rate (Hz)')\n",
        "        ax3.legend()\n",
        "        ax3.grid()\n",
        "\n",
        "        # make layout tight\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_eeg(ts=None,\n",
        "                 raw=None,\n",
        "                 filtered=None,\n",
        "                 labels=None,\n",
        "                 features_ts=None,\n",
        "                 theta=None,\n",
        "                 alpha_low=None,\n",
        "                 alpha_high=None,\n",
        "                 beta=None,\n",
        "                 gamma=None,\n",
        "                 plf_pairs=None,\n",
        "                 plf=None,\n",
        "                 path=None,\n",
        "                 show=False):\n",
        "        \"\"\"Create a summary plot from the output of signals.eeg.eeg.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ts : array\n",
        "            Signal time axis reference (seconds).\n",
        "        raw : array\n",
        "            Raw EEG signal.\n",
        "        filtered : array\n",
        "            Filtered EEG signal.\n",
        "        labels : list\n",
        "            Channel labels.\n",
        "        features_ts : array\n",
        "            Features time axis reference (seconds).\n",
        "        theta : array\n",
        "            Average power in the 4 to 8 Hz frequency band; each column is one\n",
        "            EEG channel.\n",
        "        alpha_low : array\n",
        "            Average power in the 8 to 10 Hz frequency band; each column is one\n",
        "            EEG channel.\n",
        "        alpha_high : array\n",
        "            Average power in the 10 to 13 Hz frequency band; each column is one\n",
        "            EEG channel.\n",
        "        beta : array\n",
        "            Average power in the 13 to 25 Hz frequency band; each column is one\n",
        "            EEG channel.\n",
        "        gamma : array\n",
        "            Average power in the 25 to 40 Hz frequency band; each column is one\n",
        "            EEG channel.\n",
        "        plf_pairs : list\n",
        "            PLF pair indices.\n",
        "        plf : array\n",
        "            PLF matrix; each column is a channel pair.\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        nrows = MAX_ROWS\n",
        "        alpha = 2.\n",
        "\n",
        "        figs = []\n",
        "\n",
        "        # raw\n",
        "        fig = _plot_multichannel(ts=ts,\n",
        "                                 signal=raw,\n",
        "                                 labels=labels,\n",
        "                                 nrows=nrows,\n",
        "                                 alpha=alpha,\n",
        "                                 title='EEG Summary - Raw',\n",
        "                                 xlabel='Time (s)',\n",
        "                                 ylabel='Amplitude')\n",
        "        figs.append(('_Raw', fig))\n",
        "\n",
        "        # filtered\n",
        "        fig = _plot_multichannel(ts=ts,\n",
        "                                 signal=filtered,\n",
        "                                 labels=labels,\n",
        "                                 nrows=nrows,\n",
        "                                 alpha=alpha,\n",
        "                                 title='EEG Summary - Filtered',\n",
        "                                 xlabel='Time (s)',\n",
        "                                 ylabel='Amplitude')\n",
        "        figs.append(('_Filtered', fig))\n",
        "\n",
        "        # band-power\n",
        "        names = ('Theta Band', 'Lower Alpha Band', 'Higher Alpha Band',\n",
        "                 'Beta Band', 'Gamma Band')\n",
        "        args = (theta, alpha_low, alpha_high, beta, gamma)\n",
        "        for n, a in zip(names, args):\n",
        "            fig = _plot_multichannel(ts=features_ts,\n",
        "                                     signal=a,\n",
        "                                     labels=labels,\n",
        "                                     nrows=nrows,\n",
        "                                     alpha=alpha,\n",
        "                                     title='EEG Summary - %s' % n,\n",
        "                                     xlabel='Time (s)',\n",
        "                                     ylabel='Power')\n",
        "            figs.append(('_' + n.replace(' ', '_'), fig))\n",
        "\n",
        "        # PLF\n",
        "        plf_labels = ['%s vs %s' % (labels[p[0]], labels[p[1]]) for p in plf_pairs]\n",
        "        fig = _plot_multichannel(ts=features_ts,\n",
        "                                 signal=plf,\n",
        "                                 labels=plf_labels,\n",
        "                                 nrows=nrows,\n",
        "                                 alpha=alpha,\n",
        "                                 title='EEG Summary - Phase-Locking Factor',\n",
        "                                 xlabel='Time (s)',\n",
        "                                 ylabel='PLF')\n",
        "        figs.append(('_PLF', fig))\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                ext = '.png'\n",
        "\n",
        "            for n, fig in figs:\n",
        "                path = root + n + ext\n",
        "                fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            for _, fig in figs:\n",
        "                plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def _yscaling(signal=None, alpha=1.5):\n",
        "        \"\"\"Get y axis limits for a signal with scaling.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        signal : array\n",
        "            Input signal.\n",
        "        alpha : float, optional\n",
        "            Scaling factor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ymin : float\n",
        "            Minimum y value.\n",
        "        ymax : float\n",
        "            Maximum y value.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        mi = np.min(signal)\n",
        "        m = np.mean(signal)\n",
        "        mx = np.max(signal)\n",
        "\n",
        "        if mi == mx:\n",
        "            ymin = m - 1\n",
        "            ymax = m + 1\n",
        "        else:\n",
        "            ymin = m - alpha * (m - mi)\n",
        "            ymax = m + alpha * (mx - m)\n",
        "\n",
        "        return ymin, ymax\n",
        "\n",
        "    @staticmethod\n",
        "    def _plot_multichannel(ts=None,\n",
        "                           signal=None,\n",
        "                           labels=None,\n",
        "                           nrows=10,\n",
        "                           alpha=2.,\n",
        "                           title=None,\n",
        "                           xlabel=None,\n",
        "                           ylabel=None):\n",
        "        \"\"\"Plot a multi-channel signal.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ts : array\n",
        "            Signal time axis reference (seconds).\n",
        "        signal : array\n",
        "            Multi-channel signal; each column is one channel.\n",
        "        labels : list, optional\n",
        "            Channel labels.\n",
        "        nrows : int, optional\n",
        "            Maximum number of rows to use.\n",
        "        alpha : float, optional\n",
        "            Scaling factor for y axis.\n",
        "        title : str, optional\n",
        "            Plot title.\n",
        "        xlabel : str, optional\n",
        "            Label for x axis.\n",
        "        ylabel : str, optional\n",
        "            Label for y axis.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        fig : Figure\n",
        "            Figure object.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # ensure numpy\n",
        "        signal = np.array(signal)\n",
        "        nch = signal.shape[1]\n",
        "\n",
        "        # check labels\n",
        "        if labels is None:\n",
        "            labels = ['Ch. %d' % i for i in range(nch)]\n",
        "\n",
        "        if nch < nrows:\n",
        "            nrows = nch\n",
        "\n",
        "        ncols = int(np.ceil(nch / float(nrows)))\n",
        "\n",
        "        fig = plt.figure()\n",
        "\n",
        "        # title\n",
        "        if title is not None:\n",
        "            fig.suptitle(title)\n",
        "\n",
        "        gs = gridspec.GridSpec(nrows, ncols, hspace=0, wspace=0.2)\n",
        "\n",
        "        # reference axes\n",
        "        ax0 = fig.add_subplot(gs[0, 0])\n",
        "        ax0.plot(ts, signal[:, 0], linewidth=MAJOR_LW, label=labels[0])\n",
        "        ymin, ymax = _yscaling(signal[:, 0], alpha=alpha)\n",
        "        ax0.set_ylim(ymin, ymax)\n",
        "        ax0.legend()\n",
        "        ax0.grid()\n",
        "        axs = {(0, 0): ax0}\n",
        "\n",
        "        for i in range(1, nch - 1):\n",
        "            a = i % nrows\n",
        "            b = int(np.floor(i / float(nrows)))\n",
        "            ax = fig.add_subplot(gs[a, b], sharex=ax0)\n",
        "            axs[(a, b)] = ax\n",
        "\n",
        "            ax.plot(ts, signal[:, i], linewidth=MAJOR_LW, label=labels[i])\n",
        "            ymin, ymax = _yscaling(signal[:, i], alpha=alpha)\n",
        "            ax.set_ylim(ymin, ymax)\n",
        "            ax.legend()\n",
        "            ax.grid()\n",
        "\n",
        "        # last plot\n",
        "        i = nch - 1\n",
        "        a = i % nrows\n",
        "        b = int(np.floor(i / float(nrows)))\n",
        "        ax = fig.add_subplot(gs[a, b], sharex=ax0)\n",
        "        axs[(a, b)] = ax\n",
        "\n",
        "        ax.plot(ts, signal[:, -1], linewidth=MAJOR_LW, label=labels[-1])\n",
        "        ymin, ymax = _yscaling(signal[:, -1], alpha=alpha)\n",
        "        ax.set_ylim(ymin, ymax)\n",
        "        ax.legend()\n",
        "        ax.grid()\n",
        "\n",
        "        if xlabel is not None:\n",
        "            ax.set_xlabel(xlabel)\n",
        "\n",
        "            for b in range(0, ncols - 1):\n",
        "                a = nrows - 1\n",
        "                ax = axs[(a, b)]\n",
        "                ax.set_xlabel(xlabel)\n",
        "\n",
        "        if ylabel is not None:\n",
        "            # middle left\n",
        "            a = nrows // 2\n",
        "            ax = axs[(a, 0)]\n",
        "            ax.set_ylabel(ylabel)\n",
        "\n",
        "        # make layout tight\n",
        "        gs.tight_layout(fig)\n",
        "\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_ecg(ts=None,\n",
        "                 raw=None,\n",
        "                 filtered=None,\n",
        "                 rpeaks=None,\n",
        "                 templates_ts=None,\n",
        "                 templates=None,\n",
        "                 heart_rate_ts=None,\n",
        "                 heart_rate=None,\n",
        "                 path=None,\n",
        "                 show=False):\n",
        "        \"\"\"Create a summary plot from the output of signals.ecg.ecg.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ts : array\n",
        "            Signal time axis reference (seconds).\n",
        "        raw : array\n",
        "            Raw ECG signal.\n",
        "        filtered : array\n",
        "            Filtered ECG signal.\n",
        "        rpeaks : array\n",
        "            R-peak location indices.\n",
        "        templates_ts : array\n",
        "            Templates time axis reference (seconds).\n",
        "        templates : array\n",
        "            Extracted heartbeat templates.\n",
        "        heart_rate_ts : array\n",
        "            Heart rate time axis reference (seconds).\n",
        "        heart_rate : array\n",
        "            Instantaneous heart rate (bpm).\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        fig = plt.figure()\n",
        "        fig.suptitle('ECG Summary')\n",
        "        gs = gridspec.GridSpec(6, 2)\n",
        "\n",
        "        # raw signal\n",
        "        ax1 = fig.add_subplot(gs[:2, 0])\n",
        "\n",
        "        ax1.plot(ts, raw, linewidth=MAJOR_LW, label='Raw')\n",
        "\n",
        "        ax1.set_ylabel('Amplitude')\n",
        "        ax1.legend()\n",
        "        ax1.grid()\n",
        "\n",
        "        # filtered signal with rpeaks\n",
        "        ax2 = fig.add_subplot(gs[2:4, 0], sharex=ax1)\n",
        "\n",
        "        ymin = np.min(filtered)\n",
        "        ymax = np.max(filtered)\n",
        "        alpha = 0.1 * (ymax - ymin)\n",
        "        ymax += alpha\n",
        "        ymin -= alpha\n",
        "\n",
        "        ax2.plot(ts, filtered, linewidth=MAJOR_LW, label='Filtered')\n",
        "        ax2.vlines(ts[rpeaks], ymin, ymax,\n",
        "                   color='m',\n",
        "                   linewidth=MINOR_LW,\n",
        "                   label='R-peaks')\n",
        "\n",
        "        ax2.set_ylabel('Amplitude')\n",
        "        ax2.legend()\n",
        "        ax2.grid()\n",
        "\n",
        "        # heart rate\n",
        "        ax3 = fig.add_subplot(gs[4:, 0], sharex=ax1)\n",
        "\n",
        "        ax3.plot(heart_rate_ts, heart_rate, linewidth=MAJOR_LW, label='Heart Rate')\n",
        "\n",
        "        ax3.set_xlabel('Time (s)')\n",
        "        ax3.set_ylabel('Heart Rate (bpm)')\n",
        "        ax3.legend()\n",
        "        ax3.grid()\n",
        "\n",
        "        # templates\n",
        "        ax4 = fig.add_subplot(gs[1:5, 1])\n",
        "\n",
        "        ax4.plot(templates_ts, templates.T, 'm', linewidth=MINOR_LW, alpha=0.7)\n",
        "\n",
        "        ax4.set_xlabel('Time (s)')\n",
        "        ax4.set_ylabel('Amplitude')\n",
        "        ax4.set_title('Templates')\n",
        "        ax4.grid()\n",
        "\n",
        "        # make layout tight\n",
        "        gs.tight_layout(fig)\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def _plot_rates(thresholds, rates, variables,\n",
        "                    lw=1,\n",
        "                    colors=None,\n",
        "                    alpha=1,\n",
        "                    eer_idx=None,\n",
        "                    labels=False,\n",
        "                    ax=None):\n",
        "        \"\"\"Plot biometric rates.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        thresholds : array\n",
        "            Classifier thresholds.\n",
        "        rates : dict\n",
        "            Dictionary of rates.\n",
        "        variables : list\n",
        "            Keys from 'rates' to plot.\n",
        "        lw : int, float, optional\n",
        "            Plot linewidth.\n",
        "        colors : list, optional\n",
        "            Plot line color for each variable.\n",
        "        alpha : float, optional\n",
        "            Plot line alpha value.\n",
        "        eer_idx : int, optional\n",
        "            Classifier reference index for the Equal Error Rate.\n",
        "        labels : bool, optional\n",
        "            If True, will show plot labels.\n",
        "        ax : axis, optional\n",
        "            Plot Axis to use.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        fig : Figure\n",
        "            Figure object.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if ax is None:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(111)\n",
        "        else:\n",
        "            fig = ax.figure\n",
        "\n",
        "        if colors is None:\n",
        "            x = np.linspace(0., 1., len(variables))\n",
        "            colors = plt.get_cmap('rainbow')(x)\n",
        "\n",
        "        if labels:\n",
        "            for i, v in enumerate(variables):\n",
        "                ax.plot(thresholds, rates[v], colors[i],\n",
        "                        lw=lw,\n",
        "                        alpha=alpha,\n",
        "                        label=v)\n",
        "        else:\n",
        "            for i, v in enumerate(variables):\n",
        "                ax.plot(thresholds, rates[v], colors[i], lw=lw, alpha=alpha)\n",
        "\n",
        "        if eer_idx is not None:\n",
        "            x, y = rates['EER'][eer_idx]\n",
        "            ax.vlines(x, 0, 1, 'r', lw=lw)\n",
        "            ax.set_title('EER = %0.2f %%' % (100. * y))\n",
        "\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_biometrics(assessment=None, eer_idx=None, path=None, show=False):\n",
        "        \"\"\"Create a summary plot of a biometrics test run.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        assessment : dict\n",
        "            Classification assessment results.\n",
        "        eer_idx : int, optional\n",
        "            Classifier reference index for the Equal Error Rate.\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        fig = plt.figure()\n",
        "        fig.suptitle('Biometrics Summary')\n",
        "\n",
        "        c_sub = ['#008bff', '#8dd000']\n",
        "        c_global = ['#0037ff', 'g']\n",
        "\n",
        "        ths = assessment['thresholds']\n",
        "\n",
        "        auth_ax = fig.add_subplot(121)\n",
        "        id_ax = fig.add_subplot(122)\n",
        "\n",
        "        # subject results\n",
        "        for sub in six.iterkeys(assessment['subject']):\n",
        "            auth_rates = assessment['subject'][sub]['authentication']['rates']\n",
        "            _ = _plot_rates(ths, auth_rates, ['FAR', 'FRR'],\n",
        "                            lw=MINOR_LW,\n",
        "                            colors=c_sub,\n",
        "                            alpha=0.4,\n",
        "                            eer_idx=None,\n",
        "                            labels=False,\n",
        "                            ax=auth_ax)\n",
        "\n",
        "            id_rates = assessment['subject'][sub]['identification']['rates']\n",
        "            _ = _plot_rates(ths, id_rates, ['MR', 'RR'],\n",
        "                            lw=MINOR_LW,\n",
        "                            colors=c_sub,\n",
        "                            alpha=0.4,\n",
        "                            eer_idx=None,\n",
        "                            labels=False,\n",
        "                            ax=id_ax)\n",
        "\n",
        "        # global results\n",
        "        auth_rates = assessment['global']['authentication']['rates']\n",
        "        _ = _plot_rates(ths, auth_rates, ['FAR', 'FRR'],\n",
        "                        lw=MAJOR_LW,\n",
        "                        colors=c_global,\n",
        "                        alpha=1,\n",
        "                        eer_idx=eer_idx,\n",
        "                        labels=True,\n",
        "                        ax=auth_ax)\n",
        "\n",
        "        id_rates = assessment['global']['identification']['rates']\n",
        "        _ = _plot_rates(ths, id_rates, ['MR', 'RR'],\n",
        "                        lw=MAJOR_LW,\n",
        "                        colors=c_global,\n",
        "                        alpha=1,\n",
        "                        eer_idx=eer_idx,\n",
        "                        labels=True,\n",
        "                        ax=id_ax)\n",
        "\n",
        "        # set labels and grids\n",
        "        auth_ax.set_xlabel('Threshold')\n",
        "        auth_ax.set_ylabel('Authentication')\n",
        "        auth_ax.grid()\n",
        "        auth_ax.legend()\n",
        "\n",
        "        id_ax.set_xlabel('Threshold')\n",
        "        id_ax.set_ylabel('Identification')\n",
        "        id_ax.grid()\n",
        "        id_ax.legend()\n",
        "\n",
        "        # make layout tight\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_clustering(data=None, clusters=None, path=None, show=False):\n",
        "        \"\"\"Create a summary plot of a data clustering.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : array\n",
        "            An m by n array of m data samples in an n-dimensional space.\n",
        "        clusters : dict\n",
        "            Dictionary with the sample indices (rows from `data`) for each cluster.\n",
        "        path : str, optional\n",
        "            If provided, the plot will be saved to the specified file.\n",
        "        show : bool, optional\n",
        "            If True, show the plot immediately.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        fig = plt.figure()\n",
        "        fig.suptitle('Clustering Summary')\n",
        "\n",
        "        ymin, ymax = _yscaling(data, alpha=1.2)\n",
        "\n",
        "        # determine number of clusters\n",
        "        keys = list(clusters)\n",
        "        nc = len(keys)\n",
        "\n",
        "        if nc <= 4:\n",
        "            nrows = 2\n",
        "            ncols = 4\n",
        "        else:\n",
        "            area = nc + 4\n",
        "\n",
        "            # try to fit to a square\n",
        "            nrows = int(np.ceil(np.sqrt(area)))\n",
        "\n",
        "            if nrows > MAX_ROWS:\n",
        "                # prefer to increase number of columns\n",
        "                nrows = MAX_ROWS\n",
        "\n",
        "            ncols = int(np.ceil(area / float(nrows)))\n",
        "\n",
        "        # plot grid\n",
        "        gs = gridspec.GridSpec(nrows, ncols, hspace=0.2, wspace=0.2)\n",
        "\n",
        "        # global axes\n",
        "        ax_global = fig.add_subplot(gs[:2, :2])\n",
        "\n",
        "        # cluster axes\n",
        "        c_grid = np.ones((nrows, ncols), dtype='bool')\n",
        "        c_grid[:2, :2] = False\n",
        "        c_rows, c_cols = np.nonzero(c_grid)\n",
        "\n",
        "        # generate color map\n",
        "        x = np.linspace(0., 1., nc)\n",
        "        cmap = plt.get_cmap('rainbow')\n",
        "\n",
        "        for i, k in enumerate(keys):\n",
        "            aux = data[clusters[k]]\n",
        "            color = cmap(x[i])\n",
        "            label = 'Cluster %s' % k\n",
        "            ax = fig.add_subplot(gs[c_rows[i], c_cols[i]], sharex=ax_global)\n",
        "            ax.set_ylim([ymin, ymax])\n",
        "            ax.set_title(label)\n",
        "            ax.grid()\n",
        "\n",
        "            if len(aux) > 0:\n",
        "                ax_global.plot(aux.T, color=color, lw=MINOR_LW, alpha=0.7)\n",
        "                ax.plot(aux.T, color=color, lw=MAJOR_LW)\n",
        "\n",
        "        ax_global.set_title('All Clusters')\n",
        "        ax_global.set_ylim([ymin, ymax])\n",
        "        ax_global.grid()\n",
        "\n",
        "        # make layout tight\n",
        "        gs.tight_layout(fig)\n",
        "\n",
        "        # save to file\n",
        "        if path is not None:\n",
        "            path = utils.normpath(path)\n",
        "            root, ext = os.path.splitext(path)\n",
        "            ext = ext.lower()\n",
        "            if ext not in ['png', 'jpg']:\n",
        "                path = root + '.png'\n",
        "\n",
        "            fig.savefig(path, dpi=200, bbox_inches='tight')\n",
        "\n",
        "        # show\n",
        "        if show:\n",
        "            plt.show()\n",
        "        else:\n",
        "            # close\n",
        "            plt.close(fig)\n",
        "\n",
        "plotting = Plotting\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "biosppy.signals.ecg\n",
        "-------------------\n",
        "\n",
        "This module provides methods to process Electrocardiographic (ECG) signals.\n",
        "Implemented code assumes a single-channel Lead I like ECG signal.\n",
        "\n",
        ":copyright: (c) 2015-2018 by Instituto de Telecomunicacoes\n",
        ":license: BSD 3-clause, see LICENSE for more details.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def ecg(signal=None, sampling_rate=1000., show=True):\n",
        "    \"\"\"Process a raw ECG signal and extract relevant signal features using\n",
        "    default parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Raw ECG signal.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "    show : bool, optional\n",
        "        If True, show a summary plot.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ts : array\n",
        "        Signal time axis reference (seconds).\n",
        "    filtered : array\n",
        "        Filtered ECG signal.\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "    templates_ts : array\n",
        "        Templates time axis reference (seconds).\n",
        "    templates : array\n",
        "        Extracted heartbeat templates.\n",
        "    heart_rate_ts : array\n",
        "        Heart rate time axis reference (seconds).\n",
        "    heart_rate : array\n",
        "        Instantaneous heart rate (bpm).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    # ensure numpy\n",
        "    signal = np.array(signal)\n",
        "\n",
        "    sampling_rate = float(sampling_rate)\n",
        "\n",
        "    # filter signal\n",
        "    order = int(0.3 * sampling_rate)\n",
        "    filtered, _, _ = st.filter_signal(signal=signal,\n",
        "                                      ftype='FIR',\n",
        "                                      band='bandpass',\n",
        "                                      order=order,\n",
        "                                      frequency=[3, 45],\n",
        "                                      sampling_rate=sampling_rate)\n",
        "\n",
        "    # segment\n",
        "    rpeaks, = hamilton_segmenter(signal=filtered, sampling_rate=sampling_rate)\n",
        "\n",
        "    # correct R-peak locations\n",
        "    rpeaks, = correct_rpeaks(signal=filtered,\n",
        "                             rpeaks=rpeaks,\n",
        "                             sampling_rate=sampling_rate,\n",
        "                             tol=0.05)\n",
        "\n",
        "    # extract templates\n",
        "    templates, rpeaks = extract_heartbeats(signal=filtered,\n",
        "                                           rpeaks=rpeaks,\n",
        "                                           sampling_rate=sampling_rate,\n",
        "                                           before=0.2,\n",
        "                                           after=0.4)\n",
        "\n",
        "    # compute heart rate\n",
        "    hr_idx, hr = st.get_heart_rate(beats=rpeaks,\n",
        "                                   sampling_rate=sampling_rate,\n",
        "                                   smooth=True,\n",
        "                                   size=3)\n",
        "\n",
        "    # get time vectors\n",
        "    length = len(signal)\n",
        "    T = (length - 1) / sampling_rate\n",
        "    ts = np.linspace(0, T, length, endpoint=False)\n",
        "    ts_hr = ts[hr_idx]\n",
        "    ts_tmpl = np.linspace(-0.2, 0.4, templates.shape[1], endpoint=False)\n",
        "\n",
        "    # plot\n",
        "    if show:\n",
        "        plotting.plot_ecg(ts=ts,\n",
        "                          raw=signal,\n",
        "                          filtered=filtered,\n",
        "                          rpeaks=rpeaks,\n",
        "                          templates_ts=ts_tmpl,\n",
        "                          templates=templates,\n",
        "                          heart_rate_ts=ts_hr,\n",
        "                          heart_rate=hr,\n",
        "                          path=None,\n",
        "                          show=True)\n",
        "\n",
        "    # output\n",
        "    args = (ts, filtered, rpeaks, ts_tmpl, templates, ts_hr, hr)\n",
        "    names = ('ts', 'filtered', 'rpeaks', 'templates_ts', 'templates',\n",
        "             'heart_rate_ts', 'heart_rate')\n",
        "\n",
        "    return utils.ReturnTuple(args, names)\n",
        "\n",
        "\n",
        "def _extract_heartbeats(signal=None, rpeaks=None, before=200, after=400):\n",
        "    \"\"\"Extract heartbeat templates from an ECG signal, given a list of\n",
        "    R-peak locations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Input ECG signal.\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "    before : int, optional\n",
        "        Number of samples to include before the R peak.\n",
        "    after : int, optional\n",
        "        Number of samples to include after the R peak.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    templates : array\n",
        "        Extracted heartbeat templates.\n",
        "    rpeaks : array\n",
        "        Corresponding R-peak location indices of the extracted heartbeat\n",
        "        templates.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    R = np.sort(rpeaks)\n",
        "    length = len(signal)\n",
        "    templates = []\n",
        "    newR = []\n",
        "\n",
        "    for r in R:\n",
        "        a = r - before\n",
        "        if a < 0:\n",
        "            continue\n",
        "        b = r + after\n",
        "        if b > length:\n",
        "            break\n",
        "        templates.append(signal[a:b])\n",
        "        newR.append(r)\n",
        "\n",
        "    templates = np.array(templates)\n",
        "    newR = np.array(newR, dtype='int')\n",
        "\n",
        "    return templates, newR\n",
        "\n",
        "\n",
        "def extract_heartbeats(signal=None, rpeaks=None, sampling_rate=1000.,\n",
        "                       before=0.2, after=0.4):\n",
        "    \"\"\"Extract heartbeat templates from an ECG signal, given a list of\n",
        "    R-peak locations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Input ECG signal.\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "    before : float, optional\n",
        "        Window size to include before the R peak (seconds).\n",
        "    after : int, optional\n",
        "        Window size to include after the R peak (seconds).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    templates : array\n",
        "        Extracted heartbeat templates.\n",
        "    rpeaks : array\n",
        "        Corresponding R-peak location indices of the extracted heartbeat\n",
        "        templates.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    if rpeaks is None:\n",
        "        raise TypeError(\"Please specify the input R-peak locations.\")\n",
        "\n",
        "    if before < 0:\n",
        "        raise ValueError(\"Please specify a non-negative 'before' value.\")\n",
        "    if after < 0:\n",
        "        raise ValueError(\"Please specify a non-negative 'after' value.\")\n",
        "\n",
        "    # convert delimiters to samples\n",
        "    before = int(before * sampling_rate)\n",
        "    after = int(after * sampling_rate)\n",
        "\n",
        "    # get heartbeats\n",
        "    templates, newR = _extract_heartbeats(signal=signal,\n",
        "                                          rpeaks=rpeaks,\n",
        "                                          before=before,\n",
        "                                          after=after)\n",
        "\n",
        "    return utils.ReturnTuple((templates, newR), ('templates', 'rpeaks'))\n",
        "\n",
        "\n",
        "def compare_segmentation(reference=None, test=None, sampling_rate=1000.,\n",
        "                         offset=0, minRR=None, tol=0.05):\n",
        "    \"\"\"Compare the segmentation performance of a list of R-peak positions\n",
        "    against a reference list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    reference : array\n",
        "        Reference R-peak location indices.\n",
        "    test : array\n",
        "        Test R-peak location indices.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "    offset : int, optional\n",
        "        Constant a priori offset (number of samples) between reference and\n",
        "        test R-peak locations.\n",
        "    minRR : float, optional\n",
        "        Minimum admissible RR interval (seconds).\n",
        "    tol : float, optional\n",
        "        Tolerance between corresponding reference and test R-peak\n",
        "        locations (seconds).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    TP : int\n",
        "        Number of true positive R-peaks.\n",
        "    FP : int\n",
        "        Number of false positive R-peaks.\n",
        "    performance : float\n",
        "        Test performance; TP / len(reference).\n",
        "    acc : float\n",
        "        Accuracy rate; TP / (TP + FP).\n",
        "    err : float\n",
        "        Error rate; FP / (TP + FP).\n",
        "    match : list\n",
        "        Indices of the elements of 'test' that match to an R-peak\n",
        "        from 'reference'.\n",
        "    deviation : array\n",
        "        Absolute errors of the matched R-peaks (seconds).\n",
        "    mean_deviation : float\n",
        "        Mean error (seconds).\n",
        "    std_deviation : float\n",
        "        Standard deviation of error (seconds).\n",
        "    mean_ref_ibi : float\n",
        "        Mean of the reference interbeat intervals (seconds).\n",
        "    std_ref_ibi : float\n",
        "        Standard deviation of the reference interbeat intervals (seconds).\n",
        "    mean_test_ibi : float\n",
        "        Mean of the test interbeat intervals (seconds).\n",
        "    std_test_ibi : float\n",
        "        Standard deviation of the test interbeat intervals (seconds).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if reference is None:\n",
        "        raise TypeError(\"Please specify an input reference list of R-peak \\\n",
        "                        locations.\")\n",
        "\n",
        "    if test is None:\n",
        "        raise TypeError(\"Please specify an input test list of R-peak \\\n",
        "                        locations.\")\n",
        "\n",
        "    if minRR is None:\n",
        "        minRR = np.inf\n",
        "\n",
        "    sampling_rate = float(sampling_rate)\n",
        "\n",
        "    # ensure numpy\n",
        "    reference = np.array(reference)\n",
        "    test = np.array(test)\n",
        "\n",
        "    # convert to samples\n",
        "    minRR = minRR * sampling_rate\n",
        "    tol = tol * sampling_rate\n",
        "\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "\n",
        "    matchIdx = []\n",
        "    dev = []\n",
        "\n",
        "    for i, r in enumerate(test):\n",
        "        # deviation to closest R in reference\n",
        "        ref = reference[np.argmin(np.abs(reference - (r + offset)))]\n",
        "        error = np.abs(ref - (r + offset))\n",
        "\n",
        "        if error < tol:\n",
        "            TP += 1\n",
        "            matchIdx.append(i)\n",
        "            dev.append(error)\n",
        "        else:\n",
        "            if len(matchIdx) > 0:\n",
        "                bdf = r - test[matchIdx[-1]]\n",
        "                if bdf < minRR:\n",
        "                    # false positive, but removable with RR interval check\n",
        "                    pass\n",
        "                else:\n",
        "                    FP += 1\n",
        "            else:\n",
        "                FP += 1\n",
        "\n",
        "    # convert deviations to time\n",
        "    dev = np.array(dev, dtype='float')\n",
        "    dev /= sampling_rate\n",
        "    nd = len(dev)\n",
        "    if nd == 0:\n",
        "        mdev = np.nan\n",
        "        sdev = np.nan\n",
        "    elif nd == 1:\n",
        "        mdev = np.mean(dev)\n",
        "        sdev = 0.\n",
        "    else:\n",
        "        mdev = np.mean(dev)\n",
        "        sdev = np.std(dev, ddof=1)\n",
        "\n",
        "    # interbeat interval\n",
        "    th1 = 1.5  # 40 bpm\n",
        "    th2 = 0.3  # 200 bpm\n",
        "\n",
        "    rIBI = np.diff(reference)\n",
        "    rIBI = np.array(rIBI, dtype='float')\n",
        "    rIBI /= sampling_rate\n",
        "\n",
        "    good = np.nonzero((rIBI < th1) & (rIBI > th2))[0]\n",
        "    rIBI = rIBI[good]\n",
        "\n",
        "    nr = len(rIBI)\n",
        "    if nr == 0:\n",
        "        rIBIm = np.nan\n",
        "        rIBIs = np.nan\n",
        "    elif nr == 1:\n",
        "        rIBIm = np.mean(rIBI)\n",
        "        rIBIs = 0.\n",
        "    else:\n",
        "        rIBIm = np.mean(rIBI)\n",
        "        rIBIs = np.std(rIBI, ddof=1)\n",
        "\n",
        "    tIBI = np.diff(test[matchIdx])\n",
        "    tIBI = np.array(tIBI, dtype='float')\n",
        "    tIBI /= sampling_rate\n",
        "\n",
        "    good = np.nonzero((tIBI < th1) & (tIBI > th2))[0]\n",
        "    tIBI = tIBI[good]\n",
        "\n",
        "    nt = len(tIBI)\n",
        "    if nt == 0:\n",
        "        tIBIm = np.nan\n",
        "        tIBIs = np.nan\n",
        "    elif nt == 1:\n",
        "        tIBIm = np.mean(tIBI)\n",
        "        tIBIs = 0.\n",
        "    else:\n",
        "        tIBIm = np.mean(tIBI)\n",
        "        tIBIs = np.std(tIBI, ddof=1)\n",
        "\n",
        "    # output\n",
        "    perf = float(TP) / len(reference)\n",
        "    acc = float(TP) / (TP + FP)\n",
        "    err = float(FP) / (TP + FP)\n",
        "\n",
        "    args = (TP, FP, perf, acc, err, matchIdx, dev, mdev, sdev, rIBIm, rIBIs,\n",
        "            tIBIm, tIBIs)\n",
        "    names = ('TP', 'FP', 'performance', 'acc', 'err', 'match', 'deviation',\n",
        "             'mean_deviation', 'std_deviation', 'mean_ref_ibi', 'std_ref_ibi',\n",
        "             'mean_test_ibi', 'std_test_ibi',)\n",
        "\n",
        "    return utils.ReturnTuple(args, names)\n",
        "\n",
        "\n",
        "def correct_rpeaks(signal=None, rpeaks=None, sampling_rate=1000., tol=0.05):\n",
        "    \"\"\"Correct R-peak locations to the maximum within a tolerance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        ECG signal.\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "    tol : int, float, optional\n",
        "        Correction tolerance (seconds).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rpeaks : array\n",
        "        Cerrected R-peak location indices.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    * The tolerance is defined as the time interval :math:`[R-tol, R+tol[`.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    if rpeaks is None:\n",
        "        raise TypeError(\"Please specify the input R-peaks.\")\n",
        "\n",
        "    tol = int(tol * sampling_rate)\n",
        "    length = len(signal)\n",
        "\n",
        "    newR = []\n",
        "    for r in rpeaks:\n",
        "        a = r - tol\n",
        "        if a < 0:\n",
        "            continue\n",
        "        b = r + tol\n",
        "        if b > length:\n",
        "            break\n",
        "        newR.append(a + np.argmax(signal[a:b]))\n",
        "\n",
        "    newR = sorted(list(set(newR)))\n",
        "    newR = np.array(newR, dtype='int')\n",
        "\n",
        "    return utils.ReturnTuple((newR,), ('rpeaks',))\n",
        "\n",
        "\n",
        "def ssf_segmenter(signal=None, sampling_rate=1000., threshold=20, before=0.03,\n",
        "                  after=0.01):\n",
        "    \"\"\"ECG R-peak segmentation based on the Slope Sum Function (SSF).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Input filtered ECG signal.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "    threshold : float, optional\n",
        "        SSF threshold.\n",
        "    before : float, optional\n",
        "        Search window size before R-peak candidate (seconds).\n",
        "    after : float, optional\n",
        "        Search window size after R-peak candidate (seconds).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    # convert to samples\n",
        "    winB = int(before * sampling_rate)\n",
        "    winA = int(after * sampling_rate)\n",
        "\n",
        "    Rset = set()\n",
        "    length = len(signal)\n",
        "\n",
        "    # diff\n",
        "    dx = np.diff(signal)\n",
        "    dx[dx >= 0] = 0\n",
        "    dx = dx ** 2\n",
        "\n",
        "    # detection\n",
        "    idx, = np.nonzero(dx > threshold)\n",
        "    idx0 = np.hstack(([0], idx))\n",
        "    didx = np.diff(idx0)\n",
        "\n",
        "    # search\n",
        "    sidx = idx[didx > 1]\n",
        "    for item in sidx:\n",
        "        a = item - winB\n",
        "        if a < 0:\n",
        "            a = 0\n",
        "        b = item + winA\n",
        "        if b > length:\n",
        "            continue\n",
        "\n",
        "        r = np.argmax(signal[a:b]) + a\n",
        "        Rset.add(r)\n",
        "\n",
        "    # output\n",
        "    rpeaks = list(Rset)\n",
        "    rpeaks.sort()\n",
        "    rpeaks = np.array(rpeaks, dtype='int')\n",
        "\n",
        "    return utils.ReturnTuple((rpeaks,), ('rpeaks',))\n",
        "\n",
        "\n",
        "def christov_segmenter(signal=None, sampling_rate=1000.):\n",
        "    \"\"\"ECG R-peak segmentation algorithm.\n",
        "\n",
        "    Follows the approach by Christov [Chri04]_.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Input filtered ECG signal.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [Chri04] Ivaylo I. Christov, \"Real time electrocardiogram QRS\n",
        "       detection using combined adaptive threshold\", BioMedical Engineering\n",
        "       OnLine 2004, vol. 3:28, 2004\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    length = len(signal)\n",
        "\n",
        "    # algorithm parameters\n",
        "    v100ms = int(0.1 * sampling_rate)\n",
        "    v50ms = int(0.050 * sampling_rate)\n",
        "    v300ms = int(0.300 * sampling_rate)\n",
        "    v350ms = int(0.350 * sampling_rate)\n",
        "    v200ms = int(0.2 * sampling_rate)\n",
        "    v1200ms = int(1.2 * sampling_rate)\n",
        "    M_th = 0.4  # paper is 0.6\n",
        "\n",
        "    # Pre-processing\n",
        "    # 1. Moving averaging filter for power-line interference suppression:\n",
        "    # averages samples in one period of the powerline\n",
        "    # interference frequency with a first zero at this frequency.\n",
        "    b = np.ones(int(0.02 * sampling_rate)) / 50.\n",
        "    a = [1]\n",
        "    X = ss.filtfilt(b, a, signal)\n",
        "    # 2. Moving averaging of samples in 28 ms interval for electromyogram\n",
        "    # noise suppression a filter with first zero at about 35 Hz.\n",
        "    b = np.ones(int(sampling_rate / 35.)) / 35.\n",
        "    X = ss.filtfilt(b, a, X)\n",
        "    X, _, _ = st.filter_signal(signal=X,\n",
        "                               ftype='butter',\n",
        "                               band='lowpass',\n",
        "                               order=7,\n",
        "                               frequency=40.,\n",
        "                               sampling_rate=sampling_rate)\n",
        "    X, _, _ = st.filter_signal(signal=X,\n",
        "                               ftype='butter',\n",
        "                               band='highpass',\n",
        "                               order=7,\n",
        "                               frequency=9.,\n",
        "                               sampling_rate=sampling_rate)\n",
        "\n",
        "    k, Y, L = 1, [], len(X)\n",
        "    for n in range(k + 1, L - k):\n",
        "        Y.append(X[n] ** 2 - X[n - k] * X[n + k])\n",
        "    Y = np.array(Y)\n",
        "    Y[Y < 0] = 0\n",
        "\n",
        "    # Complex lead\n",
        "    # Y = abs(scipy.diff(X)) # 1-lead\n",
        "    # 3. Moving averaging of a complex lead (the sintesis is\n",
        "    # explained in the next section) in 40 ms intervals a filter\n",
        "    # with first zero at about 25 Hz. It is suppressing the noise\n",
        "    # magnified by the differentiation procedure used in the\n",
        "    # process of the complex lead sintesis.\n",
        "    b = np.ones(int(sampling_rate / 25.)) / 25.\n",
        "    Y = ss.lfilter(b, a, Y)\n",
        "\n",
        "    # Init\n",
        "    MM = M_th * np.max(Y[:int(5 * sampling_rate)]) * np.ones(5)\n",
        "    MMidx = 0\n",
        "    M = np.mean(MM)\n",
        "    slope = np.linspace(1.0, 0.6, int(sampling_rate))\n",
        "    Rdec = 0\n",
        "    R = 0\n",
        "    RR = np.zeros(5)\n",
        "    RRidx = 0\n",
        "    Rm = 0\n",
        "    QRS = []\n",
        "    Rpeak = []\n",
        "    current_sample = 0\n",
        "    skip = False\n",
        "    F = np.mean(Y[:v350ms])\n",
        "\n",
        "    # Go through each sample\n",
        "    while current_sample < len(Y):\n",
        "        if QRS:\n",
        "            # No detection is allowed 200 ms after the current one. In\n",
        "            # the interval QRS to QRS+200ms a new value of M5 is calculated: newM5 = 0.6*max(Yi)\n",
        "            if current_sample <= QRS[-1] + v200ms:\n",
        "                Mnew = M_th * max(Y[QRS[-1]:QRS[-1] + v200ms])\n",
        "                # The estimated newM5 value can become quite high, if\n",
        "                # steep slope premature ventricular contraction or artifact\n",
        "                # appeared, and for that reason it is limited to newM5 = 1.1*M5 if newM5 > 1.5* M5\n",
        "                # The MM buffer is refreshed excluding the oldest component, and including M5 = newM5.\n",
        "                Mnew = Mnew if Mnew <= 1.5 * MM[MMidx - 1] else 1.1 * MM[MMidx - 1]\n",
        "                MM[MMidx] = Mnew\n",
        "                MMidx = np.mod(MMidx + 1, 5)\n",
        "                # M is calculated as an average value of MM.\n",
        "                Mtemp = np.mean(MM)\n",
        "                M = Mtemp\n",
        "                skip = True\n",
        "            # M is decreased in an interval 200 to 1200 ms following\n",
        "            # the last QRS detection at a low slope, reaching 60 % of its\n",
        "            # refreshed value at 1200 ms.\n",
        "            elif current_sample >= QRS[-1] + v200ms and current_sample < QRS[-1] + v1200ms:\n",
        "                M = Mtemp * slope[current_sample - QRS[-1] - v200ms]\n",
        "            # After 1200 ms M remains unchanged.\n",
        "            # R = 0 V in the interval from the last detected QRS to 2/3 of the expected Rm.\n",
        "            if current_sample >= QRS[-1] and current_sample < QRS[-1] + (2 / 3.) * Rm:\n",
        "                R = 0\n",
        "            # In the interval QRS + Rm * 2/3 to QRS + Rm, R decreases\n",
        "            # 1.4 times slower then the decrease of the previously discussed\n",
        "            # steep slope threshold (M in the 200 to 1200 ms interval).\n",
        "            elif current_sample >= QRS[-1] + (2 / 3.) * Rm and current_sample < QRS[-1] + Rm:\n",
        "                R += Rdec\n",
        "            # After QRS + Rm the decrease of R is stopped\n",
        "            # MFR = M + F + R\n",
        "        MFR = M + F + R\n",
        "        # QRS or beat complex is detected if Yi = MFR\n",
        "        if not skip and Y[current_sample] >= MFR:\n",
        "            QRS += [current_sample]\n",
        "            Rpeak += [QRS[-1] + np.argmax(Y[QRS[-1]:QRS[-1] + v300ms])]\n",
        "            if len(QRS) >= 2:\n",
        "                # A buffer with the 5 last RR intervals is updated at any new QRS detection.\n",
        "                RR[RRidx] = QRS[-1] - QRS[-2]\n",
        "                RRidx = np.mod(RRidx + 1, 5)\n",
        "        skip = False\n",
        "        # With every signal sample, F is updated adding the maximum\n",
        "        # of Y in the latest 50 ms of the 350 ms interval and\n",
        "        # subtracting maxY in the earliest 50 ms of the interval.\n",
        "        if current_sample >= v350ms:\n",
        "            Y_latest50 = Y[current_sample - v50ms:current_sample]\n",
        "            Y_earliest50 = Y[current_sample - v350ms:current_sample - v300ms]\n",
        "            F += (max(Y_latest50) - max(Y_earliest50)) / 1000.\n",
        "        # Rm is the mean value of the buffer RR.\n",
        "        Rm = np.mean(RR)\n",
        "        current_sample += 1\n",
        "\n",
        "    rpeaks = []\n",
        "    for i in Rpeak:\n",
        "        a, b = i - v100ms, i + v100ms\n",
        "        if a < 0:\n",
        "            a = 0\n",
        "        if b > length:\n",
        "            b = length\n",
        "        rpeaks.append(np.argmax(signal[a:b]) + a)\n",
        "\n",
        "    rpeaks = sorted(list(set(rpeaks)))\n",
        "    rpeaks = np.array(rpeaks, dtype='int')\n",
        "\n",
        "    return utils.ReturnTuple((rpeaks,), ('rpeaks',))\n",
        "\n",
        "\n",
        "def engzee_segmenter(signal=None, sampling_rate=1000., threshold=0.48):\n",
        "    \"\"\"ECG R-peak segmentation algorithm.\n",
        "\n",
        "    Follows the approach by Engelse and Zeelenberg [EnZe79]_ with the\n",
        "    modifications by Lourenco *et al.* [LSLL12]_.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Input filtered ECG signal.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "    threshold : float, optional\n",
        "        Detection threshold.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [EnZe79] W. Engelse and C. Zeelenberg, \"A single scan algorithm for\n",
        "       QRS detection and feature extraction\", IEEE Comp. in Cardiology,\n",
        "       vol. 6, pp. 37-42, 1979\n",
        "    .. [LSLL12] A. Lourenco, H. Silva, P. Leite, R. Lourenco and A. Fred,\n",
        "       \"Real Time Electrocardiogram Segmentation for Finger Based ECG\n",
        "       Biometrics\", BIOSIGNALS 2012, pp. 49-54, 2012\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    # algorithm parameters\n",
        "    changeM = int(0.75 * sampling_rate)\n",
        "    Miterate = int(1.75 * sampling_rate)\n",
        "    v250ms = int(0.25 * sampling_rate)\n",
        "    v1200ms = int(1.2 * sampling_rate)\n",
        "    v1500ms = int(1.5 * sampling_rate)\n",
        "    v180ms = int(0.18 * sampling_rate)\n",
        "    p10ms = int(np.ceil(0.01 * sampling_rate))\n",
        "    p20ms = int(np.ceil(0.02 * sampling_rate))\n",
        "    err_kill = int(0.01 * sampling_rate)\n",
        "    inc = 1\n",
        "    mmth = threshold\n",
        "    mmp = 0.2\n",
        "\n",
        "    # Differentiator (1)\n",
        "    y1 = [signal[i] - signal[i - 4] for i in range(4, len(signal))]\n",
        "\n",
        "    # Low pass filter (2)\n",
        "    c = [1, 4, 6, 4, 1, -1, -4, -6, -4, -1]\n",
        "    y2 = np.array([np.dot(c, y1[n - 9:n + 1]) for n in range(9, len(y1))])\n",
        "    y2_len = len(y2)\n",
        "\n",
        "    # vars\n",
        "    MM = mmth * max(y2[:Miterate]) * np.ones(3)\n",
        "    MMidx = 0\n",
        "    Th = np.mean(MM)\n",
        "    NN = mmp * min(y2[:Miterate]) * np.ones(2)\n",
        "    NNidx = 0\n",
        "    ThNew = np.mean(NN)\n",
        "    update = False\n",
        "    nthfpluss = []\n",
        "    rpeaks = []\n",
        "\n",
        "    # Find nthf+ point\n",
        "    while True:\n",
        "        # If a previous intersection was found, continue the analysis from there\n",
        "        if update:\n",
        "            if inc * changeM + Miterate < y2_len:\n",
        "                a = (inc - 1) * changeM\n",
        "                b = inc * changeM + Miterate\n",
        "                Mnew = mmth * max(y2[a:b])\n",
        "                Nnew = mmp * min(y2[a:b])\n",
        "            elif y2_len - (inc - 1) * changeM > v1500ms:\n",
        "                a = (inc - 1) * changeM\n",
        "                Mnew = mmth * max(y2[a:])\n",
        "                Nnew = mmp * min(y2[a:])\n",
        "            if len(y2) - inc * changeM > Miterate:\n",
        "                MM[MMidx] = Mnew if Mnew <= 1.5 * MM[MMidx - 1] else 1.1 * MM[MMidx - 1]\n",
        "                NN[NNidx] = Nnew if abs(Nnew) <= 1.5 * abs(NN[NNidx - 1]) else 1.1 * NN[NNidx - 1]\n",
        "            MMidx = np.mod(MMidx + 1, len(MM))\n",
        "            NNidx = np.mod(NNidx + 1, len(NN))\n",
        "            Th = np.mean(MM)\n",
        "            ThNew = np.mean(NN)\n",
        "            inc += 1\n",
        "            update = False\n",
        "        if nthfpluss:\n",
        "            lastp = nthfpluss[-1] + 1\n",
        "            if lastp < (inc - 1) * changeM:\n",
        "                lastp = (inc - 1) * changeM\n",
        "            y22 = y2[lastp:inc * changeM + err_kill]\n",
        "            # find intersection with Th\n",
        "            try:\n",
        "                nthfplus = np.intersect1d(np.nonzero(y22 > Th)[0], np.nonzero(y22 < Th)[0] - 1)[0]\n",
        "            except IndexError:\n",
        "                if inc * changeM > len(y2):\n",
        "                    break\n",
        "                else:\n",
        "                    update = True\n",
        "                    continue\n",
        "            # adjust index\n",
        "            nthfplus += int(lastp)\n",
        "            # if a previous R peak was found:\n",
        "            if rpeaks:\n",
        "                # check if intersection is within the 200-1200 ms interval. Modification: 300 ms -> 200 bpm\n",
        "                if nthfplus - rpeaks[-1] > v250ms and nthfplus - rpeaks[-1] < v1200ms:\n",
        "                    pass\n",
        "                # if new intersection is within the <200ms interval, skip it. Modification: 300 ms -> 200 bpm\n",
        "                elif nthfplus - rpeaks[-1] < v250ms:\n",
        "                    nthfpluss += [nthfplus]\n",
        "                    continue\n",
        "        # no previous intersection, find the first one\n",
        "        else:\n",
        "            try:\n",
        "                aux = np.nonzero(y2[(inc - 1) * changeM:inc * changeM + err_kill] > Th)[0]\n",
        "                bux = np.nonzero(y2[(inc - 1) * changeM:inc * changeM + err_kill] < Th)[0] - 1\n",
        "                nthfplus = int((inc - 1) * changeM) + np.intersect1d(aux, bux)[0]\n",
        "            except IndexError:\n",
        "                if inc * changeM > len(y2):\n",
        "                    break\n",
        "                else:\n",
        "                    update = True\n",
        "                    continue\n",
        "        nthfpluss += [nthfplus]\n",
        "        # Define 160ms search region\n",
        "        windowW = np.arange(nthfplus, nthfplus + v180ms)\n",
        "        # Check if the condition y2[n] < Th holds for a specified\n",
        "        # number of consecutive points (experimentally we found this number to be at least 10 points)\"\n",
        "        i, f = windowW[0], windowW[-1] if windowW[-1] < len(y2) else -1\n",
        "        hold_points = np.diff(np.nonzero(y2[i:f] < ThNew)[0])\n",
        "        cont = 0\n",
        "        for hp in hold_points:\n",
        "            if hp == 1:\n",
        "                cont += 1\n",
        "                if cont == p10ms - 1:  # -1 is because diff eats a sample\n",
        "                    max_shift = p20ms  # looks for X's max a bit to the right\n",
        "                    if nthfpluss[-1] > max_shift:\n",
        "                        rpeaks += [np.argmax(signal[i - max_shift:f]) + i - max_shift]\n",
        "                    else:\n",
        "                        rpeaks += [np.argmax(signal[i:f]) + i]\n",
        "                    break\n",
        "            else:\n",
        "                cont = 0\n",
        "\n",
        "    rpeaks = sorted(list(set(rpeaks)))\n",
        "    rpeaks = np.array(rpeaks, dtype='int')\n",
        "\n",
        "    return utils.ReturnTuple((rpeaks,), ('rpeaks',))\n",
        "\n",
        "\n",
        "def gamboa_segmenter(signal=None, sampling_rate=1000., tol=0.002):\n",
        "    \"\"\"ECG R-peak segmentation algorithm.\n",
        "\n",
        "    Follows the approach by Gamboa.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Input filtered ECG signal.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "    tol : float, optional\n",
        "        Tolerance parameter.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    # convert to samples\n",
        "    v_100ms = int(0.1 * sampling_rate)\n",
        "    v_300ms = int(0.3 * sampling_rate)\n",
        "    hist, edges = np.histogram(signal, 100, density=True)\n",
        "\n",
        "    TH = 0.01\n",
        "    F = np.cumsum(hist)\n",
        "\n",
        "    v0 = edges[np.nonzero(F > TH)[0][0]]\n",
        "    v1 = edges[np.nonzero(F < (1 - TH))[0][-1]]\n",
        "\n",
        "    nrm = max([abs(v0), abs(v1)])\n",
        "    norm_signal = signal / float(nrm)\n",
        "\n",
        "    d2 = np.diff(norm_signal, 2)\n",
        "\n",
        "    b = np.nonzero((np.diff(np.sign(np.diff(-d2)))) == -2)[0] + 2\n",
        "    b = np.intersect1d(b, np.nonzero(-d2 > tol)[0])\n",
        "\n",
        "    if len(b) < 3:\n",
        "        rpeaks = []\n",
        "    else:\n",
        "        b = b.astype('float')\n",
        "        rpeaks = []\n",
        "        previous = b[0]\n",
        "        for i in b[1:]:\n",
        "            if i - previous > v_300ms:\n",
        "                previous = i\n",
        "                rpeaks.append(np.argmax(signal[int(i):int(i + v_100ms)]) + i)\n",
        "\n",
        "    rpeaks = sorted(list(set(rpeaks)))\n",
        "    rpeaks = np.array(rpeaks, dtype='int')\n",
        "\n",
        "    return utils.ReturnTuple((rpeaks,), ('rpeaks',))\n",
        "\n",
        "\n",
        "def hamilton_segmenter(signal=None, sampling_rate=1000.):\n",
        "    \"\"\"ECG R-peak segmentation algorithm.\n",
        "\n",
        "    Follows the approach by Hamilton [Hami02]_.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array\n",
        "        Input filtered ECG signal.\n",
        "    sampling_rate : int, float, optional\n",
        "        Sampling frequency (Hz).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rpeaks : array\n",
        "        R-peak location indices.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [Hami02] P.S. Hamilton, \"Open Source ECG Analysis Software\n",
        "       Documentation\", E.P.Limited, 2002\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # check inputs\n",
        "    if signal is None:\n",
        "        raise TypeError(\"Please specify an input signal.\")\n",
        "\n",
        "    sampling_rate = float(sampling_rate)\n",
        "    length = len(signal)\n",
        "    dur = length / sampling_rate\n",
        "\n",
        "    # algorithm parameters\n",
        "    v1s = int(1. * sampling_rate)\n",
        "    v100ms = int(0.1 * sampling_rate)\n",
        "    TH_elapsed = np.ceil(0.36 * sampling_rate)\n",
        "    sm_size = int(0.08 * sampling_rate)\n",
        "    init_ecg = 8  # seconds for initialization\n",
        "    if dur < init_ecg:\n",
        "        init_ecg = int(dur)\n",
        "\n",
        "    # filtering\n",
        "    filtered, _, _ = st.filter_signal(signal=signal,\n",
        "                                      ftype='butter',\n",
        "                                      band='lowpass',\n",
        "                                      order=4,\n",
        "                                      frequency=25.,\n",
        "                                      sampling_rate=sampling_rate)\n",
        "    filtered, _, _ = st.filter_signal(signal=filtered,\n",
        "                                      ftype='butter',\n",
        "                                      band='highpass',\n",
        "                                      order=4,\n",
        "                                      frequency=3.,\n",
        "                                      sampling_rate=sampling_rate)\n",
        "\n",
        "    # diff\n",
        "    dx = np.abs(np.diff(filtered, 1) * sampling_rate)\n",
        "\n",
        "    # smoothing\n",
        "    dx, _ = st.smoother(signal=dx, kernel='hamming', size=sm_size, mirror=True)\n",
        "\n",
        "    # buffers\n",
        "    qrspeakbuffer = np.zeros(init_ecg)\n",
        "    noisepeakbuffer = np.zeros(init_ecg)\n",
        "    peak_idx_test = np.zeros(init_ecg)\n",
        "    noise_idx = np.zeros(init_ecg)\n",
        "    rrinterval = sampling_rate * np.ones(init_ecg)\n",
        "\n",
        "    a, b = 0, v1s\n",
        "    all_peaks, _ = st.find_extrema(signal=dx, mode='max')\n",
        "    for i in range(init_ecg):\n",
        "        peaks, values = st.find_extrema(signal=dx[a:b], mode='max')\n",
        "        try:\n",
        "            ind = np.argmax(values)\n",
        "        except ValueError:\n",
        "            pass\n",
        "        else:\n",
        "            # peak amplitude\n",
        "            qrspeakbuffer[i] = values[ind]\n",
        "            # peak location\n",
        "            peak_idx_test[i] = peaks[ind] + a\n",
        "\n",
        "        a += v1s\n",
        "        b += v1s\n",
        "\n",
        "    # thresholds\n",
        "    ANP = np.median(noisepeakbuffer)\n",
        "    AQRSP = np.median(qrspeakbuffer)\n",
        "    TH = 0.475\n",
        "    DT = ANP + TH * (AQRSP - ANP)\n",
        "    DT_vec = []\n",
        "    indexqrs = 0\n",
        "    indexnoise = 0\n",
        "    indexrr = 0\n",
        "    npeaks = 0\n",
        "    offset = 0\n",
        "\n",
        "    beats = []\n",
        "\n",
        "    # detection rules\n",
        "    # 1 - ignore all peaks that precede or follow larger peaks by less than 200ms\n",
        "    lim = int(np.ceil(0.2 * sampling_rate))\n",
        "    diff_nr = int(np.ceil(0.045 * sampling_rate))\n",
        "    bpsi, bpe = offset, 0\n",
        "\n",
        "    for f in all_peaks:\n",
        "        DT_vec += [DT]\n",
        "        # 1 - Checking if f-peak is larger than any peak following or preceding it by less than 200 ms\n",
        "        peak_cond = np.array((all_peaks > f - lim) * (all_peaks < f + lim) * (all_peaks != f))\n",
        "        peaks_within = all_peaks[peak_cond]\n",
        "        if peaks_within.any() and (max(dx[peaks_within]) > dx[f]):\n",
        "            continue\n",
        "\n",
        "        # 4 - If the peak is larger than the detection threshold call it a QRS complex, otherwise call it noise\n",
        "        if dx[f] > DT:\n",
        "            # 2 - look for both positive and negative slopes in raw signal\n",
        "            if f < diff_nr:\n",
        "                diff_now = np.diff(signal[0:f + diff_nr])\n",
        "            elif f + diff_nr >= len(signal):\n",
        "                diff_now = np.diff(signal[f - diff_nr:len(dx)])\n",
        "            else:\n",
        "                diff_now = np.diff(signal[f - diff_nr:f + diff_nr])\n",
        "            diff_signer = diff_now[diff_now > 0]\n",
        "            if len(diff_signer) == 0 or len(diff_signer) == len(diff_now):\n",
        "                continue\n",
        "            # RR INTERVALS\n",
        "            if npeaks > 0:\n",
        "                # 3 - in here we check point 3 of the Hamilton paper\n",
        "                # that is, we check whether our current peak is a valid R-peak.\n",
        "                prev_rpeak = beats[npeaks - 1]\n",
        "\n",
        "                elapsed = f - prev_rpeak\n",
        "                # if the previous peak was within 360 ms interval\n",
        "                if elapsed < TH_elapsed:\n",
        "                    # check current and previous slopes\n",
        "                    if prev_rpeak < diff_nr:\n",
        "                        diff_prev = np.diff(signal[0:prev_rpeak + diff_nr])\n",
        "                    elif prev_rpeak + diff_nr >= len(signal):\n",
        "                        diff_prev = np.diff(signal[prev_rpeak - diff_nr:len(dx)])\n",
        "                    else:\n",
        "                        diff_prev = np.diff(signal[prev_rpeak - diff_nr:prev_rpeak + diff_nr])\n",
        "\n",
        "                    slope_now = max(diff_now)\n",
        "                    slope_prev = max(diff_prev)\n",
        "\n",
        "                    if (slope_now < 0.5 * slope_prev):\n",
        "                        # if current slope is smaller than half the previous one, then it is a T-wave\n",
        "                        continue\n",
        "                if dx[f] < 3. * np.median(qrspeakbuffer):  # avoid retarded noise peaks\n",
        "                    beats += [int(f) + bpsi]\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if bpe == 0:\n",
        "                    rrinterval[indexrr] = beats[npeaks] - beats[npeaks - 1]\n",
        "                    indexrr += 1\n",
        "                    if indexrr == init_ecg:\n",
        "                        indexrr = 0\n",
        "                else:\n",
        "                    if beats[npeaks] > beats[bpe - 1] + v100ms:\n",
        "                        rrinterval[indexrr] = beats[npeaks] - beats[npeaks - 1]\n",
        "                        indexrr += 1\n",
        "                        if indexrr == init_ecg:\n",
        "                            indexrr = 0\n",
        "\n",
        "            elif dx[f] < 3. * np.median(qrspeakbuffer):\n",
        "                beats += [int(f) + bpsi]\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            npeaks += 1\n",
        "            qrspeakbuffer[indexqrs] = dx[f]\n",
        "            peak_idx_test[indexqrs] = f\n",
        "            indexqrs += 1\n",
        "            if indexqrs == init_ecg:\n",
        "                indexqrs = 0\n",
        "        if dx[f] <= DT:\n",
        "            # 4 - not valid\n",
        "            # 5 - If no QRS has been detected within 1.5 R-to-R intervals,\n",
        "            # there was a peak that was larger than half the detection threshold,\n",
        "            # and the peak followed the preceding detection by at least 360 ms,\n",
        "            # classify that peak as a QRS complex\n",
        "            tf = f + bpsi\n",
        "            # RR interval median\n",
        "            RRM = np.median(rrinterval)  # initial values are good?\n",
        "\n",
        "            if len(beats) >= 2:\n",
        "                elapsed = tf - beats[npeaks - 1]\n",
        "\n",
        "                if elapsed >= 1.5 * RRM and elapsed > TH_elapsed:\n",
        "                    if dx[f] > 0.5 * DT:\n",
        "                        beats += [int(f) + offset]\n",
        "                        # RR INTERVALS\n",
        "                        if npeaks > 0:\n",
        "                            rrinterval[indexrr] = beats[npeaks] - beats[npeaks - 1]\n",
        "                            indexrr += 1\n",
        "                            if indexrr == init_ecg:\n",
        "                                indexrr = 0\n",
        "                        npeaks += 1\n",
        "                        qrspeakbuffer[indexqrs] = dx[f]\n",
        "                        peak_idx_test[indexqrs] = f\n",
        "                        indexqrs += 1\n",
        "                        if indexqrs == init_ecg:\n",
        "                            indexqrs = 0\n",
        "                else:\n",
        "                    noisepeakbuffer[indexnoise] = dx[f]\n",
        "                    noise_idx[indexnoise] = f\n",
        "                    indexnoise += 1\n",
        "                    if indexnoise == init_ecg:\n",
        "                        indexnoise = 0\n",
        "            else:\n",
        "                noisepeakbuffer[indexnoise] = dx[f]\n",
        "                noise_idx[indexnoise] = f\n",
        "                indexnoise += 1\n",
        "                if indexnoise == init_ecg:\n",
        "                    indexnoise = 0\n",
        "\n",
        "        # Update Detection Threshold\n",
        "        ANP = np.median(noisepeakbuffer)\n",
        "        AQRSP = np.median(qrspeakbuffer)\n",
        "        DT = ANP + 0.475 * (AQRSP - ANP)\n",
        "\n",
        "    beats = np.array(beats)\n",
        "\n",
        "    r_beats = []\n",
        "    thres_ch = 0.85\n",
        "    adjacency = 0.05 * sampling_rate\n",
        "    for i in beats:\n",
        "        error = [False, False]\n",
        "        if i - lim < 0:\n",
        "            window = signal[0:i + lim]\n",
        "            add = 0\n",
        "        elif i + lim >= length:\n",
        "            window = signal[i - lim:length]\n",
        "            add = i - lim\n",
        "        else:\n",
        "            window = signal[i - lim:i + lim]\n",
        "            add = i - lim\n",
        "        # meanval = np.mean(window)\n",
        "        w_peaks, _ = st.find_extrema(signal=window, mode='max')\n",
        "        w_negpeaks, _ = st.find_extrema(signal=window, mode='min')\n",
        "        zerdiffs = np.where(np.diff(window) == 0)[0]\n",
        "        w_peaks = np.concatenate((w_peaks, zerdiffs))\n",
        "        w_negpeaks = np.concatenate((w_negpeaks, zerdiffs))\n",
        "\n",
        "        pospeaks = sorted(zip(window[w_peaks], w_peaks), reverse=True)\n",
        "        negpeaks = sorted(zip(window[w_negpeaks], w_negpeaks))\n",
        "\n",
        "        try:\n",
        "            twopeaks = [pospeaks[0]]\n",
        "        except IndexError:\n",
        "            twopeaks = []\n",
        "        try:\n",
        "            twonegpeaks = [negpeaks[0]]\n",
        "        except IndexError:\n",
        "            twonegpeaks = []\n",
        "\n",
        "        # getting positive peaks\n",
        "        for i in range(len(pospeaks) - 1):\n",
        "            if abs(pospeaks[0][1] - pospeaks[i + 1][1]) > adjacency:\n",
        "                twopeaks.append(pospeaks[i + 1])\n",
        "                break\n",
        "        try:\n",
        "            posdiv = abs(twopeaks[0][0] - twopeaks[1][0])\n",
        "        except IndexError:\n",
        "            error[0] = True\n",
        "\n",
        "        # getting negative peaks\n",
        "        for i in range(len(negpeaks) - 1):\n",
        "            if abs(negpeaks[0][1] - negpeaks[i + 1][1]) > adjacency:\n",
        "                twonegpeaks.append(negpeaks[i + 1])\n",
        "                break\n",
        "        try:\n",
        "            negdiv = abs(twonegpeaks[0][0] - twonegpeaks[1][0])\n",
        "        except IndexError:\n",
        "            error[1] = True\n",
        "\n",
        "        # choosing type of R-peak\n",
        "        n_errors = sum(error)\n",
        "        try:\n",
        "            if not n_errors:\n",
        "                if posdiv > thres_ch * negdiv:\n",
        "                    # pos noerr\n",
        "                    r_beats.append(twopeaks[0][1] + add)\n",
        "                else:\n",
        "                    # neg noerr\n",
        "                    r_beats.append(twonegpeaks[0][1] + add)\n",
        "            elif n_errors == 2:\n",
        "                if abs(twopeaks[0][1]) > abs(twonegpeaks[0][1]):\n",
        "                    # pos allerr\n",
        "                    r_beats.append(twopeaks[0][1] + add)\n",
        "                else:\n",
        "                    # neg allerr\n",
        "                    r_beats.append(twonegpeaks[0][1] + add)\n",
        "            elif error[0]:\n",
        "                # pos poserr\n",
        "                r_beats.append(twopeaks[0][1] + add)\n",
        "            else:\n",
        "                # neg negerr\n",
        "                r_beats.append(twonegpeaks[0][1] + add)\n",
        "        except IndexError:\n",
        "            continue\n",
        "\n",
        "    rpeaks = sorted(list(set(r_beats)))\n",
        "    rpeaks = np.array(rpeaks, dtype='int')\n",
        "\n",
        "    return utils.ReturnTuple((rpeaks,), ('rpeaks',))\n",
        "def extract_rri(signal, ir, CHUNK_DURATION):\n",
        "    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))  # TIME METRIC FOR INTERPOLATION\n",
        "\n",
        "    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * CHAT_FREQ),\n",
        "                                      frequency=[3, 45], sampling_rate=CHAT_FREQ)\n",
        "    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=CHAT_FREQ)\n",
        "    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=CHAT_FREQ, tol=0.05)\n",
        "\n",
        "    if 4 < len(rpeaks) < 200:  # and np.max(signal) < 0.0015 and np.min(signal) > -0.0015:\n",
        "        rri_tm, rri_signal = rpeaks[1:] / float(CHAT_FREQ), np.diff(rpeaks) / float(CHAT_FREQ)\n",
        "        ampl_tm, ampl_signal = rpeaks / float(CHAT_FREQ), signal[rpeaks]\n",
        "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n",
        "        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n",
        "\n",
        "        return np.clip(rri_interp_signal, 0, 2) * 100, np.clip(amp_interp_signal, -0.001, 0.002) * 10000, True\n",
        "    else:\n",
        "        return np.zeros((CHAT_FREQ * CHAT_EPOCH_LENGTH)), np.zeros((CHAT_FREQ * CHAT_EPOCH_LENGTH)), False\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    # demo = pd.read_csv(\"../misc/result.csv\")\n",
        "    # ahi = pd.read_csv(r\"C:\\Data\\AHI.csv\")\n",
        "    # ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n",
        "    root_dir = os.path.expanduser(path)\n",
        "    file_list = os.listdir(root_dir)\n",
        "    length = len(file_list)\n",
        "\n",
        "    ################################### Fold the data based on number of respiratory events #########################\n",
        "    study_event_counts = [i for i in range(0, length)]\n",
        "    folds = []\n",
        "    for i in range(5):\n",
        "        folds.append(study_event_counts[i::5])\n",
        "\n",
        "    x = []\n",
        "    y_apnea = []\n",
        "    y_hypopnea = []\n",
        "    counter = 0\n",
        "    for idx, fold in enumerate(folds):\n",
        "        first = True\n",
        "        for patient in fold:\n",
        "            rri_succ_counter = 0\n",
        "            rri_fail_counter = 0\n",
        "            counter += 1\n",
        "            print(counter)\n",
        "            # for study in glob.glob(PATH + patient[0] + \"_*\"):\n",
        "            study_data = np.load(CHAT_PREPROCESSED_PATH + \"\\\\\" + file_list[patient - 1])\n",
        "\n",
        "            signals = study_data['data']\n",
        "            labels_apnea = study_data['labels_apnea']\n",
        "            labels_hypopnea = study_data['labels_hypopnea']\n",
        "\n",
        "            # identifier = study.split('\\\\')[-1].split('_')[0] + \"_\" + study.split('\\\\')[-1].split('_')[1]\n",
        "            # demo_arr = demo[demo['id'] == identifier].drop(columns=['id']).to_numpy().squeeze()\n",
        "\n",
        "            y_c = labels_apnea + labels_hypopnea\n",
        "            neg_samples = np.where(y_c == 0)[0]\n",
        "            pos_samples = list(np.where(y_c > 0)[0])\n",
        "            ratio = len(pos_samples) / len(neg_samples)\n",
        "            neg_survived = []\n",
        "            for s in range(len(neg_samples)):\n",
        "                if random.random() < ratio:\n",
        "                    neg_survived.append(neg_samples[s])\n",
        "            samples = neg_survived + pos_samples\n",
        "            signals = signals[samples, :, :]\n",
        "            labels_apnea = labels_apnea[samples]\n",
        "            labels_hypopnea = labels_hypopnea[samples]\n",
        "\n",
        "            data = np.zeros((signals.shape[0], CHAT_EPOCH_LENGTH * CHAT_FREQ, chat_s_count + 2))\n",
        "            for i in range(signals.shape[0]):  # for each epoch\n",
        "                # data[i, :len(demo_arr), -3] = demo_arr\n",
        "                data[i, :, -1], data[i, :, -2], status = extract_rri(signals[i, CHAT_ECG_SIG, :], CHAT_FREQ,\n",
        "                                                                     float(CHAT_EPOCH_LENGTH))\n",
        "\n",
        "                if status:\n",
        "                    rri_succ_counter += 1\n",
        "                else:\n",
        "                    rri_fail_counter += 1\n",
        "\n",
        "                for j in range(chat_s_count):  # for each signal\n",
        "                    data[i, :, j] = signals[i, CHAT_SIGS[j], :]\n",
        "\n",
        "            if first:\n",
        "                aggregated_data = data\n",
        "                aggregated_label_apnea = labels_apnea\n",
        "                aggregated_label_hypopnea = labels_hypopnea\n",
        "                first = False\n",
        "            else:\n",
        "                aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n",
        "                aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n",
        "                aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n",
        "            print(rri_succ_counter, rri_fail_counter)\n",
        "\n",
        "        x.append(aggregated_data)\n",
        "        y_apnea.append(aggregated_label_apnea)\n",
        "        y_hypopnea.append(aggregated_label_hypopnea)\n",
        "\n",
        "    return x, y_apnea, y_hypopnea\n",
        "x, y_apnea, y_hypopnea = load_data(CHAT_PREPROCESSED_PATH)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x2y8OIV1PA2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nch_preprocessor\n",
        "import concurrent.futures\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import mne\n",
        "import numpy as np\n",
        "import scipy\n",
        "from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n",
        "from biosppy.signals import tools as st\n",
        "from mne import make_fixed_length_events\n",
        "from scipy.interpolate import splev, splrep\n",
        "from itertools import compress\n",
        "import sleep_study as ss\n",
        "\n",
        "THRESHOLD = 3\n",
        "NUM_WORKER = 8\n",
        "SN = 3984  # STUDY NUMBER\n",
        "FREQ = 64.0\n",
        "CHUNK_DURATION = 30.0\n",
        "OUT_FOLDER = 'D:\\\\nch_30x64'\n",
        "\n",
        "# channels = [\n",
        "#     \"EOG LOC-M2\",  # 0\n",
        "#     \"EOG ROC-M1\",  # 1\n",
        "#     \"EEG F3-M2\",  # 2\n",
        "#     \"EEG F4-M1\",  # 3\n",
        "#     \"EEG C3-M2\",  # 4\n",
        "#     \"EEG C4-M1\",  # 5\n",
        "#     \"EEG O1-M2\",  # 6\n",
        "#     \"EEG O2-M1\",  # 7\n",
        "#     \"EEG CZ-O1\",  # 8\n",
        "#     \"ECG EKG2-EKG\",  # 9\n",
        "#     \"RESP PTAF\",  # 10\n",
        "#     \"RESP AIRFLOW\",  # 11\n",
        "#     \"RESP THORACIC\",  # 12\n",
        "#     \"RESP ABDOMINAL\",  # 13\n",
        "#     \"SPO2\",  # 14\n",
        "#     \"RATE\",  # 15\n",
        "#     \"CAPNO\",  # 16\n",
        "#     \"RESP RATE\",  # 17\n",
        "# ]\n",
        "\n",
        "\n",
        "channels = [\n",
        "    \"EOG LOC-M2\",  # 0\n",
        "    \"EOG ROC-M1\",  # 1\n",
        "    \"EEG C3-M2\",  # 2\n",
        "    \"EEG C4-M1\",  # 3\n",
        "    \"ECG EKG2-EKG\",  # 4\n",
        "\n",
        "    \"RESP PTAF\",  # 5\n",
        "    \"RESP AIRFLOW\",  # 6\n",
        "    \"RESP THORACIC\",  # 7\n",
        "    \"RESP ABDOMINAL\",  # 8\n",
        "    \"SPO2\",  # 9\n",
        "    \"CAPNO\",  # 10\n",
        "]\n",
        "\n",
        "APNEA_EVENT_DICT = {\n",
        "    \"Obstructive Apnea\": 2,\n",
        "    \"Central Apnea\": 2,\n",
        "    \"Mixed Apnea\": 2,\n",
        "    \"apnea\": 2,\n",
        "    \"obstructive apnea\": 2,\n",
        "    \"central apnea\": 2,\n",
        "    \"apnea\": 2,\n",
        "    \"Apnea\": 2,\n",
        "}\n",
        "\n",
        "HYPOPNEA_EVENT_DICT = {\n",
        "    \"Obstructive Hypopnea\": 1,\n",
        "    \"Hypopnea\": 1,\n",
        "    \"hypopnea\": 1,\n",
        "    \"Mixed Hypopnea\": 1,\n",
        "    \"Central Hypopnea\": 1,\n",
        "}\n",
        "\n",
        "POS_EVENT_DICT = {\n",
        "    \"Obstructive Hypopnea\": 1,\n",
        "    \"Hypopnea\": 1,\n",
        "    \"hypopnea\": 1,\n",
        "    \"Mixed Hypopnea\": 1,\n",
        "    \"Central Hypopnea\": 1,\n",
        "\n",
        "    \"Obstructive Apnea\": 2,\n",
        "    \"Central Apnea\": 2,\n",
        "    \"Mixed Apnea\": 2,\n",
        "    \"apnea\": 2,\n",
        "    \"obstructive apnea\": 2,\n",
        "    \"central apnea\": 2,\n",
        "    \"Apnea\": 2,\n",
        "}\n",
        "\n",
        "NEG_EVENT_DICT = {\n",
        "    'Sleep stage N1': 0,\n",
        "    'Sleep stage N2': 0,\n",
        "    'Sleep stage N3': 0,\n",
        "    'Sleep stage R': 0,\n",
        "}\n",
        "\n",
        "WAKE_DICT = {\n",
        "    \"Sleep stage W\": 10\n",
        "}\n",
        "\n",
        "mne.set_log_file('log.txt', overwrite=False)\n",
        "\n",
        "########################################## Annotation Modifier functions ##########################################\n",
        "def identity(df):\n",
        "    return df\n",
        "\n",
        "\n",
        "def apnea2bad(df):\n",
        "    df = df.replace(r'.*pnea.*', 'badevent', regex=True)\n",
        "    print(\"bad replaced!\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def wake2bad(df):\n",
        "    return df.replace(\"Sleep stage W\", 'badevent')\n",
        "\n",
        "\n",
        "def change_duration(df, label_dict=POS_EVENT_DICT, duration=CHUNK_DURATION):\n",
        "    for key in label_dict:\n",
        "        df.loc[df.description == key, 'duration'] = duration\n",
        "    print(\"change duration!\")\n",
        "    return df\n",
        "\n",
        "def preprocess(i, annotation_modifier, out_dir, ahi_dict):\n",
        "    is_apnea_available, is_hypopnea_available = True, True\n",
        "    study = ss.data.study_list[i]\n",
        "    raw = ss.data.load_study(study, annotation_modifier, verbose=True)\n",
        "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
        "    if not all([name in raw.ch_names for name in channels]):\n",
        "        print(\"study \" + str(study) + \" skipped since insufficient channels\")\n",
        "        return 0\n",
        "\n",
        "    if ahi_dict.get(study, 0) < THRESHOLD:\n",
        "        print(\"study \" + str(study) + \" skipped since low AHI ---  AHI = \" + str(ahi_dict.get(study, 0)))\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=POS_EVENT_DICT, chunk_duration=1.0,\n",
        "                                                              verbose=None)\n",
        "    except ValueError:\n",
        "        print(\"No Chunk found!\")\n",
        "        return 0\n",
        "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
        "    print(str(i) + \"---\" + str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %d' % i)\n",
        "\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=APNEA_EVENT_DICT, chunk_duration=1.0,\n",
        "                                                              verbose=None)\n",
        "    except ValueError:\n",
        "        is_apnea_available = False\n",
        "\n",
        "    try:\n",
        "        hypopnea_events, event_ids = mne.events_from_annotations(raw, event_id=HYPOPNEA_EVENT_DICT, chunk_duration=1.0,\n",
        "                                                                 verbose=None)\n",
        "    except ValueError:\n",
        "        is_hypopnea_available = False\n",
        "\n",
        "    wake_events, event_ids = mne.events_from_annotations(raw, event_id=WAKE_DICT, chunk_duration=1.0, verbose=None)\n",
        "    ####################################################################################################################\n",
        "    sfreq = raw.info['sfreq']\n",
        "    tmax = CHUNK_DURATION - 1. / sfreq\n",
        "\n",
        "    raw = raw.pick_channels(channels, ordered=True)\n",
        "    fixed_events = make_fixed_length_events(raw, id=0, duration=CHUNK_DURATION, overlap=0.)\n",
        "    epochs = mne.Epochs(raw, fixed_events, event_id=[0], tmin=0, tmax=tmax, baseline=None, preload=True, proj=False, verbose=None)\n",
        "    epochs.load_data()\n",
        "    if sfreq != FREQ:\n",
        "        epochs = epochs.resample(FREQ, npad='auto', n_jobs=4, verbose=None)\n",
        "    data = epochs.get_data()\n",
        "    ####################################################################################################################\n",
        "    if is_apnea_available:\n",
        "        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n",
        "    if is_hypopnea_available:\n",
        "        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n",
        "    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n",
        "\n",
        "    starts = (epochs.events[:, 0] / sfreq).astype(int)\n",
        "\n",
        "    labels_apnea = []\n",
        "    labels_hypopnea = []\n",
        "    labels_wake = []\n",
        "    total_apnea_event_second = 0\n",
        "    total_hypopnea_event_second = 0\n",
        "\n",
        "    for seq in range(data.shape[0]):\n",
        "        epoch_set = set(range(starts[seq], starts[seq] + int(CHUNK_DURATION)))\n",
        "        if is_apnea_available:\n",
        "            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n",
        "            total_apnea_event_second += apnea_seconds\n",
        "            labels_apnea.append(apnea_seconds)\n",
        "        else:\n",
        "            labels_apnea.append(0)\n",
        "\n",
        "        if is_hypopnea_available:\n",
        "            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n",
        "            total_hypopnea_event_second += hypopnea_seconds\n",
        "            labels_hypopnea.append(hypopnea_seconds)\n",
        "        else:\n",
        "            labels_hypopnea.append(0)\n",
        "\n",
        "        labels_wake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n",
        "    ####################################################################################################################\n",
        "    print(study + \"    HAMED    \" + str(len(labels_wake) - sum(labels_wake)))\n",
        "    data = data[labels_wake, :, :]\n",
        "    labels_apnea = list(compress(labels_apnea, labels_wake))\n",
        "    labels_hypopnea = list(compress(labels_hypopnea, labels_wake))\n",
        "\n",
        "    np.savez_compressed(\n",
        "        out_dir + '\\\\' + study + \"_\" + str(total_apnea_event_second) + \"_\" + str(total_hypopnea_event_second),\n",
        "        data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n",
        "\n",
        "    return data.shape[0]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ahi = pd.read_csv(r\"D:\\Data\\AHI.csv\")\n",
        "    ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n",
        "    ss.__init__()\n",
        "\n",
        "    if NUM_WORKER < 2:\n",
        "        for idx in range(SN):\n",
        "            preprocess(idx, identity, OUT_FOLDER, ahi_dict)\n",
        "    else:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKER) as executor:\n",
        "            executor.map(preprocess, range(SN), [identity] * SN, [OUT_FOLDER] * SN, [ahi_dict] * SN)"
      ],
      "metadata": {
        "id": "AQKOSkMPT0kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nch dataloader\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import resample\n",
        "from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n",
        "from biosppy.signals import tools as st\n",
        "from scipy.interpolate import splev, splrep\n",
        "\n",
        "# \"EOG LOC-M2\",  # 0\n",
        "# \"EOG ROC-M1\",  # 1\n",
        "# \"EEG C3-M2\",  # 2\n",
        "# \"EEG C4-M1\",  # 3\n",
        "# \"ECG EKG2-EKG\",  # 4\n",
        "#\n",
        "# \"RESP PTAF\",  # 5\n",
        "# \"RESP AIRFLOW\",  # 6\n",
        "# \"RESP THORACIC\",  # 7\n",
        "# \"RESP ABDOMINAL\",  # 8\n",
        "# \"SPO2\",  # 9\n",
        "# \"CAPNO\",  # 10\n",
        "\n",
        "######### ADDED IN THIS STEP #########\n",
        "# RRI #11\n",
        "# Ramp #12\n",
        "# Demo #13\n",
        "\n",
        "\n",
        "SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "s_count = len(SIGS)\n",
        "\n",
        "THRESHOLD = 3\n",
        "PATH = \"D:\\\\nch_30x64\\\\\"\n",
        "FREQ = 64\n",
        "EPOCH_DURATION = 30\n",
        "ECG_SIG = 4\n",
        "OUT_PATH = \"D:\\\\nch_30x64\"\n",
        "\n",
        "def extract_rri(signal, ir, CHUNK_DURATION):\n",
        "    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))  # TIME METRIC FOR INTERPOLATION\n",
        "\n",
        "    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * FREQ),\n",
        "                                      frequency=[3, 45], sampling_rate=FREQ, )\n",
        "    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=FREQ)\n",
        "    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=FREQ, tol=0.05)\n",
        "\n",
        "    if 4 < len(rpeaks) < 200:  # and np.max(signal) < 0.0015 and np.min(signal) > -0.0015:\n",
        "        rri_tm, rri_signal = rpeaks[1:] / float(FREQ), np.diff(rpeaks) / float(FREQ)\n",
        "        ampl_tm, ampl_signal = rpeaks / float(FREQ), signal[rpeaks]\n",
        "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n",
        "        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n",
        "\n",
        "        return np.clip(rri_interp_signal, 0, 2), np.clip(amp_interp_signal, -0.001, 0.002)\n",
        "    else:\n",
        "        return np.zeros((FREQ * EPOCH_DURATION)), np.zeros((FREQ * EPOCH_DURATION))\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    # demo = pd.read_csv(\"../misc/result.csv\") # TODO\n",
        "    ahi = pd.read_csv(r\"D:\\Data\\AHI.csv\")\n",
        "    ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n",
        "    root_dir = os.path.expanduser(path)\n",
        "    file_list = os.listdir(root_dir)\n",
        "    length = len(file_list)\n",
        "\n",
        "    study_event_counts = {}\n",
        "    apnea_event_counts = {}\n",
        "    hypopnea_event_counts = {}\n",
        "    ######################################## Count the respiratory events ###########################################\n",
        "    for i in range(length):\n",
        "        patient_id = (file_list[i].split(\"_\")[0])\n",
        "        study_id = (file_list[i].split(\"_\")[1])\n",
        "        apnea_count = int((file_list[i].split(\"_\")[2]))\n",
        "        hypopnea_count = int((file_list[i].split(\"_\")[3]).split(\".\")[0])\n",
        "\n",
        "        if ahi_dict.get(patient_id + \"_\" + study_id, 0) > THRESHOLD:\n",
        "            apnea_event_counts[patient_id] = apnea_event_counts.get(patient_id, 0) + apnea_count\n",
        "            hypopnea_event_counts[patient_id] = hypopnea_event_counts.get(patient_id, 0) + hypopnea_count\n",
        "            study_event_counts[patient_id] = study_event_counts.get(patient_id, 0) + apnea_count + hypopnea_count\n",
        "        else:\n",
        "            os.remove(PATH + file_list[i])\n",
        "\n",
        "    apnea_event_counts = sorted(apnea_event_counts.items(), key=lambda item: item[1])\n",
        "    hypopnea_event_counts = sorted(hypopnea_event_counts.items(), key=lambda item: item[1])\n",
        "    study_event_counts = sorted(study_event_counts.items(), key=lambda item: item[1])\n",
        "\n",
        "    ################################### Fold the data based on number of respiratory events #########################\n",
        "    folds = []\n",
        "    for i in range(5):\n",
        "        folds.append(study_event_counts[i::5])\n",
        "\n",
        "    x = []\n",
        "    y_apnea = []\n",
        "    y_hypopnea = []\n",
        "    counter = 0\n",
        "    for idx, fold in enumerate(folds):\n",
        "        first = True\n",
        "        for patient in fold:\n",
        "            counter += 1\n",
        "            print(counter)\n",
        "            for study in glob.glob(PATH + patient[0] + \"_*\"):\n",
        "                study_data = np.load(study)\n",
        "\n",
        "                signals = study_data['data']\n",
        "                labels_apnea = study_data['labels_apnea']\n",
        "                labels_hypopnea = study_data['labels_hypopnea']\n",
        "\n",
        "                identifier = study.split('\\\\')[-1].split('_')[0] + \"_\" + study.split('\\\\')[-1].split('_')[1]\n",
        "                # demo_arr = demo[demo['id'] == identifier].drop(columns=['id']).to_numpy().squeeze() # TODO\n",
        "\n",
        "                y_c = labels_apnea + labels_hypopnea\n",
        "                neg_samples = np.where(y_c == 0)[0]\n",
        "                pos_samples = list(np.where(y_c > 0)[0])\n",
        "                ratio = len(pos_samples) / len(neg_samples)\n",
        "                neg_survived = []\n",
        "                for s in range(len(neg_samples)):\n",
        "                    if random.random() < ratio:\n",
        "                        neg_survived.append(neg_samples[s])\n",
        "                samples = neg_survived + pos_samples\n",
        "                signals = signals[samples, :, :]\n",
        "                labels_apnea = labels_apnea[samples]\n",
        "                labels_hypopnea = labels_hypopnea[samples]\n",
        "\n",
        "                data = np.zeros((signals.shape[0], EPOCH_DURATION * FREQ, s_count + 3))\n",
        "                for i in range(signals.shape[0]):  # for each epoch\n",
        "                    # data[i, :len(demo_arr), -1] = demo_arr TODO\n",
        "                    data[i, :, -2], data[i, :, -3] = extract_rri(signals[i, ECG_SIG, :], FREQ, float(EPOCH_DURATION))\n",
        "                    for j in range(s_count):  # for each signal\n",
        "                        data[i, :, j] = resample(signals[i, SIGS[j], :], EPOCH_DURATION * FREQ)\n",
        "\n",
        "                if first:\n",
        "                    aggregated_data = data\n",
        "                    aggregated_label_apnea = labels_apnea\n",
        "                    aggregated_label_hypopnea = labels_hypopnea\n",
        "                    first = False\n",
        "                else:\n",
        "                    aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n",
        "                    aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n",
        "                    aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n",
        "\n",
        "\n",
        "        x.append(aggregated_data)\n",
        "        y_apnea.append(aggregated_label_apnea)\n",
        "        y_hypopnea.append(aggregated_label_hypopnea)\n",
        "\n",
        "    return x, y_apnea, y_hypopnea\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x, y_apnea, y_hypopnea = load_data(PATH)\n",
        "    np.savez_compressed(OUT_PATH, x=x, y_apnea=y_apnea, y_hypopnea=y_hypopnea)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPab-VoKT0vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it.\n",
        "\n",
        "  \n",
        "Model Architecture: A model statistics report was produced as a result of running the provided model.py code. According to the report, there are six Conv1D layers and three SeparableConv1D layers. Six Dropout layers were counted, as well as nine Dense layers. There is one MultiHeadAttention mechanism present. The total number of parameters is 389284, which is also equal to the number of trainable parameters. The model occupies 1.49 MB of storage.\n",
        "\n",
        "Training Objectives: The purpose of training the model is to update the parameters until a minimized loss function is achieved. The parameters will be updated by adjusting the connection weights and output biases. The goal of training the model is to obtain an accurate prediction of a sleep apnea event, based on sleep signals, including SpO2 and ECG.\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from metrics import Result\n",
        "from data.noise_util import add_noise_to_data\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "THRESHOLD = 1\n",
        "FOLD = 5\n",
        "\n",
        "\n",
        "def test(config, fold=None):\n",
        "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
        "    ############################################################################\n",
        "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
        "    y = y_apnea + y_hypopnea\n",
        "    for i in range(FOLD):\n",
        "        x[i], y[i] = shuffle(x[i], y[i])\n",
        "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
        "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
        "        x[i] = x[i][:, :, config[\"channels\"]]\n",
        "    ############################################################################\n",
        "    result = Result()\n",
        "    folds = range(FOLD) if fold is None else [fold]\n",
        "    for fold in folds:\n",
        "        x_test = x[fold]\n",
        "        if config.get(\"test_noise_snr\"):\n",
        "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
        "\n",
        "        y_test = y[fold]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
        "\n",
        "        model = tf.keras.models.load_model(config[\"model_path\"] + str(fold), compile=False)\n",
        "\n",
        "        predict = model.predict(x_test)\n",
        "        y_score = predict\n",
        "        y_predict = np.where(predict > 0.5, 1, 0)# For MultiClass np.argmax(y_score, axis=-1)\n",
        "\n",
        "        result.add(y_test, y_predict, y_score)\n",
        "\n",
        "    result.print()\n",
        "    result.save(\"./results/\" + config[\"model_name\"] + \".txt\", config)\n",
        "\n",
        "    del data, x_test, y_test, model, predict, y_score, y_predict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_age_seperated(config):\n",
        "    x = []\n",
        "    y_apnea = []\n",
        "    y_hypopnea = []\n",
        "    for i in range(10):\n",
        "        data = np.load(config[\"data_path\"] + str(i) + \".npz\", allow_pickle=True)\n",
        "        x.append(data['x'])\n",
        "        y_apnea.append(data['y_apnea'])\n",
        "        y_hypopnea.append(data['y_hypopnea'])\n",
        "    ############################################################################\n",
        "    y = np.array(y_apnea) + np.array(y_hypopnea)\n",
        "    for i in range(10):\n",
        "        x[i], y[i] = shuffle(x[i], y[i])\n",
        "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
        "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
        "        x[i] = x[i][:, :, config[\"channels\"]]\n",
        "    ############################################################################\n",
        "    result = Result()\n",
        "\n",
        "    for fold in range(10):\n",
        "        x_test = x[fold]\n",
        "        if config.get(\"test_noise_snr\"):\n",
        "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
        "\n",
        "        y_test = y[fold]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
        "\n",
        "        model = tf.keras.models.load_model(config[\"model_path\"] + str(0), compile=False)\n",
        "\n",
        "        predict = model.predict(x_test)\n",
        "        y_score = predict\n",
        "        y_predict = np.where(predict > 0.5, 1, 0)# For MultiClass np.argmax(y_score, axis=-1)\n",
        "\n",
        "        result.add(y_test, y_predict, y_score)\n",
        "\n",
        "    result.print()\n",
        "    result.save(\"./results/\" + config[\"model_name\"] + \".txt\", config)\n",
        "\n",
        "    del data, x_test, y_test, model, predict, y_score, y_predict\n",
        "\n"
      ],
      "metadata": {
        "id": "LLtnZj2GQj_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "import keras\n",
        "import keras.metrics\n",
        "import numpy as np\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from models.models import get_model\n",
        "from models import get_model\n",
        "\n",
        "THRESHOLD = 1\n",
        "FOLD = 5\n",
        "\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch > 50 and (epoch - 1) % 5 == 0:\n",
        "        lr *= 0.5\n",
        "    return lr\n",
        "\n",
        "\n",
        "def train(config, fold=None):\n",
        "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
        "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
        "    y = y_apnea + y_hypopnea\n",
        "    ########################################################################################\n",
        "    for i in range(FOLD):\n",
        "        x[i], y[i] = shuffle(x[i], y[i])\n",
        "        x[i] = np.nan_to_num(x[i], nan=1)\n",
        "        if config[\"regression\"]:\n",
        "            y[i] = np.sqrt(y[i])\n",
        "            y[i][y[i] != 0] += 2\n",
        "        else:\n",
        "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
        "\n",
        "        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n",
        "\n",
        "    ########################################################################################\n",
        "    folds = range(FOLD) if fold is None else [fold]\n",
        "    for fold in folds:\n",
        "        first = True\n",
        "        for i in range(5):\n",
        "            if i != fold:\n",
        "                if first:\n",
        "                    x_train = x[i]\n",
        "                    y_train = y[i]\n",
        "                    first = False\n",
        "                else:\n",
        "                    x_train = np.concatenate((x_train, x[i]))\n",
        "                    y_train = np.concatenate((y_train, y[i]))\n",
        "\n",
        "        model = get_model(config)\n",
        "        if config[\"regression\"]:\n",
        "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
        "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        else:\n",
        "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
        "                          metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "        model.fit(x=x_train, y=y_train, batch_size=512, epochs=config[\"epochs\"], validation_split=0.1,\n",
        "                  callbacks=[early_stopper, lr_scheduler])\n",
        "        ################################################################################################################\n",
        "        model.save(config[\"model_path\"] + str(fold))\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "\n",
        "def train_age_seperated(config):\n",
        "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
        "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
        "    y = y_apnea + y_hypopnea\n",
        "    ########################################################################################\n",
        "    for i in range(10):\n",
        "        x[i], y[i] = shuffle(x[i], y[i])\n",
        "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
        "        if config[\"regression\"]:\n",
        "            y[i] = np.sqrt(y[i])\n",
        "            y[i][y[i] != 0] += 2\n",
        "        else:\n",
        "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
        "\n",
        "        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n",
        "\n",
        "    ########################################################################################\n",
        "    first = True\n",
        "    for i in range(10):\n",
        "        if first:\n",
        "            x_train = x[i]\n",
        "            y_train = y[i]\n",
        "            first = False\n",
        "        else:\n",
        "            x_train = np.concatenate((x_train, x[i]))\n",
        "            y_train = np.concatenate((y_train, y[i]))\n",
        "\n",
        "    model = get_model(config)\n",
        "    if config[\"regression\"]:\n",
        "        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
        "        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    else:\n",
        "        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
        "                      metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "    model.fit(x=x_train, y=y_train, batch_size=512, epochs=config[\"epochs\"], validation_split=0.1,\n",
        "              callbacks=[early_stopper, lr_scheduler])\n",
        "    ################################################################################################################\n",
        "    model.save(config[\"model_path\"] + str(0))\n",
        "    keras.backend.clear_session()\n"
      ],
      "metadata": {
        "id": "m3A4B59sQzEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score, average_precision_score, roc_auc_score\n",
        "\n",
        "\n",
        "class FromLogitsMixin:\n",
        "    def __init__(self, from_logits=False, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        if self.from_logits:\n",
        "            y_pred = tf.nn.sigmoid(y_pred)\n",
        "        return super().update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "\n",
        "class AUC(FromLogitsMixin, tf.metrics.AUC):\n",
        "    ...\n",
        "\n",
        "\n",
        "class BinaryAccuracy(FromLogitsMixin, tf.metrics.BinaryAccuracy):\n",
        "    ...\n",
        "\n",
        "\n",
        "class TruePositives(FromLogitsMixin, tf.metrics.TruePositives):\n",
        "    ...\n",
        "\n",
        "\n",
        "class FalsePositives(FromLogitsMixin, tf.metrics.FalsePositives):\n",
        "    ...\n",
        "\n",
        "\n",
        "class TrueNegatives(FromLogitsMixin, tf.metrics.TrueNegatives):\n",
        "    ...\n",
        "\n",
        "\n",
        "class FalseNegatives(FromLogitsMixin, tf.metrics.FalseNegatives):\n",
        "    ...\n",
        "\n",
        "\n",
        "class Precision(FromLogitsMixin, tf.metrics.Precision):\n",
        "    ...\n",
        "\n",
        "\n",
        "class Recall(FromLogitsMixin, tf.metrics.Recall):\n",
        "    ...\n",
        "\n",
        "\n",
        "class F1Score(FromLogitsMixin, tfa.metrics.F1Score):\n",
        "    ...\n",
        "\n",
        "\n",
        "class Result:\n",
        "    def __init__(self):\n",
        "        self.accuracy_list = []\n",
        "        self.sensitivity_list = []\n",
        "        self.specificity_list = []\n",
        "        self.f1_list = []\n",
        "        self.auroc_list = []\n",
        "        self.auprc_list = []\n",
        "        self.precision_list = []\n",
        "\n",
        "    def add(self, y_test, y_predict, y_score):\n",
        "        C = confusion_matrix(y_test, y_predict, labels=(1, 0))\n",
        "        TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
        "\n",
        "        acc, sn, sp, pr = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP), 1. * TP / (\n",
        "                TP + FP)\n",
        "        f1 = f1_score(y_test, y_predict)\n",
        "        auc = roc_auc_score(y_test, y_score)\n",
        "        auprc = average_precision_score(y_test, y_score)\n",
        "\n",
        "        self.accuracy_list.append(acc * 100)\n",
        "        self.precision_list.append(pr * 100)\n",
        "        self.sensitivity_list.append(sn * 100)\n",
        "        self.specificity_list.append(sp * 100)\n",
        "        self.f1_list.append(f1 * 100)\n",
        "        self.auroc_list.append(auc * 100)\n",
        "        self.auprc_list.append(auprc * 100)\n",
        "\n",
        "\n",
        "    def get(self):\n",
        "        out_str = \"=========================================================================== \\n\"\n",
        "        out_str += str(self.accuracy_list) + \" \\n\"\n",
        "        out_str += str(self.precision_list) + \" \\n\"\n",
        "        out_str += str(self.sensitivity_list) + \" \\n\"\n",
        "        out_str += str(self.specificity_list) + \" \\n\"\n",
        "        out_str += str(self.f1_list) + \" \\n\"\n",
        "        out_str += str(self.auroc_list) + \" \\n\"\n",
        "        out_str += str(self.auprc_list) + \" \\n\"\n",
        "        out_str += str(\"Accuracy: %.2f -+ %.3f\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \" \\n\"\n",
        "        out_str += str(\"Precision: %.2f -+ %.3f\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \" \\n\"\n",
        "        out_str += str(\n",
        "            \"Recall: %.2f -+ %.3f\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \" \\n\"\n",
        "        out_str += str(\n",
        "            \"Specifity: %.2f -+ %.3f\" % (np.mean(self.specificity_list), np.std(self.specificity_list))) + \" \\n\"\n",
        "        out_str += str(\"F1: %.2f -+ %.3f\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \" \\n\"\n",
        "        out_str += str(\"AUROC: %.2f -+ %.3f\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \" \\n\"\n",
        "        out_str += str(\"AUPRC: %.2f -+ %.3f\" % (np.mean(self.auprc_list), np.std(self.auprc_list))) + \" \\n\"\n",
        "\n",
        "        out_str += str(\"$ %.1f \\pm %.1f$\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \"& \"\n",
        "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \"& \"\n",
        "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \"& \"\n",
        "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \"& \"\n",
        "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \"& \"\n",
        "\n",
        "        return out_str\n",
        "\n",
        "    def print(self):\n",
        "        print(self.get())\n",
        "\n",
        "    def save(self, path, config):\n",
        "        file = open(path, \"w+\")\n",
        "        file.write(str(config))\n",
        "        file.write(\"\\n\")\n",
        "        file.write(self.get())\n",
        "        file.flush()\n",
        "        file.close()\n"
      ],
      "metadata": {
        "id": "KsvZWDzCQzQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from keras import Model\n",
        "from keras.activations import sigmoid, relu\n",
        "from keras.layers import Dense, Dropout, Reshape, LayerNormalization, MultiHeadAttention, Add, Flatten, Input, Layer, \\\n",
        "    GlobalAveragePooling1D, AveragePooling1D, Concatenate, SeparableConvolution1D, Conv1D\n",
        "from keras.regularizers import L2\n",
        "\n",
        "\n",
        "\n",
        "class Patches(Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, input):\n",
        "        input = input[:, tf.newaxis, :, :]\n",
        "        batch_size = tf.shape(input)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=input,\n",
        "            sizes=[1, 1, self.patch_size, 1],\n",
        "            strides=[1, 1, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches,\n",
        "                             [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "\n",
        "class PatchEncoder(Layer):\n",
        "    def __init__(self, num_patches, projection_dim, l2_weight):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.projection_dim = projection_dim\n",
        "        self.l2_weight = l2_weight\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = Dense(units=projection_dim, kernel_regularizer=L2(l2_weight),\n",
        "                                bias_regularizer=L2(l2_weight))\n",
        "        self.position_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) # + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate, l2_weight):\n",
        "    for _, units in enumerate(hidden_units):\n",
        "        x = Dense(units, activation=None, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight))(x)\n",
        "        x = tf.nn.gelu(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def create_transformer_model(input_shape, num_patches,\n",
        "                             projection_dim, transformer_layers,\n",
        "                             num_heads, transformer_units, mlp_head_units,\n",
        "                             num_classes, drop_out, reg, l2_weight, demographic=False):\n",
        "    if reg:\n",
        "        activation = None\n",
        "    else:\n",
        "        activation = 'sigmoid'\n",
        "    inputs = Input(shape=input_shape)\n",
        "    patch_size = input_shape[0] / num_patches\n",
        "    if demographic:\n",
        "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
        "                                                             beta_initializer=\"glorot_uniform\",\n",
        "                                                             gamma_initializer=\"glorot_uniform\")(inputs[:,:,:-1])\n",
        "        demo = inputs[:, :12, -1]\n",
        "\n",
        "    else:\n",
        "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
        "                                                             beta_initializer=\"glorot_uniform\",\n",
        "                                                             gamma_initializer=\"glorot_uniform\")(inputs)\n",
        "\n",
        "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
        "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
        "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
        "    for i in range(transformer_layers):\n",
        "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
        "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
        "        x2 = Add()([attention_output, encoded_patches])\n",
        "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
        "        encoded_patches = Add()([x3, x2])\n",
        "\n",
        "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    #x = Concatenate()([x, demo])\n",
        "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
        "\n",
        "    logits = Dense(num_classes, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
        "                   activation=activation)(features)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "\n",
        "\n",
        "def create_hybrid_transformer_model(input_shape):\n",
        "    transformer_units =  [32,32]\n",
        "    transformer_layers = 2\n",
        "    num_heads = 4\n",
        "    l2_weight = 0.001\n",
        "    drop_out= 0.25\n",
        "    mlp_head_units = [256, 128]\n",
        "    num_patches=30\n",
        "    projection_dim=  32\n",
        "\n",
        "    # Conv1D(32...\n",
        "    input1 = Input(shape=input_shape)\n",
        "    conv11 = Conv1D(16, 256)(input1) #13\n",
        "    conv12 = Conv1D(16, 256)(input1) #13\n",
        "    conv13 = Conv1D(16, 256)(input1) #13\n",
        "\n",
        "    pwconv1 = SeparableConvolution1D(32, 1)(input1)\n",
        "    pwconv2 = SeparableConvolution1D(32, 1)(pwconv1)\n",
        "\n",
        "    conv21 = Conv1D(16, 256)(conv11) # 7\n",
        "    conv22 = Conv1D(16, 256)(conv12) # 7\n",
        "    conv23 = Conv1D(16, 256)(conv13) # 7\n",
        "\n",
        "    concat = keras.layers.concatenate([conv21, conv22, conv23], axis=-1)\n",
        "    concat = Dense(64, activation=relu)(concat) #192\n",
        "    concat = Dense(64, activation=sigmoid)(concat) #192\n",
        "    concat = SeparableConvolution1D(32,1)(concat)\n",
        "    concat = keras.layers.concatenate([concat, pwconv2], axis=1)\n",
        "\n",
        "    ####################################################################################################################\n",
        "    patch_size = input_shape[0] / num_patches\n",
        "\n",
        "    normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
        "                                                             beta_initializer=\"glorot_uniform\",\n",
        "                                                             gamma_initializer=\"glorot_uniform\")(concat)\n",
        "\n",
        "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
        "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
        "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
        "    for i in range(transformer_layers):\n",
        "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
        "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
        "        x2 = Add()([attention_output, encoded_patches])\n",
        "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
        "        encoded_patches = Add()([x3, x2])\n",
        "\n",
        "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    #x = Concatenate()([x, demo])\n",
        "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
        "\n",
        "    logits = Dense(1, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
        "                   activation='sigmoid')(features)\n",
        "\n",
        "    ####################################################################################################################\n",
        "\n",
        "    model = Model(inputs=input1, outputs=logits)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "RTpXY6I1RdgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#models\n",
        "#the following code was obtained from the GitHub repository provided by the authors of the original paper.\n",
        "#https://github.com/healthylaife/Pediatric-Apnea-Detection.\n",
        "#small changes were made to allow the code to run locally on a Windows computer.\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"C:\\\\Users\\Admin\\\\Desktop\\\\sleep_data\")\n",
        "import torch\n",
        "\n",
        "import keras\n",
        "from keras import Input, Model\n",
        "from keras.layers import Dense, Flatten, MaxPooling2D, Conv2D, BatchNormalization, LSTM, Bidirectional, Permute, \\\n",
        "    Reshape, GRU, Conv1D, MaxPooling1D, Activation, Dropout, GlobalAveragePooling1D, multiply, MultiHeadAttention, Add, \\\n",
        "    LayerNormalization, SeparableConvolution1D\n",
        "from keras.models import Sequential\n",
        "from keras.activations import relu, sigmoid\n",
        "from keras.regularizers import l2\n",
        "import tensorflow_addons as tfa\n",
        "#from .transformer import create_transformer_model, mlp, create_hybrid_transformer_model\n",
        "from transformer import create_transformer_model, mlp, create_hybrid_transformer_model\n",
        "\n",
        "\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    for i in range(5): # 10\n",
        "        model.add(Conv1D(45, 32, padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(relu))\n",
        "        model.add(MaxPooling1D())\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    for i in range(2): #4\n",
        "        model.add(Dense(512))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(relu))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_cnnlstm_model(input_a_shape, weight=1e-3):\n",
        "    cnn_filters = 32 # 128\n",
        "    cnn_kernel_size = 4 # 4\n",
        "    input1 = Input(shape=input_a_shape)\n",
        "    input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
        "                                              beta_initializer=\"glorot_uniform\",\n",
        "                                              gamma_initializer=\"glorot_uniform\")(input1)\n",
        "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(input1)\n",
        "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = MaxPooling1D()(x1)\n",
        "\n",
        "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = MaxPooling1D()(x1)\n",
        "\n",
        "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = MaxPooling1D()(x1)\n",
        "\n",
        "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
        "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
        "    x1 = LSTM(32)(x1) #256\n",
        "    x1 = Flatten()(x1)\n",
        "\n",
        "    x1 = Dense(32, activation='relu')(x1) #64\n",
        "    x1 = Dense(32, activation='relu')(x1) #64\n",
        "    outputs = Dense(1, activation='sigmoid')(x1)\n",
        "\n",
        "    model = Model(inputs=input1, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_semscnn_model(input_a_shape):\n",
        "    input1 = Input(shape=input_a_shape)\n",
        "    # input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
        "    #                                           beta_initializer=\"glorot_uniform\",\n",
        "    #                                           gamma_initializer=\"glorot_uniform\")(input1)\n",
        "    x1 = Conv1D(45, 32, strides=1)(input1) #kernel_size=11\n",
        "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation(relu)(x1)\n",
        "    x1 = MaxPooling1D()(x1)\n",
        "\n",
        "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation(relu)(x1)\n",
        "    x1 = MaxPooling1D()(x1)\n",
        "\n",
        "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
        "    x1 = BatchNormalization()(x1)\n",
        "    x1 = Activation(relu)(x1)\n",
        "    x1 = MaxPooling1D()(x1)\n",
        "\n",
        "    squeeze = Flatten()(x1)\n",
        "    excitation = Dense(128, activation='relu')(squeeze)\n",
        "    excitation = Dense(64, activation='relu')(excitation)\n",
        "    logits = Dense(1, activation='sigmoid')(excitation)\n",
        "    model = Model(inputs=input1, outputs=logits)\n",
        "    return model\n",
        "\n",
        "\n",
        "model_dict = {\n",
        "\n",
        "    \"cnn\": create_cnn_model((60 * 32, 3)),\n",
        "    \"sem-mscnn\": create_semscnn_model((60 * 32, 3)),\n",
        "    \"cnn-lstm\": create_cnnlstm_model((60 * 32, 3)),\n",
        "    \"hybrid\": create_hybrid_transformer_model((60 * 32, 3)),\n",
        "}\n",
        "\n",
        "\n",
        "def get_model(config):\n",
        "    if config[\"model_name\"].split('_')[0] == \"Transformer\":\n",
        "        return create_transformer_model(input_shape=(60 * 32, len(config[\"channels\"])),\n",
        "                                        num_patches=config[\"num_patches\"], projection_dim=config[\"transformer_units\"],\n",
        "                                        transformer_layers=config[\"transformer_layers\"], num_heads=config[\"num_heads\"],\n",
        "                                        transformer_units=[config[\"transformer_units\"] * 2,\n",
        "                                                           config[\"transformer_units\"]],\n",
        "                                        mlp_head_units=[256, 128], num_classes=1, drop_out=config[\"drop_out_rate\"],\n",
        "                                        reg=config[\"regression\"], l2_weight=config[\"regularization_weight\"])\n",
        "    else:\n",
        "        return model_dict.get(config[\"model_name\"].split('_')[0])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        \"model_name\": \"hybrid\",\n",
        "        \"regression\": False,\n",
        "\n",
        "        \"transformer_layers\": 4,  # best 5\n",
        "        \"drop_out_rate\": 0.25,\n",
        "        \"num_patches\": 20,  # best\n",
        "        \"transformer_units\": 32,  # best 32\n",
        "        \"regularization_weight\": 0.001,  # best 0.001\n",
        "        \"num_heads\": 4,\n",
        "        \"epochs\": 100,  # best\n",
        "        \"channels\": [14, 18, 19, 20],\n",
        "    }\n",
        "    model = get_model(config)\n",
        "    model.build(input_shape=(1, 60 * 32, 10))\n",
        "    print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "    #########################\n",
        "    torch.save(model, \"C:\\\\Users\\\\Admin\\\\Desktop\\\\sleep_data\\\\savedmodel.pt\")"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main_chat\n",
        "#this is the training part for the CHAT dataset\n",
        "#the source is main_chat.py, from the original GitHub repository\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "newpath = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "!pip install tensorflow_addons\n",
        "!pip install mne\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/Colab Notebooks/\")\n",
        "\n",
        "newpath2 = newpath + \"test.py\"\n",
        "!cp \"$newpath2\".\n",
        "\n",
        "newpath3 = newpath + \"train.py\"\n",
        "!cp \"$newpath3\".\n",
        "\n",
        "newpath4 = newpath + \"metrics.py\"\n",
        "!cp \"$newpath4\".\n",
        "\n",
        "newpath5 = newpath + \"data.py\"\n",
        "!cp \"$newpath5\".\n",
        "\n",
        "newpath6 = newpath + \"sleep_study\"\n",
        "!cp \"$newpath6\".\n",
        "\n",
        "#import sleep_study\n",
        "import data\n",
        "\n",
        "\n",
        "import keras\n",
        "import sys\n",
        "\n",
        "#from test import test\n",
        "import test\n",
        "#from train import train\n",
        "import train\n",
        "\n",
        "# 0- E1 - M2\n",
        "# 1- E2 - M1\n",
        "\n",
        "# 2- F3 - M2\n",
        "# 3- F4 - M1\n",
        "# 4- C3 - M2\n",
        "# 5- C4 - M1\n",
        "# 6- O1 - M2\n",
        "# 7- O2 - M1\n",
        "\n",
        "# 8- ECG3 - ECG1\n",
        "\n",
        "# 9- CANULAFLOW\n",
        "# 10- AIRFLOW\n",
        "# 11- CHEST\n",
        "# 12- ABD\n",
        "\n",
        "# 13- SAO2\n",
        "# 14- CAP\n",
        "######### ADDED IN THIS STEP #########\n",
        "# 15- RRI\n",
        "# 16 Ramp\n",
        "\n",
        "sig_dict_chat = {\n",
        "    \"EOG\": [0, 1],\n",
        "    \"EEG\": [4, 5],\n",
        "    \"ECG\": [15,16],\n",
        "    \"Resp\": [9, 10],\n",
        "    \"SPO2\": [13],\n",
        "    \"CO2\": [14],\n",
        "}\n",
        "\n",
        "channel_list_chat = [\n",
        "    [\"ECG\", \"SPO2\"],\n",
        "\n",
        "]\n",
        "\n",
        "for ch in channel_list_chat:\n",
        "    chs = []\n",
        "    chstr = \"\"\n",
        "    for name in ch:\n",
        "        chstr += name\n",
        "        chs = chs + sig_dict_chat[name]\n",
        "    print(chstr, chs)\n",
        "    config = {\n",
        "        \"data_path\": \"/content/drive/My Drive/Colab Notebooks/_1.npz\",\n",
        "        \"model_path\": \"/content/drive/My Drive/Colab Notebooks/saved_model.pt\",\n",
        "        \"model_name\": \"hybrid_\"+ chstr,\n",
        "        \"regression\": False,\n",
        "\n",
        "        \"transformer_layers\": 5,  # best 5\n",
        "        \"drop_out_rate\": 0.25,  # best 0.25\n",
        "        \"num_patches\": 30,  # best 30 TBD\n",
        "        \"transformer_units\": 32,  # best 32\n",
        "        \"regularization_weight\": 0.001,  # best 0.001\n",
        "        \"num_heads\": 4,\n",
        "        \"epochs\": 100,  # best 200\n",
        "        \"channels\": chs,\n",
        "    }\n",
        "    train.train(config, 0)\n",
        "    # test(config, 0)"
      ],
      "metadata": {
        "id": "7cCx2C2HyeRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main_nch\n",
        "import gc\n",
        "\n",
        "from test import test\n",
        "from train import train\n",
        "\n",
        "# \"EOG LOC-M2\",  # 0\n",
        "# \"EOG ROC-M1\",  # 1\n",
        "# \"EEG C3-M2\",  # 2\n",
        "# \"EEG C4-M1\",  # 3\n",
        "# \"ECG EKG2-EKG\",  # 4\n",
        "#\n",
        "# \"RESP PTAF\",  # 5\n",
        "# \"RESP AIRFLOW\",  # 6\n",
        "# \"RESP THORACIC\",  # 7\n",
        "# \"RESP ABDOMINAL\",  # 8\n",
        "# \"SPO2\",  # 9\n",
        "# \"CAPNO\",  # 10\n",
        "\n",
        "######### ADDED IN THIS STEP #########\n",
        "# RRI #11\n",
        "# Ramp #12\n",
        "# Demo #13\n",
        "\n",
        "\n",
        "sig_dict = {\"EOG\": [0, 1],\n",
        "            \"EEG\": [2, 3],\n",
        "            \"RESP\": [5, 6],\n",
        "            \"SPO2\": [9],\n",
        "            \"CO2\": [10],\n",
        "            \"ECG\": [11, 12],\n",
        "            \"DEMO\": [13],\n",
        "            }\n",
        "\n",
        "channel_list = [\n",
        "\n",
        "    [\"ECG\", \"SPO2\"],\n",
        "\n",
        "]\n",
        "\n",
        "for ch in channel_list:\n",
        "    chs = []\n",
        "    chstr = \"\"\n",
        "    for name in ch:\n",
        "        chstr += name\n",
        "        chs = chs + sig_dict[name]\n",
        "    config = {\n",
        "        \"data_path\": \"D:\\\\Data\\\\nch_30x64.npz\",\n",
        "        \"model_path\": \"./weights/semscnn_ecgspo2/f\",\n",
        "        \"model_name\": \"sem-mscnn_\" + chstr,\n",
        "        \"regression\": False,\n",
        "\n",
        "        \"transformer_layers\": 5,  # best 5\n",
        "        \"drop_out_rate\": 0.25,  # best 0.25\n",
        "        \"num_patches\": 30,  # best 30 TBD\n",
        "        \"transformer_units\": 32,  # best 32\n",
        "        \"regularization_weight\": 0.001,  # best 0.001\n",
        "        \"num_heads\": 4,\n",
        "        \"epochs\": 100,  # best 200\n",
        "        \"channels\": chs,\n",
        "    }\n",
        "    train(config, 0)\n",
        "    test(config, 0)\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "vHEk5WDlTONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n",
        "\n",
        "The model we attemped to train resulted in an error.\n",
        "\n",
        "TypeError: Singleton array 0 cannot be considered a valid collection.\n",
        "\n",
        "We made a great effort to resolve this issue. We are still actively searching for a solution to this error."
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n",
        "We have been successful at accomplishing many of the important project steps, such as downloading the data and running the preprocessor output with the data loader program. When we are able to accurately train the model, our goal is to obtain results similar to the original research paper.\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1. Fayyaz H, Strang A, Beheshti R. Bringing At-home Pediatric Sleep Apnea Testing Closer to Reality: A Multi-modal Transformer Approach. Proc Mach Learn Res. 2023 Aug;219:167-185. PMID: 38344396; PMCID: PMC10854997.\n",
        "\n",
        "2. Lee H, Li B, DeForte S, Splaingard ML, Huang Y, Chi Y, Linwood SL. A large collection of real-world pediatric sleep studies. Sci Data. 2022 Jul 19;9(1):421. doi: 10.1038/s41597-022-01545-6. PMID: 35853958; PMCID: PMC9296671.\n",
        "\n",
        "3. Marcus CL, Moore RH, Rosen CL, Giordani B, Garetz SL, Taylor HG, Mitchell RB, Amin R, Katz ES, Arens R, Paruthi S, Muzumdar H, Gozal D, Thomas NH, Ware J, Beebe D, Snyder K, Elden L, Sprecher RC, Willging P, Jones D, Bent JP, Hoban T, Chervin RD, Ellenberg SS, Redline S; Childhood Adenotonsillectomy Trial (CHAT). A randomized trial of adenotonsillectomy for childhood sleep apnea. N Engl J Med. 2013 Jun 20;368(25):2366-76. doi: 10.1056/NEJMoa1215881. Epub 2013 May 21. PMID: 23692173; PMCID: PMC3756808.\n",
        "\n",
        "4. Zhang GQ, Cui L, Mueller R, Tao S, Kim M, Rueschman M, Mariani S, Mobley D, Redline S. The National Sleep Research Resource: towards a sleep data commons. J Am Med Inform Assoc. 2018 Oct 1;25(10):1351-1358. doi: 10.1093/jamia/ocy064. PMID: 29860441; PMCID: PMC6188513.\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}