{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8205b769",
   "metadata": {},
   "source": [
    "# To Run:\n",
    "- You need to have a local installation of pytorch and its dependencies (not in this ntbk)\n",
    "- You need the 5 .npz files named `_0.npz` to `_4.npz` in a directory and change it in chat_train model config. \n",
    "- To generate a saved model (.pt), you need to run afresh e.g. Kernel > Restart and Clear Output. Because the files were all smushed into 1 notebook, there seems to be some conflict in variable name usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb7ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in c:\\users\\elies\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\elies\\anaconda3\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\elies\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\elies\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.4)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61dfcf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.metrics\n",
    "import numpy as np\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.activations import sigmoid, relu\n",
    "from keras.regularizers import L2\n",
    "\n",
    "import torch\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense, Flatten, MaxPooling2D, Conv2D, BatchNormalization, LSTM, Bidirectional, Permute, \\\n",
    "    Reshape, GRU, Conv1D, MaxPooling1D, Activation, Dropout, GlobalAveragePooling1D, multiply, MultiHeadAttention, Add, \\\n",
    "    LayerNormalization, SeparableConvolution1D, Input, Layer, AveragePooling1D, Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.regularizers import l2\n",
    "import tensorflow_addons as tfa\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca869e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "FOLD = 5\n",
    "DATA_FREQ = 128 # (change to 64 for FREQ = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0790868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.py\n",
    "\n",
    "class Patches(Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, input):\n",
    "        input = input[:, tf.newaxis, :, :]\n",
    "        batch_size = tf.shape(input)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=input,\n",
    "            sizes=[1, 1, self.patch_size, 1],\n",
    "            strides=[1, 1, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches,\n",
    "                             [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim, l2_weight):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.l2_weight = l2_weight\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(units=projection_dim, kernel_regularizer=L2(l2_weight),\n",
    "                                bias_regularizer=L2(l2_weight))\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) # + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate, l2_weight):\n",
    "    for _, units in enumerate(hidden_units):\n",
    "        x = Dense(units, activation=None, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight))(x)\n",
    "        x = tf.nn.gelu(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_transformer_model(input_shape, num_patches,\n",
    "                             projection_dim, transformer_layers,\n",
    "                             num_heads, transformer_units, mlp_head_units,\n",
    "                             num_classes, drop_out, reg, l2_weight, demographic=False):\n",
    "    if reg:\n",
    "        activation = None\n",
    "    else:\n",
    "        activation = 'sigmoid'\n",
    "    inputs = Input(shape=input_shape)\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "    if demographic:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(inputs[:,:,:-1])\n",
    "        demo = inputs[:, :12, -1]\n",
    "\n",
    "    else:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(inputs)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
    "\n",
    "    logits = Dense(num_classes, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
    "                   activation=activation)(features)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "\n",
    "def create_hybrid_transformer_model(input_shape):\n",
    "    transformer_units =  [32,32]\n",
    "    transformer_layers = 2\n",
    "    num_heads = 4\n",
    "    l2_weight = 0.001\n",
    "    drop_out= 0.25\n",
    "    mlp_head_units = [256, 128]\n",
    "    num_patches=30\n",
    "    projection_dim=  32\n",
    "\n",
    "    # Conv1D(32...\n",
    "    input1 = Input(shape=input_shape)\n",
    "    conv11 = Conv1D(16, 256)(input1) #13\n",
    "    conv12 = Conv1D(16, 256)(input1) #13\n",
    "    conv13 = Conv1D(16, 256)(input1) #13\n",
    "\n",
    "    pwconv1 = SeparableConvolution1D(32, 1)(input1)\n",
    "    pwconv2 = SeparableConvolution1D(32, 1)(pwconv1)\n",
    "\n",
    "    conv21 = Conv1D(16, 256)(conv11) # 7\n",
    "    conv22 = Conv1D(16, 256)(conv12) # 7\n",
    "    conv23 = Conv1D(16, 256)(conv13) # 7\n",
    "\n",
    "    concat = keras.layers.concatenate([conv21, conv22, conv23], axis=-1)\n",
    "    concat = Dense(64, activation=relu)(concat) #192\n",
    "    concat = Dense(64, activation=sigmoid)(concat) #192\n",
    "    concat = SeparableConvolution1D(32,1)(concat)\n",
    "    concat = keras.layers.concatenate([concat, pwconv2], axis=1)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "\n",
    "    normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(concat)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
    "\n",
    "    logits = Dense(1, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
    "                   activation='sigmoid')(features)\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0dd375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 3840, 3)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 3585, 16)     12304       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 3585, 16)     12304       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 3585, 16)     12304       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 3330, 16)     65552       ['conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 3330, 16)     65552       ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 3330, 16)     65552       ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3330, 48)     0           ['conv1d_16[0][0]',              \n",
      "                                                                  'conv1d_17[0][0]',              \n",
      "                                                                  'conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 3330, 64)     3136        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 3330, 64)     4160        ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 3840, 32)    131         ['input_4[0][0]']                \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " separable_conv1d_2 (SeparableC  (None, 3330, 32)    2144        ['dense_10[0][0]']               \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 3840, 32)    1088        ['separable_conv1d[0][0]']       \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 7170, 32)     0           ['separable_conv1d_2[0][0]',     \n",
      "                                                                  'separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " instance_normalization_1 (Inst  (None, 7170, 32)    0           ['concatenate_1[0][0]']          \n",
      " anceNormalization)                                                                               \n",
      "                                                                                                  \n",
      " patches (Patches)              (None, None, 4096)   0           ['instance_normalization_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, None, 32)     131104      ['patches[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, None, 32)    16800       ['patch_encoder[0][0]',          \n",
      " dAttention)                                                      'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " add (Add)                      (None, None, 32)     0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, None, 32)    64          ['add[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, None, 32)     1056        ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, None, 32)     0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, None, 32)     0           ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, None, 32)     1056        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, None, 32)     0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, None, 32)     0           ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, None, 32)     0           ['dropout_8[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, None, 32)    16800       ['add_1[0][0]',                  \n",
      " eadAttention)                                                    'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, None, 32)     0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, None, 32)    64          ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, None, 32)     1056        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, None, 32)     0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, None, 32)     0           ['tf.nn.gelu_2[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, None, 32)     1056        ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, None, 32)     0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, None, 32)     0           ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, None, 32)     0           ['dropout_10[0][0]',             \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, None, 32)    64          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 32)          0           ['layer_normalization_2[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 256)          8448        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 256)          0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 256)          0           ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 128)          32896       ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 128)          0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 128)          0           ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 1)            129         ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 454,820\n",
      "Trainable params: 454,820\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.embedding.Embedding object at 0x000001C608B09C10>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 35). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c704f935-34a6-48ca-bd50-9e9c92522916/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c704f935-34a6-48ca-bd50-9e9c92522916/assets\n"
     ]
    }
   ],
   "source": [
    "# transformer.py\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    for i in range(5): # 10\n",
    "        model.add(Conv1D(45, 32, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(MaxPooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    for i in range(2): #4\n",
    "        model.add(Dense(512))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnnlstm_model(input_a_shape, weight=1e-3):\n",
    "    cnn_filters = 32 # 128\n",
    "    cnn_kernel_size = 4 # 4\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                              beta_initializer=\"glorot_uniform\",\n",
    "                                              gamma_initializer=\"glorot_uniform\")(input1)\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(input1)\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
    "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
    "    x1 = LSTM(32)(x1) #256\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    x1 = Dense(32, activation='relu')(x1) #64\n",
    "    x1 = Dense(32, activation='relu')(x1) #64\n",
    "    outputs = Dense(1, activation='sigmoid')(x1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_semscnn_model(input_a_shape):\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    # input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "    #                                           beta_initializer=\"glorot_uniform\",\n",
    "    #                                           gamma_initializer=\"glorot_uniform\")(input1)\n",
    "    x1 = Conv1D(45, 32, strides=1)(input1) #kernel_size=11\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    squeeze = Flatten()(x1)\n",
    "    excitation = Dense(128, activation='relu')(squeeze)\n",
    "    excitation = Dense(64, activation='relu')(excitation)\n",
    "    logits = Dense(1, activation='sigmoid')(excitation)\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "\n",
    "    \"cnn\": create_cnn_model((30 * DATA_FREQ, 3)),\n",
    "    \"sem-mscnn\": create_semscnn_model((30 * DATA_FREQ, 3)),\n",
    "    \"cnn-lstm\": create_cnnlstm_model((30 * DATA_FREQ, 3)),\n",
    "    \n",
    "    ### team 4 change ###\n",
    "    \"hybrid\": create_hybrid_transformer_model((30 * DATA_FREQ, 3)),\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(config):\n",
    "    if config[\"model_name\"].split('_')[0] == \"Transformer\":\n",
    "        return create_transformer_model(input_shape=(30 * DATA_FREQ, len(config[\"channels\"])),\n",
    "                                        num_patches=config[\"num_patches\"], projection_dim=config[\"transformer_units\"],\n",
    "                                        transformer_layers=config[\"transformer_layers\"], num_heads=config[\"num_heads\"],\n",
    "                                        transformer_units=[config[\"transformer_units\"] * 2,\n",
    "                                                           config[\"transformer_units\"]],\n",
    "                                        mlp_head_units=[256, 128], num_classes=1, drop_out=config[\"drop_out_rate\"],\n",
    "                                        reg=config[\"regression\"], l2_weight=config[\"regularization_weight\"])\n",
    "    else:\n",
    "        return model_dict.get(config[\"model_name\"].split('_')[0])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"model_name\": \"hybrid\",\n",
    "        \"regression\": False,\n",
    "\n",
    "        \"transformer_layers\": 4,  # best 5\n",
    "        \"drop_out_rate\": 0.25,\n",
    "        \"num_patches\": 20,  # best\n",
    "        \"transformer_units\": 32,  # best 32\n",
    "        \"regularization_weight\": 0.001,  # best 0.001\n",
    "        \"num_heads\": 4,\n",
    "        \"epochs\": 100,  # best\n",
    "        \"channels\": [14, 18, 19, 20],\n",
    "    }\n",
    "    model = get_model(config)\n",
    "    model.build(input_shape=(1, 30 * DATA_FREQ, 10))\n",
    "    print(model.summary())\n",
    "\n",
    "    #########################\n",
    "    torch.save(model, f\"./{config['model_name']}_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b94ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50 and (epoch - 1) % 5 == 0:\n",
    "        lr *= 0.5\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train(config, fold=None):\n",
    "    \n",
    "    ### Team 4 changes ###\n",
    "    directory = config[\"data_path\"]\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.npz'):\n",
    "            data = np.load(os.path.join(directory, filename), allow_pickle=True)\n",
    "            x.append(data['x'])\n",
    "            y.append(data['y_apnea'] + data['y_hypopnea'])\n",
    "    \n",
    "    ########################################################################################\n",
    "    for i in range(FOLD):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=1)\n",
    "        if config[\"regression\"]:\n",
    "            y[i] = np.sqrt(y[i])\n",
    "            y[i][y[i] != 0] += 2\n",
    "        else:\n",
    "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n",
    "\n",
    "    ########################################################################################\n",
    "    folds = range(FOLD) if fold is None else [fold]\n",
    "    \n",
    "    for fold in folds:\n",
    "        first = True\n",
    "        for i in range(5):\n",
    "            if i != fold:\n",
    "                if first:\n",
    "                    x_train = x[i]\n",
    "                    y_train = y[i]\n",
    "                    first = False\n",
    "                else:\n",
    "                    x_train = np.concatenate((x_train, x[i]))\n",
    "                    y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "        model = get_model(config)\n",
    "        if config[\"regression\"]:\n",
    "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
    "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        else:\n",
    "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
    "                          metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        \n",
    "        print(f\"train {x_train.shape}\")\n",
    "        model.fit(x=x_train, y=y_train, batch_size=512, epochs=config[\"epochs\"], validation_split=0.1,\n",
    "                  callbacks=[early_stopper, lr_scheduler])\n",
    "        ################################################################################################################\n",
    "        model.save(config[\"model_path\"] + str(fold))\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f174015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics.py\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "class FromLogitsMixin:\n",
    "    def __init__(self, from_logits=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.sigmoid(y_pred)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class AUC(FromLogitsMixin, tf.metrics.AUC):\n",
    "    ...\n",
    "\n",
    "\n",
    "class BinaryAccuracy(FromLogitsMixin, tf.metrics.BinaryAccuracy):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TruePositives(FromLogitsMixin, tf.metrics.TruePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalsePositives(FromLogitsMixin, tf.metrics.FalsePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TrueNegatives(FromLogitsMixin, tf.metrics.TrueNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalseNegatives(FromLogitsMixin, tf.metrics.FalseNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Precision(FromLogitsMixin, tf.metrics.Precision):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Recall(FromLogitsMixin, tf.metrics.Recall):\n",
    "    ...\n",
    "\n",
    "\n",
    "class F1Score(FromLogitsMixin, tfa.metrics.F1Score):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.accuracy_list = []\n",
    "        self.sensitivity_list = []\n",
    "        self.specificity_list = []\n",
    "        self.f1_list = []\n",
    "        self.auroc_list = []\n",
    "        self.auprc_list = []\n",
    "        self.precision_list = []\n",
    "\n",
    "    def add(self, y_test, y_predict, y_score):\n",
    "        C = confusion_matrix(y_test, y_predict, labels=(1, 0))\n",
    "        TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "\n",
    "        acc, sn, sp, pr = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP), 1. * TP / (\n",
    "                TP + FP)\n",
    "        f1 = f1_score(y_test, y_predict)\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "        auprc = average_precision_score(y_test, y_score)\n",
    "        \n",
    "        print(\"test result: \",acc, pr, sn)\n",
    "\n",
    "        self.accuracy_list.append(acc * 100)\n",
    "        self.precision_list.append(pr * 100)\n",
    "        self.sensitivity_list.append(sn * 100)\n",
    "        self.specificity_list.append(sp * 100)\n",
    "        self.f1_list.append(f1 * 100)\n",
    "        self.auroc_list.append(auc * 100)\n",
    "        self.auprc_list.append(auprc * 100)\n",
    "        \n",
    "        print(self.accuracy_list)\n",
    "        print(self.precision_list)\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        out_str = \"=========================================================================== \\n\"\n",
    "        out_str += str(self.accuracy_list) + \" \\n\"\n",
    "        out_str += str(self.precision_list) + \" \\n\"\n",
    "        out_str += str(self.sensitivity_list) + \" \\n\"\n",
    "        out_str += str(self.specificity_list) + \" \\n\"\n",
    "        out_str += str(self.f1_list) + \" \\n\"\n",
    "        out_str += str(self.auroc_list) + \" \\n\"\n",
    "        out_str += str(self.auprc_list) + \" \\n\"\n",
    "        out_str += str(\"Accuracy: %.2f -+ %.3f\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \" \\n\"\n",
    "        out_str += str(\"Precision: %.2f -+ %.3f\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Recall: %.2f -+ %.3f\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Specifity: %.2f -+ %.3f\" % (np.mean(self.specificity_list), np.std(self.specificity_list))) + \" \\n\"\n",
    "        out_str += str(\"F1: %.2f -+ %.3f\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \" \\n\"\n",
    "        out_str += str(\"AUROC: %.2f -+ %.3f\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \" \\n\"\n",
    "        out_str += str(\"AUPRC: %.2f -+ %.3f\" % (np.mean(self.auprc_list), np.std(self.auprc_list))) + \" \\n\"\n",
    "\n",
    "        out_str += str(\"$ %.1f \\pm %.1f$\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \"& \"\n",
    "\n",
    "        return out_str\n",
    "\n",
    "    def print(self):\n",
    "        print(self.get())\n",
    "        \n",
    "    def visualize(self):\n",
    "        metrics = {\n",
    "            'Accuracy': self.accuracy_list,\n",
    "            'Precision': self.precision_list,\n",
    "            'Recall (Sensitivity)': self.sensitivity_list,\n",
    "            'Specificity': self.specificity_list,\n",
    "            'F1 Score': self.f1_list,\n",
    "            'AUROC': self.auroc_list,\n",
    "            'AUPRC': self.auprc_list\n",
    "        }\n",
    "        # Setup the subplot grid\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(14, 22))\n",
    "        axes = axes.flatten()  # Flatten to make indexing easier\n",
    "\n",
    "        # Plot each metric\n",
    "        for ax, (metric_name, values) in zip(axes, metrics.items()):\n",
    "            ax.plot(values, label=f'{metric_name}', marker='o', linestyle='-')\n",
    "            ax.set_title(metric_name)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            ax.set_ylabel(metric_name)\n",
    "            ax.legend(loc='best')\n",
    "\n",
    "        # Remove the empty subplot (if any)\n",
    "        if len(metrics) % 2 != 0:\n",
    "            fig.delaxes(axes[-1])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    def save(self, path, config):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "        with open(path, \"w+\") as file:\n",
    "            file.write(str(config))\n",
    "            file.write(\"\\n\")\n",
    "            file.write(self.get())\n",
    "            file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c11f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def test(config, fold=None):\n",
    "    directory = config[\"data_path\"]\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.npz'):\n",
    "            data = np.load(os.path.join(directory, filename), allow_pickle=True)\n",
    "            x.append(data['x'])\n",
    "            y.append(data['y_apnea'] + data['y_hypopnea'])\n",
    "    ############################################################################\n",
    "    for i in range(FOLD):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]\n",
    "    ############################################################################\n",
    "    result = Result()\n",
    "    folds = range(FOLD)\n",
    "    print(folds)\n",
    "    for fold in folds:\n",
    "        print(\"n fold:\", fold)\n",
    "        x_test = x[fold]\n",
    "        if config.get(\"test_noise_snr\"):\n",
    "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
    "\n",
    "        y_test = y[fold]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
    "        model = tf.keras.models.load_model(config[\"model_path\"] + str(fold), compile=False)\n",
    "\n",
    "        predict = model.predict(x_test)\n",
    "        y_score = predict\n",
    "        y_predict = np.where(predict > 0.5, 1, 0)# For MultiClass np.argmax(y_score, axis=-1)\n",
    "\n",
    "        print(\"test result before adding:\", y_test, y_predict, y_score)\n",
    "        result.add(y_test, y_predict, y_score)\n",
    "\n",
    "    result.print()\n",
    "    result.visualize()\n",
    "    result.save(\"./results/\" + config[\"model_name\"] + \".txt\", config)\n",
    "\n",
    "    del data, x_test, y_test, model, predict, y_score, y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "084ce59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECGSPO2 [15, 16, 13]\n",
      "range(0, 5)\n",
      "n fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The given value for groups will be overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 10s 783ms/step\n",
      "test result before adding: [1 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
      " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
      " 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1\n",
      " 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0\n",
      " 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1\n",
      " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1\n",
      " 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0\n",
      " 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
      " 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 1 0 0 1 0 0 1] [[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]] [[0.78447884]\n",
      " [0.22630902]\n",
      " [0.91628885]\n",
      " [0.95343316]\n",
      " [0.5231521 ]\n",
      " [0.7281968 ]\n",
      " [0.72942364]\n",
      " [0.82298356]\n",
      " [0.40891358]\n",
      " [0.5029876 ]\n",
      " [0.28721255]\n",
      " [0.27092513]\n",
      " [0.35991076]\n",
      " [0.84102845]\n",
      " [0.29968733]\n",
      " [0.6935907 ]\n",
      " [0.8964375 ]\n",
      " [0.21880436]\n",
      " [0.8833894 ]\n",
      " [0.5096323 ]\n",
      " [0.75419873]\n",
      " [0.5412239 ]\n",
      " [0.6073922 ]\n",
      " [0.22260089]\n",
      " [0.89879954]\n",
      " [0.83313745]\n",
      " [0.9162957 ]\n",
      " [0.39539242]\n",
      " [0.580253  ]\n",
      " [0.3328652 ]\n",
      " [0.34028947]\n",
      " [0.36288664]\n",
      " [0.6037154 ]\n",
      " [0.8160728 ]\n",
      " [0.9140963 ]\n",
      " [0.6336967 ]\n",
      " [0.636872  ]\n",
      " [0.4672218 ]\n",
      " [0.81821936]\n",
      " [0.36968935]\n",
      " [0.6503759 ]\n",
      " [0.6419984 ]\n",
      " [0.75887483]\n",
      " [0.8313444 ]\n",
      " [0.8683348 ]\n",
      " [0.4968891 ]\n",
      " [0.9039004 ]\n",
      " [0.707235  ]\n",
      " [0.8883644 ]\n",
      " [0.6000155 ]\n",
      " [0.45027936]\n",
      " [0.8808159 ]\n",
      " [0.2346681 ]\n",
      " [0.21860136]\n",
      " [0.16326524]\n",
      " [0.75020486]\n",
      " [0.6084365 ]\n",
      " [0.81128335]\n",
      " [0.569719  ]\n",
      " [0.8466235 ]\n",
      " [0.7641285 ]\n",
      " [0.68910843]\n",
      " [0.44588807]\n",
      " [0.85915774]\n",
      " [0.57908326]\n",
      " [0.23517682]\n",
      " [0.7259553 ]\n",
      " [0.37798592]\n",
      " [0.812922  ]\n",
      " [0.9157007 ]\n",
      " [0.7521714 ]\n",
      " [0.7319734 ]\n",
      " [0.37963787]\n",
      " [0.75048375]\n",
      " [0.8966886 ]\n",
      " [0.27642387]\n",
      " [0.79533434]\n",
      " [0.9293032 ]\n",
      " [0.47441414]\n",
      " [0.8588831 ]\n",
      " [0.7758247 ]\n",
      " [0.89036155]\n",
      " [0.87042814]\n",
      " [0.51035756]\n",
      " [0.90399617]\n",
      " [0.8839815 ]\n",
      " [0.40920666]\n",
      " [0.6416368 ]\n",
      " [0.8420292 ]\n",
      " [0.37477073]\n",
      " [0.36743373]\n",
      " [0.24575661]\n",
      " [0.89619124]\n",
      " [0.44760907]\n",
      " [0.5023753 ]\n",
      " [0.67291415]\n",
      " [0.93385226]\n",
      " [0.39210832]\n",
      " [0.32028082]\n",
      " [0.4255912 ]\n",
      " [0.8926128 ]\n",
      " [0.8393277 ]\n",
      " [0.24919464]\n",
      " [0.15539435]\n",
      " [0.8261667 ]\n",
      " [0.7597101 ]\n",
      " [0.88942313]\n",
      " [0.8921975 ]\n",
      " [0.43070424]\n",
      " [0.85600144]\n",
      " [0.7321477 ]\n",
      " [0.45562845]\n",
      " [0.6143929 ]\n",
      " [0.6322989 ]\n",
      " [0.88467556]\n",
      " [0.68125284]\n",
      " [0.16905159]\n",
      " [0.6985605 ]\n",
      " [0.91977787]\n",
      " [0.90718   ]\n",
      " [0.06063601]\n",
      " [0.8364163 ]\n",
      " [0.71888065]\n",
      " [0.9050291 ]\n",
      " [0.28169742]\n",
      " [0.62960464]\n",
      " [0.6899385 ]\n",
      " [0.4488239 ]\n",
      " [0.5085112 ]\n",
      " [0.1837899 ]\n",
      " [0.48147985]\n",
      " [0.3303092 ]\n",
      " [0.5506173 ]\n",
      " [0.73843783]\n",
      " [0.45641017]\n",
      " [0.8810621 ]\n",
      " [0.95869404]\n",
      " [0.91884005]\n",
      " [0.584226  ]\n",
      " [0.90853965]\n",
      " [0.48062602]\n",
      " [0.30192438]\n",
      " [0.68597984]\n",
      " [0.9074927 ]\n",
      " [0.6521155 ]\n",
      " [0.8819407 ]\n",
      " [0.3504699 ]\n",
      " [0.48638016]\n",
      " [0.9277441 ]\n",
      " [0.61151564]\n",
      " [0.49196374]\n",
      " [0.61893165]\n",
      " [0.61120266]\n",
      " [0.69911   ]\n",
      " [0.52308446]\n",
      " [0.87558705]\n",
      " [0.39632013]\n",
      " [0.7692829 ]\n",
      " [0.9595942 ]\n",
      " [0.93029577]\n",
      " [0.7313862 ]\n",
      " [0.52887434]\n",
      " [0.67313296]\n",
      " [0.40399173]\n",
      " [0.5329273 ]\n",
      " [0.75430346]\n",
      " [0.8861591 ]\n",
      " [0.6172488 ]\n",
      " [0.7487362 ]\n",
      " [0.8820661 ]\n",
      " [0.92729586]\n",
      " [0.8028859 ]\n",
      " [0.5035991 ]\n",
      " [0.8694351 ]\n",
      " [0.55466545]\n",
      " [0.7493951 ]\n",
      " [0.8178115 ]\n",
      " [0.7804928 ]\n",
      " [0.44005847]\n",
      " [0.34652105]\n",
      " [0.84473485]\n",
      " [0.8672751 ]\n",
      " [0.91273403]\n",
      " [0.7952562 ]\n",
      " [0.7796913 ]\n",
      " [0.9157427 ]\n",
      " [0.57262933]\n",
      " [0.20493238]\n",
      " [0.68974054]\n",
      " [0.35326198]\n",
      " [0.7543356 ]\n",
      " [0.48155093]\n",
      " [0.84522635]\n",
      " [0.8617178 ]\n",
      " [0.30945796]\n",
      " [0.8498157 ]\n",
      " [0.802234  ]\n",
      " [0.824194  ]\n",
      " [0.3482736 ]\n",
      " [0.87887037]\n",
      " [0.46290144]\n",
      " [0.6951844 ]\n",
      " [0.46086532]\n",
      " [0.19115363]\n",
      " [0.869309  ]\n",
      " [0.8677102 ]\n",
      " [0.30555105]\n",
      " [0.66320556]\n",
      " [0.8684932 ]\n",
      " [0.73627985]\n",
      " [0.7422669 ]\n",
      " [0.6472519 ]\n",
      " [0.77072227]\n",
      " [0.82339007]\n",
      " [0.5078704 ]\n",
      " [0.54672945]\n",
      " [0.36626643]\n",
      " [0.35154918]\n",
      " [0.79344547]\n",
      " [0.6865401 ]\n",
      " [0.2046246 ]\n",
      " [0.9356268 ]\n",
      " [0.69635874]\n",
      " [0.56099224]\n",
      " [0.80297375]\n",
      " [0.4940226 ]\n",
      " [0.81518215]\n",
      " [0.8948598 ]\n",
      " [0.5708458 ]\n",
      " [0.77724254]\n",
      " [0.74551183]\n",
      " [0.5224336 ]\n",
      " [0.79948384]\n",
      " [0.6569744 ]\n",
      " [0.68490714]\n",
      " [0.8810273 ]\n",
      " [0.4161798 ]\n",
      " [0.29411164]\n",
      " [0.70199764]\n",
      " [0.1604706 ]\n",
      " [0.33868146]\n",
      " [0.39031807]\n",
      " [0.2826982 ]\n",
      " [0.81003153]\n",
      " [0.67496145]\n",
      " [0.7267383 ]\n",
      " [0.5681289 ]\n",
      " [0.6646946 ]\n",
      " [0.58510166]\n",
      " [0.45192626]\n",
      " [0.39120188]\n",
      " [0.55416507]\n",
      " [0.35188338]\n",
      " [0.1834348 ]\n",
      " [0.81113464]\n",
      " [0.72954684]\n",
      " [0.72224325]\n",
      " [0.3678648 ]\n",
      " [0.80282944]\n",
      " [0.62181556]\n",
      " [0.81384647]\n",
      " [0.68966615]\n",
      " [0.86574644]\n",
      " [0.8336392 ]\n",
      " [0.8787526 ]\n",
      " [0.77055466]\n",
      " [0.5810423 ]\n",
      " [0.45656422]\n",
      " [0.5128596 ]\n",
      " [0.44983864]\n",
      " [0.5076591 ]\n",
      " [0.44152945]\n",
      " [0.57394576]\n",
      " [0.87556213]\n",
      " [0.6932485 ]\n",
      " [0.57708585]\n",
      " [0.8223446 ]\n",
      " [0.2722034 ]\n",
      " [0.4994908 ]\n",
      " [0.7804779 ]\n",
      " [0.82661414]\n",
      " [0.8442069 ]\n",
      " [0.513288  ]\n",
      " [0.7526206 ]\n",
      " [0.8968722 ]\n",
      " [0.93086636]\n",
      " [0.30327094]\n",
      " [0.7577281 ]\n",
      " [0.8987579 ]\n",
      " [0.3595113 ]\n",
      " [0.20893158]\n",
      " [0.93435884]\n",
      " [0.86747694]\n",
      " [0.9442458 ]\n",
      " [0.95988226]\n",
      " [0.3547666 ]\n",
      " [0.7563483 ]\n",
      " [0.8613827 ]\n",
      " [0.7751223 ]\n",
      " [0.95781434]\n",
      " [0.3875967 ]\n",
      " [0.935252  ]\n",
      " [0.62376446]\n",
      " [0.4497052 ]\n",
      " [0.8476475 ]\n",
      " [0.7675661 ]\n",
      " [0.4135925 ]\n",
      " [0.68600804]\n",
      " [0.6149223 ]\n",
      " [0.89605874]\n",
      " [0.29445478]\n",
      " [0.8563646 ]\n",
      " [0.78557986]\n",
      " [0.71828   ]\n",
      " [0.8181025 ]\n",
      " [0.8815949 ]\n",
      " [0.54085916]\n",
      " [0.1677327 ]\n",
      " [0.91813934]\n",
      " [0.13520345]\n",
      " [0.56964463]\n",
      " [0.86047167]\n",
      " [0.5827766 ]\n",
      " [0.4474553 ]\n",
      " [0.5240895 ]\n",
      " [0.544678  ]\n",
      " [0.4748907 ]\n",
      " [0.39739063]\n",
      " [0.47641322]\n",
      " [0.91343045]\n",
      " [0.35122493]\n",
      " [0.4386567 ]\n",
      " [0.5827054 ]\n",
      " [0.8920537 ]\n",
      " [0.57311434]\n",
      " [0.6436509 ]\n",
      " [0.92249006]\n",
      " [0.94922715]\n",
      " [0.50692016]\n",
      " [0.7038883 ]\n",
      " [0.2837218 ]\n",
      " [0.5470061 ]\n",
      " [0.8835892 ]\n",
      " [0.9498155 ]\n",
      " [0.8067086 ]\n",
      " [0.8070922 ]\n",
      " [0.47876891]\n",
      " [0.21783778]\n",
      " [0.82771134]\n",
      " [0.882655  ]\n",
      " [0.82575136]\n",
      " [0.42083997]\n",
      " [0.69858545]\n",
      " [0.548274  ]\n",
      " [0.8898117 ]\n",
      " [0.23440109]\n",
      " [0.5638411 ]\n",
      " [0.8151093 ]\n",
      " [0.6474225 ]\n",
      " [0.6806251 ]\n",
      " [0.81150407]\n",
      " [0.29954228]\n",
      " [0.42697582]\n",
      " [0.516126  ]\n",
      " [0.46692437]\n",
      " [0.67180735]\n",
      " [0.46891373]\n",
      " [0.4325636 ]\n",
      " [0.1743784 ]\n",
      " [0.32504418]\n",
      " [0.47532067]\n",
      " [0.9429437 ]\n",
      " [0.54511136]\n",
      " [0.3500528 ]\n",
      " [0.5773145 ]\n",
      " [0.8694484 ]\n",
      " [0.91425365]\n",
      " [0.40476826]\n",
      " [0.65182936]\n",
      " [0.25095212]\n",
      " [0.23798592]\n",
      " [0.87428814]]\n",
      "test result:  0.7539267015706806 0.7105263157894737 0.9174757281553398\n",
      "[75.39267015706807]\n",
      "[71.05263157894737]\n",
      "n fold: 1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ./hybrid_model.pt1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19496/3526823749.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m     }\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# train(config, 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19496/2240207903.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(config, fold)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_path\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at ./hybrid_model.pt1"
     ]
    }
   ],
   "source": [
    "# main_chat.py\n",
    "\n",
    "# 0- E1 - M2\n",
    "# 1- E2 - M1\n",
    "\n",
    "# 2- F3 - M2\n",
    "# 3- F4 - M1\n",
    "# 4- C3 - M2\n",
    "# 5- C4 - M1\n",
    "# 6- O1 - M2\n",
    "# 7- O2 - M1\n",
    "\n",
    "# 8- ECG3 - ECG1\n",
    "\n",
    "# 9- CANULAFLOW\n",
    "# 10- AIRFLOW\n",
    "# 11- CHEST\n",
    "# 12- ABD\n",
    "\n",
    "# 13- SAO2\n",
    "# 14- CAP\n",
    "######### ADDED IN THIS STEP #########\n",
    "# 15- RRI\n",
    "# 16 Ramp\n",
    "\n",
    "sig_dict_chat = {\n",
    "    \"EOG\": [0, 1],\n",
    "    \"EEG\": [4, 5],\n",
    "    \"ECG\": [15,16],\n",
    "    \"Resp\": [9, 10],\n",
    "    \"SPO2\": [13],\n",
    "    \"CO2\": [14],\n",
    "}\n",
    "\n",
    "channel_list_chat = [\n",
    "    [\"ECG\", \"SPO2\"],\n",
    "\n",
    "]\n",
    "\n",
    "for ch in channel_list_chat:\n",
    "    chs = []\n",
    "    chstr = \"\"\n",
    "    for name in ch:\n",
    "        chstr += name\n",
    "        chs = chs + sig_dict_chat[name]\n",
    "    print(chstr, chs)\n",
    "    config = {\n",
    "        \"data_path\": \"./data/chat/dataloading\",\n",
    "        \"model_path\": \"./hybrid_model.pt\",\n",
    "        \"model_name\": \"hybrid_\"+ chstr,\n",
    "        \"regression\": False,\n",
    "\n",
    "        \"transformer_layers\": 5,  # best 5\n",
    "        \"drop_out_rate\": 0.25,  # best 0.25\n",
    "        \"num_patches\": 30,  # best 30 TBD\n",
    "        \"transformer_units\": 32,  # best 32\n",
    "        \"regularization_weight\": 0.001,  # best 0.001\n",
    "        \"num_heads\": 3,\n",
    "        \"epochs\": 40,  # best 200\n",
    "        \"channels\": chs,\n",
    "    }\n",
    "    # train(config, 0)\n",
    "    test(config, 0)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee4ce594",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data/nch/dataloading'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19496/1688081424.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;34m\"channels\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mchs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     }\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19496/368047059.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(config, fold)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './data/nch/dataloading'"
     ]
    }
   ],
   "source": [
    "# main_nch.py\n",
    "\n",
    "# \"EOG LOC-M2\",  # 0\n",
    "# \"EOG ROC-M1\",  # 1\n",
    "# \"EEG C3-M2\",  # 2\n",
    "# \"EEG C4-M1\",  # 3\n",
    "# \"ECG EKG2-EKG\",  # 4\n",
    "#\n",
    "# \"RESP PTAF\",  # 5\n",
    "# \"RESP AIRFLOW\",  # 6\n",
    "# \"RESP THORACIC\",  # 7\n",
    "# \"RESP ABDOMINAL\",  # 8\n",
    "# \"SPO2\",  # 9\n",
    "# \"CAPNO\",  # 10\n",
    "\n",
    "######### ADDED IN THIS STEP #########\n",
    "# RRI #11\n",
    "# Ramp #12\n",
    "# Demo #13\n",
    "\n",
    "\n",
    "sig_dict_nch = {\"EOG\": [0, 1],\n",
    "            \"EEG\": [2, 3],\n",
    "            \"RESP\": [5, 6],\n",
    "            \"SPO2\": [9],\n",
    "            \"CO2\": [10],\n",
    "            \"ECG\": [11, 12],\n",
    "            \"DEMO\": [13],\n",
    "            }\n",
    "\n",
    "channel_list_nch = [\n",
    "\n",
    "    [\"ECG\", \"SPO2\"],\n",
    "\n",
    "]\n",
    "\n",
    "for ch in channel_list_nch:\n",
    "    chs = []\n",
    "    chstr = \"\"\n",
    "    for name in ch:\n",
    "        chstr += name\n",
    "        chs = chs + sig_dict_nch[name]\n",
    "    config = {\n",
    "        \"data_path\": \"./data/nch/dataloading\",\n",
    "        \"model_path\": \"./saved_model.pt\", # TODO: this is hybrid; the original is using sem-mscnn, but we need to modify transformer.py (above) to generate a .pt file for it\n",
    "        \"model_name\": \"hybrid_\"+ chstr, \n",
    "        \"regression\": False,\n",
    "\n",
    "        \"transformer_layers\": 5,  # best 5\n",
    "        \"drop_out_rate\": 0.25,  # best 0.25\n",
    "        \"num_patches\": 30,  # best 30 TBD\n",
    "        \"transformer_units\": 32,  # best 32\n",
    "        \"regularization_weight\": 0.001,  # best 0.001\n",
    "        \"num_heads\": 4,\n",
    "        \"epochs\": 200,  # best 200\n",
    "        \"channels\": chs,\n",
    "    }\n",
    "    train(config, 0)\n",
    "    test(config, 0)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab26938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
