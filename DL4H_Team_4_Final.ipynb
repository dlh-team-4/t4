{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kRV_X7toK6uf",
        "gBBafnx9LNyO",
        "b5gyHxTQPhSa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 598: Deep Learning for Healthcare\n",
        "- Instructor: Dr. Jimeng Sun\n",
        "- Team Members:\n",
        "  - Carmelita Valimento\n",
        "  - William Su\n",
        "  - Austin Harmon\n",
        "\n",
        "## Final Project\n",
        "Replication Paper #42: “Investigating Sleep Apnea in Children with a Specialized Multi-Modal Transformer”\n",
        "\n",
        "\n",
        "## GitHub Repository Link\n",
        "- Original Paper: https://github.com/healthylaife/Pediatric-Apnea-Detection\n",
        "- Team 4 Repository: https://github.com/dlh-team-4/t4\n",
        "\n",
        "## Demo Video Link\n",
        "- https://drive.google.com/file/d/13CFhCvvBsJvxAmK8qwxcH7Boe2T_jbjg/view?usp=drive_link\n",
        "\n",
        "## Notice\n",
        "- Since we are dealing with large datasets, we only included snippets of the parts being requested in each section for reference. All output logs are embedding in the notebooks checked into our GitHub repository."
      ],
      "metadata": {
        "id": "hubt-Ol1dFfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Sleep apnea in children poses a significant health concern, affecting approximately one to five percent of children in the United States. Unlike adult sleep apnea, pediatric sleep apnea presents distinct clinical causes and characteristics, demanding specialized attention. However, research dedicated to pediatric sleep apnea detection has been comparatively limited, especially concerning at-home testing tools and algorithmic approaches for automatic detection. Addressing this gap is crucial due to the potential adverse effects of untreated pediatric sleep apnea on both physical and mental health.\n",
        "\n",
        "Polysomnography, the gold standard for diagnosing sleep-related breathing disorders, including apnea and hypopnea, presents numerous challenges, such as complexity, cost, and the need for clinical involvement. Consequently, there is a growing interest in developing accessible and effective diagnostic methods, particularly for pediatric populations, to mitigate the limitations of traditional polysomnography.\n",
        "\n",
        "Recent advancements in artificial intelligence (AI) have spurred research into machine learning-based approaches for diagnosing sleep apnea without relying on polysomnography. While some studies have shown promising results in adults, research focusing on pediatric populations remains scarce. This scarcity underscores the importance of the present study, which introduces a machine learning-based model specifically tailored for detecting apnea events in children using commonly collected sleep signals.\n",
        "\n",
        "The paper proposed a machine learning-based model for detecting obstructive sleep apnea-hypopnea syndrome (OSAHS) in pediatric patients using commonly collected sleep signals. It aimed to address the gap in pediatric sleep apnea detection by presenting a method that could achieve adult-level performance in detecting OSAHS patterns in children. It also introduced a customized transformer-based architecture for detecting OSAHS, which showed superior performance compared to existing methods. It utilized a novel data representation technique to handle polysomnography modalities, enabling the model to effectively process signals from different sources.\n",
        "\n",
        "The study extensively explored the role of different combinations of common modalities and demonstrated that using only two easier-to-collect signals (ECG and SpO2) could achieve close to maximum performance. The proposed method outperformed state-of-the-art methods across two public datasets, as determined by the F1-score and AUROC measures. Additionally, using only ECG and SpO2 signals, which are easier to collect at home, the method achieved very competitive results, addressing concerns about collecting various sleep signals from children outside the clinic. The paper contributes significantly to the research regime by addressing the gap in pediatric sleep apnea detection, which has been understudied compared to adult sleep apnea, introducing a novel machine learning-based approach specifically tailored for pediatric OSAHS detection, which can potentially improve the accessibility of pediatric sleep apnea testing and treatment interventions, and demonstrating the feasibility of achieving adult-level performance in detecting OSAHS in children, thereby advancing the field towards more accessible and timely diagnosis and treatment of pediatric sleep disorders.\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "We covered two hypotheses presented by the original paper:\n",
        "\n",
        "1.   Hypothesis 1: It is possible to achieve adult-level performance in detecting obstructive sleep apnea-hypopnea syndrome (OSAHS) in pediatric populations using machine learning-based methods.\n",
        "2.   Hypothesis 2: It is possible to achieve polysomnography (PSG)-level performance using only two easier-to-collect signals: ECG and SpO2.\n",
        "\n",
        "We want to verify that using ECG and SpO2 signal data, we can employ machine learning-based methods to detect OSAHS in pediatric populations. The paper included two types of runs -- one with demographics and one without. We limited our scope to replicating the paper without the demographics. Our test plan is to screen the complete polysomnography edfs from CHAT and NCHDB and use the preprocessed datasets to train and test separate Multi-Modal Tranformer models for each dataset."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment\n",
        "\n",
        "The version of python being used is Python version: 3.9.7.\n",
        "\n",
        "The original code was merged into several notebook files. Each file represented a section of the program that could be run individually. While we initially started with mounting from Google Drive, we eventually resorted to running locally due to upload limitations.\n",
        "\n",
        "To run the notebook files, it was necessary to import multiple libraries, including pytorch, tensorflow, keras, numpy, pandas, glob, os, random, scipy, and biosppy. Some of the notebooks have unversioned installation commands in them, but we took note of the versions we actually used to make future replications easier:\n",
        "- mne: 1.7.0\n",
        "- six: 1.16.0\n",
        "- numpy: 1.22.4\n",
        "- pandas: 1.2.5\n",
        "- matplotlib: 3.8.4\n",
        "- scipy: 1.7.3\n",
        "- tensorflow (used by keras and tensorflow_addons): 2.9.1\n",
        "- keras: 2.9.0\n",
        "- tensorflow_addons: 0.22.0\n",
        "- torch (PyTorch): 1.10.0\n",
        "- miniconda 24.3.0\n",
        "- cudatoolkit 11.2\n",
        "- cudnn 8.1.0\n",
        "\n",
        "For ease of installation, we also included a `requirements.txt` file in our GitHub repository so the following command can be run locally:"
      ],
      "metadata": {
        "id": "vbDw99wcwYFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip3 install -r ./requirements.txt"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "\n",
        "### Source\n",
        "The data used in this study was obtained from National Sleep Research Resource (NSRR). We are using the Childhood Adenotonsillectomy Trial (CHAT) data. A formal data access request was made by filling out a form with information about our research project. After approval, an access token was provided, which is used for downloading the necessary dataset files. The appropriate files were uploaded to Google Drive and used by our program.\n",
        "\n",
        "### Installation\n",
        "1. To install the data, we requested access by first taking the UIUC HIPAA training and then presenting our use case to NSRR for approval.\n",
        "2. Once approved, we used [nsrr-gem](https://github.com/nsrr/nsrr-gem/blob/master/README.md#prerequisites) to download the datasets we needed.\n",
        "  - For the CHAT data:\n",
        "    - We executed the command:\n",
        "      `nsrr download chat/polysomnography/edfs/baseline`\n",
        "    - We also needed the NSRR annotations for each edf file downloaded using the previous command:\n",
        "      `nsrr download chat/polysomnography/annotations-events-nsrr/baseline`\n",
        "  - For the NCH data, we executed these commands:\n",
        "    - `nsrr download nchsdb/sleep_data --fast --file=\".*\\.edf$\"`\n",
        "    - `nsrr download nchsdb/sleep_data --fast --file=\".*\\.tsv$\"`\n",
        "    - `nsrr download nchsdb/sleep_data --fast --file=\".*\\.annot$\"`\n",
        "3. Once downloaded, we took the dataset directory and supplied it to the directory variables in our python notebook.\n",
        "\n",
        "### Statistics\n",
        "1. CHAT Dataset - Childhood Adenotonsillectomy Trial (CHAT) is a multi-center, single-blind, randomized, controlled trial designed to test whether after a 7-month observation period, children, ages 5 to 9.9 years, assessed at baseline and at 7-months with standardized full polysomnography with central scoring at the Brigham and Women’s Sleep Reading Center, will show greater levels of neurocognitive functioning, specifically in the attention-executive functioning domain, than children randomized to watchful waiting plus supportive care (WWSC). In total, 1,447 children had screening polysomnographs and 464 were randomized to treatment.\n",
        "  - EDF file + Annotation XML file pairs (under `chat/polysomnography/edfs/baseline`)\n",
        "    - Total number of pairs downloaded: 453\n",
        "    - Total size in memory: 477 GB\n",
        "    - Total number of files that passed criteria checking and produced a .npz file: 330 files\n",
        "    - Total size in memory of generated .npz files during preprocessing: 120 GB\n",
        "    - Total size in memory of generated .npz files during dataloading: 21.4 GB\n",
        "      \n",
        "2. NCH Dataset - Sleep Data bank introduced by the Nationwide Children's Hospital (NCH) and Carnegie Mellon University (CMU). This dataset has 3,984 pediatric sleep studies on 3,673 unique patients conducted at NCH in Columbus, Ohio, USA between 2017 and 2019, along with the patients' longitudinal clinical data.\n",
        "  -  EDF file + Annotation XML file + TSV file\n",
        "    - Total number of 3-tuples downloaded: 3984 (for each *.annot, *.edf, *.tsv)\n",
        "    - Total size in memory: 2.08 TB\n",
        "    - Total number of files that passed criteria checking and produced a .npz file:\n",
        "    - Total size in memory of generated .npz files during preprocessing: 28\n",
        "    - Total size in memory of generated .npz files during dataloading: 8.66 GB\n",
        "\n",
        "### Data process\n",
        "After the data was downloaded, the information was preprocessed to produce .npz files. Compressing the data in another format was a necessary step to prepare the data for the data loader. The data loader produces additional compressed files that are designed to be used with the main training function.\n",
        "\n",
        "This is reflected by the order of running the notebooks:\n",
        "\n",
        " 1. **CHAT_Preprocessor.ipynb** - takes the raw .edf and .xml files from the CHAT dataset and compresses each one into its own .npz file.\n",
        " 2. **CHAT_Data_Loader.ipynb** - takes compressed signal data in the .npz files from the previous notebook, labels them using by analyzing the ECG signal data, compresses them further into K number of files, representative of each fold.\n",
        " 3.  **NCH_Preprocessor.ipynb** - takes the raw .edf, .annot and .tsv files from the NCH dataset and compresses each one into its own .npz file.\n",
        " 4. **NCH_Data_Loader.ipynb** - takes .npz files from the previous notebook and compresses them further into K number of files, representative of each fold. Initially, it uses a reference file `AHI.csv`, that presumable contains the computed Apnea-Hypopnea Index for each signal data file (.edf), but no AHI variable was available in the NSRR NCHDB dataset. Since we do not know how this was computed nor was there any AHI computation code checked into the orignal repository, we changed our approach to using the same ECG analysis that was also used in analyzing the CHAT dataset.\n",
        " 5.  **Trainer_Evaluator.ipynb** - builds the `{}_model.pt` model file, trains it, and evaluates its performance against a given set of metrics. The summary of results are writted in a `results` folder generated in the same directory as the notebook. We had to build separate models because the channel structures for the NCH and CHAT signal data are different.\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Code\n",
        "NOTE: this is a code snippet, we ran all the code locally and the notebooks with embedded results are all checked into our GitHub repository.\n",
        "\n",
        "The CHAT preprocessor is available [here](https://github.com/dlh-team-4/t4/blob/main/CHAT_Preprocessor.ipynb).\n",
        "The NCH preprocessor is available [here](https://github.com/dlh-team-4/t4/blob/main/NCH_Preprocessor.ipynb)."
      ],
      "metadata": {
        "id": "kRV_X7toK6uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHAT_Preprocessor.ipynb\n",
        "\n",
        "# Define directories for input data and output preprocessed data\n",
        "PSG_DIR = \"./data/chat/\"\n",
        "OUT_DIR = './data/chat/preprocessed'\n",
        "\n",
        "# Set constants for data processing\n",
        "THRESHOLD = 3\n",
        "NUM_WORKER = 1\n",
        "FREQ = 128.0  # Target frequency to which the data should be resampled\n",
        "EPOCH_LENGTH = 30.0  # Length of each epoch in seconds\n",
        "\n",
        "# Define list of channels used in the study\n",
        "channels = [\n",
        "    'E1', 'E2', 'F3', 'F4', 'C3', 'C4', 'M1', 'M2', 'O1', 'O2',\n",
        "    'ECG1', 'ECG3', 'CANNULAFLOW', 'AIRFLOW', 'CHEST', 'ABD', 'SAO2', 'CAP'\n",
        "]\n",
        "\n",
        "# Dictionaries mapping event names from the XML to numerical codes\n",
        "APNEA_EVENT_DICT = {\n",
        "    \"Obstructive apnea|Obstructive Apnea\": 2,\n",
        "    \"Central apnea|Central Apnea\": 2,\n",
        "}\n",
        "HYPOPNEA_EVENT_DICT = {\n",
        "    \"Hypopnea|Hypopnea\": 1,\n",
        "}\n",
        "POS_EVENT_DICT = APNEA_EVENT_DICT.copy()\n",
        "POS_EVENT_DICT.update(HYPOPNEA_EVENT_DICT)\n",
        "NEG_EVENT_DICT = {\n",
        "    'Stage 1 sleep|1': 0,\n",
        "    'Stage 2 sleep|2': 0,\n",
        "    'Stage 3 sleep|3': 0,\n",
        "    'REM sleep|5': 0,\n",
        "}\n",
        "WAKE_DICT = {\n",
        "    \"Wake\": 10,\n",
        "    \"Wake|0\": 10\n",
        "}\n",
        "\n",
        "# Placeholder function to pass data through unchanged\n",
        "def identity(df):\n",
        "    return df\n",
        "\n",
        "# Function to parse XML annotations into a pandas DataFrame\n",
        "def parseScoredEvents(annotation_path):\n",
        "    with open(annotation_path, \"r\") as f:\n",
        "        xml_data = f.read()\n",
        "    root = ET.fromstring(xml_data)\n",
        "    scored_events = []\n",
        "    for scored_event in root.find('ScoredEvents'):\n",
        "        event_data = {\n",
        "            'event_type': scored_event.find('EventType').text,\n",
        "            'description': scored_event.find('EventConcept').text,\n",
        "            'onset': scored_event.find('Start').text,\n",
        "            'duration': scored_event.find('Duration').text,\n",
        "            'clock_time': scored_event.find('ClockTime').text if scored_event.find('ClockTime') is not None else None,\n",
        "            'signal_location': scored_event.find('SignalLocation').text if scored_event.find('SignalLocation') is not None else None\n",
        "        }\n",
        "        scored_events.append(event_data)\n",
        "    df = pd.DataFrame(scored_events)\n",
        "    return df.drop(df.index[0])\n",
        "\n",
        "# Function to load EDF file, attach annotations, and preprocess the data\n",
        "def load_study_chat(edf_path, annotation_path, annotation_func, preload=False, exclude=[], verbose='CRITICAL'):\n",
        "    raw = mne.io.read_raw_edf(input_fname=edf_path, exclude=exclude, preload=preload, verbose=verbose)\n",
        "    df = annotation_func(parseScoredEvents(annotation_path))\n",
        "    annotations = mne.Annotations(df.onset, df.duration, df.description)\n",
        "    raw.set_annotations(annotations)\n",
        "    raw.rename_channels({name: name.upper() for name in raw.info['ch_names']})\n",
        "    return raw\n",
        "\n",
        "# Main preprocessing function\n",
        "def preprocess(path, annotation_modifier, out_dir):\n",
        "    print(path)\n",
        "    raw = load_study_chat(path[0], path[1], annotation_modifier, verbose=True)\n",
        "    if not all([name in raw.ch_names for name in channels]):\n",
        "        print(\"study \" + os.path.basename(path[0]) + \" skipped since insufficient channels\")\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=POS_EVENT_DICT, chunk_duration=1.0)\n",
        "    except ValueError as e:\n",
        "        print(str(e))\n",
        "        print(\"No Chunk found!\")\n",
        "        return 0\n",
        "\n",
        "    # Attempt to identify different types of sleep events\n",
        "    is_apnea_available, is_hypopnea_available = True, True\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=APNEA_EVENT_DICT, chunk_duration=1.0)\n",
        "    except ValueError:\n",
        "        is_apnea_available = False\n",
        "    try:\n",
        "        hypopnea_events, event_ids = mne.events_from_annotations(raw, event_id=HYPOPNEA_EVENT_DICT, chunk_duration=1.0)\n",
        "    except ValueError:\n",
        "        is_hypopnea_available = False\n",
        "    wake_events, event_ids = mne.events_from_annotations(raw, event_id=WAKE_DICT, chunk_duration=1.0)\n",
        "\n",
        "    # Processing and labeling the data according to detected events\n",
        "    sfreq = raw.info['sfreq']\n",
        "    tmax = EPOCH_LENGTH - 1. / sfreq\n",
        "    raw = raw.pick_channels(channels, ordered=True)\n",
        "    fixed_events = mne.make_fixed_length_events(raw, id=0, duration=EPOCH_LENGTH, overlap=0.)\n",
        "    try:\n",
        "        epochs = mne.Epochs(raw, fixed_events, event_id=[0], tmin=0, tmax=tmax, baseline=None, preload=True, proj=False)\n",
        "        epochs.load_data()\n",
        "    except AssertionError:\n",
        "        return 0\n",
        "    if sfreq != FREQ:\n",
        "        epochs = epochs.resample(FREQ, npad='auto', n_jobs=8)\n",
        "    data = epochs.get_data()\n",
        "    if is_apnea_available:\n",
        "        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n",
        "    if is_hypopnea_available:\n",
        "        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n",
        "    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n",
        "\n",
        "    starts = (epochs.events[:, 0] / sfreq).astype(int)\n",
        "    labels_apnea = []\n",
        "    labels_hypopnea = []\n",
        "    labels_not_awake = []\n",
        "    total_apnea_event_second = 0\n",
        "    total_hypopnea_event_second = 0\n",
        "    for seq in range(data.shape[0]):\n",
        "        epoch_set = set(range(starts[seq], starts[seq] + int(EPOCH_LENGTH)))\n",
        "        if is_apnea_available:\n",
        "            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n",
        "            total_apnea_event_second += apnea_seconds\n",
        "            labels_apnea.append(apnea_seconds)\n",
        "        else:\n",
        "            labels_apnea.append(0)\n",
        "        if is_hypopnea_available:\n",
        "            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n",
        "            total_hypopnea_event_second += hypopnea_seconds\n",
        "            labels_hypopnea.append(hypopnea_seconds)\n",
        "        else:\n",
        "            labels_hypopnea.append(0)\n",
        "        labels_not_awake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n",
        "\n",
        "    # Save the processed data to a compressed .npz file\n",
        "    np.savez_compressed(\n",
        "        out_dir + '\\\\' + os.path.basename(path[0]) + \"_\" + str(total_apnea_event_second) + \"_\" + str(total_hypopnea_event_second),\n",
        "        data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n",
        "    return data.shape[0]\n",
        "\n",
        "# Set log file for MNE processing information\n",
        "mne.set_log_file('log.txt', overwrite=False)\n",
        "\n",
        "# Iterate through each EDF file in the directory and preprocess using the 'identity' annotation modifier\n",
        "edf_files = glob.glob(PSG_DIR + \"*.edf\")\n",
        "for edf_file in edf_files:\n",
        "    annot_file = edf_file.replace(\".edf\", \"-nsrr.xml\")  # Construct the corresponding XML file path\n",
        "    preprocess((edf_file, annot_file), identity, OUT_DIR)  # Process each file pair"
      ],
      "metadata": {
        "id": "UkRYtyBpLDbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NCH_Preprocessor.ipynb\n",
        "\n",
        "# Directory where polysomnography (PSG) data files are stored\n",
        "PSG_DIR = \"./data/nch/\"\n",
        "# Directory where preprocessed data will be saved\n",
        "OUT_DIR = './data/nch/preprocessed'\n",
        "\n",
        "# Various configuration constants for data processing\n",
        "THRESHOLD = 3\n",
        "NUM_WORKER = 8  # Number of workers for parallel processing\n",
        "FREQ = 128.0  # Sampling frequency to which the data will be resampled\n",
        "EPOCH_LENGTH = 30.0  # Duration of each epoch in seconds\n",
        "SN = 3984  # Serial number, possibly for tracking or identification\n",
        "\n",
        "# List of channels to be included in the analysis\n",
        "channels = [\n",
        "    \"EOG LOC-M2\", \"EOG ROC-M1\", \"EEG C3-M2\", \"EEG C4-M1\", \"ECG EKG2-EKG\",\n",
        "    \"RESP PTAF\", \"RESP AIRFLOW\", \"RESP THORACIC\", \"RESP ABDOMINAL\", \"SPO2\", \"CAPNO\"\n",
        "]\n",
        "\n",
        "# Dictionaries for event identification and labeling\n",
        "APNEA_EVENT_DICT = {\n",
        "    \"Obstructive Apnea\": 2, \"Central Apnea\": 2, \"Mixed Apnea\": 2, \"apnea\": 2,\n",
        "    \"obstructive apnea\": 2, \"central apnea\": 2, \"Apnea\": 2\n",
        "}\n",
        "HYPOPNEA_EVENT_DICT = {\n",
        "    \"Obstructive Hypopnea\": 1, \"Hypopnea\": 1, \"hypopnea\": 1, \"Mixed Hypopnea\": 1, \"Central Hypopnea\": 1\n",
        "}\n",
        "POS_EVENT_DICT = {\n",
        "    **APNEA_EVENT_DICT, **HYPOPNEA_EVENT_DICT\n",
        "}\n",
        "NEG_EVENT_DICT = {\n",
        "    'Sleep stage N1': 0, 'Sleep stage N2': 0, 'Sleep stage N3': 0, 'Sleep stage R': 0\n",
        "}\n",
        "WAKE_DICT = {\n",
        "    \"Sleep stage W\": 10\n",
        "}\n",
        "\n",
        "# Functions to handle and modify annotations\n",
        "def identity(df):\n",
        "    \"\"\"Returns the DataFrame as is without any modifications.\"\"\"\n",
        "    return df\n",
        "\n",
        "def apnea2bad(df):\n",
        "    \"\"\"Replaces any apnea-related descriptions with 'badevent'.\"\"\"\n",
        "    df = df.replace(r'.*pnea.*', 'badevent', regex=True)\n",
        "    print(\"bad replaced!\")\n",
        "    return df\n",
        "\n",
        "def wake2bad(df):\n",
        "    \"\"\"Replaces 'Sleep stage W' with 'badevent'.\"\"\"\n",
        "    return df.replace(\"Sleep stage W\", 'badevent')\n",
        "\n",
        "def change_duration(df, label_dict=POS_EVENT_DICT, duration=EPOCH_LENGTH):\n",
        "    \"\"\"Adjusts the duration for events defined in label_dict to the specified epoch length.\"\"\"\n",
        "    for key in label_dict:\n",
        "        df.loc[df.description == key, 'duration'] = duration\n",
        "    print(\"change duration!\")\n",
        "    return df\n",
        "\n",
        "def load_study_chat(edf_path, annotation_path, annotation_func, preload=False, exclude=[], verbose='CRITICAL'):\n",
        "    \"\"\"Loads EDF files, applies annotations, and processes according to the specified functions.\"\"\"\n",
        "    raw = mne.io.read_raw_edf(input_fname=edf_path, exclude=exclude, preload=preload, verbose=verbose)\n",
        "    df = annotation_func(pd.read_csv(annotation_path, sep='\\t'))\n",
        "    annotations = mne.Annotations(df.onset, df.duration, df.description)\n",
        "    raw.set_annotations(annotations)\n",
        "    raw.rename_channels({name: name.upper() for name in raw.info['ch_names']})\n",
        "    return raw\n",
        "\n",
        "def preprocess(path, annotation_modifier, out_dir):\n",
        "    \"\"\"Main preprocessing function that loads, checks, and processes each study.\"\"\"\n",
        "    print(path)\n",
        "    raw = load_study_chat(path[0], path[1], annotation_modifier, verbose=True)\n",
        "\n",
        "    # Check if all necessary channels are present\n",
        "    if not all([name in raw.ch_names for name in channels]):\n",
        "        print(\"study \" + os.path.basename(path[0]) + \" skipped since insufficient channels\")\n",
        "        return 0\n",
        "\n",
        "    # Try to extract and label apnea and hypopnea events\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=POS_EVENT_DICT, chunk_duration=1.0)\n",
        "    except ValueError as e:\n",
        "        print(str(e))\n",
        "        print(\"No Chunk found!\")\n",
        "        return 0\n",
        "\n",
        "    print(str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %s' % os.path.basename(path[0]))\n",
        "\n",
        "    # Attempt to capture apnea, hypopnea, and wake events\n",
        "    try:\n",
        "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=APNEA_EVENT_DICT, chunk_duration=1.0)\n",
        "    except ValueError:\n",
        "        is_apnea_available = False\n",
        "\n",
        "    try:\n",
        "        hypopnea_events, event_ids = mne.events_from_annotations(raw, event_id=HYPOPNEA_EVENT_DICT, chunk_duration=1.0)\n",
        "    except ValueError:\n",
        "        is_hypopnea_available = False\n",
        "\n",
        "    wake_events, event_ids = mne.events_from_annotations(raw, event_id=WAKE_DICT, chunk_duration=1.0)\n",
        "\n",
        "    sfreq = raw.info['sfreq']\n",
        "    tmax = EPOCH_LENGTH - 1. / sfreq\n",
        "    raw = raw.pick_channels(channels, ordered=True)\n",
        "    fixed_events = mne.make_fixed_length_events(raw, id=0, duration=EPOCH_LENGTH, overlap=0.)\n",
        "\n",
        "    # Create epochs from the raw data\n",
        "    try:\n",
        "        epochs = mne.Epochs(raw, fixed_events, event_id=[0], tmin=0, tmax=tmax, baseline=None, preload=True, proj=False)\n",
        "        epochs.load_data()\n",
        "    except AssertionError:\n",
        "        return 0\n",
        "\n",
        "    if sfreq != FREQ:\n",
        "        epochs = epochs.resample(FREQ, npad='auto', n_jobs=8)\n",
        "\n",
        "    data = epochs.get_data()\n",
        "    if is_apnea_available:\n",
        "        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n",
        "    if is_hypopnea_available:\n",
        "        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n",
        "    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n",
        "\n",
        "    starts = (epochs.events[:, 0] / sfreq).astype(int)\n",
        "    labels_apnea = []\n",
        "    labels_hypopnea = []\n",
        "    labels_not_awake = []\n",
        "    total_apnea_event_second = 0\n",
        "    total_hypopnea_event_second = 0\n",
        "\n",
        "    # Label each epoch based on the presence of specific events\n",
        "    for seq in range(data.shape[0]):\n",
        "        epoch_set = set(range(starts[seq], starts[seq] + int(EPOCH_LENGTH)))\n",
        "        if is_apnea_available:\n",
        "            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n",
        "            total_apnea_event_second += apnea_seconds\n",
        "            labels_apnea.append(apnea_seconds)\n",
        "        else:\n",
        "            labels_apnea.append(0)\n",
        "\n",
        "        if is_hypopnea_available:\n",
        "            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n",
        "            total_hypopnea_event_second += hypopnea_seconds\n",
        "            labels_hypopnea.append(hypopnea_seconds)\n",
        "        else:\n",
        "            labels_hypopnea.append(0)\n",
        "\n",
        "        labels_not_awake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n",
        "\n",
        "    # Filter out epochs corresponding to wake times and reformat the data\n",
        "    data = data[labels_not_awake, :, :]\n",
        "    labels_apnea = list(compress(labels_apnea, labels_not_awake))\n",
        "    labels_hypopnea = list(compress(labels_hypopnea, labels_not_awake))\n",
        "\n",
        "    # Save the processed data in a compressed format\n",
        "    np.savez_compressed(\n",
        "        out_dir + '\\\\' + os.path.basename(path[0]) + \"_\" + str(total_apnea_event_second) + \"_\" + str(total_hypopnea_event_second),\n",
        "        data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n",
        "\n",
        "    return data.shape[0]\n",
        "\n",
        "# Set log file for recording processing details\n",
        "mne.set_log_file('log.txt', overwrite=False)\n",
        "\n",
        "# Process each .edf file found in the PSG directory\n",
        "edf_files = glob.glob(PSG_DIR + \"*.edf\")\n",
        "for edf_file in edf_files:\n",
        "    annot_file = edf_file.replace(\".edf\", \".tsv\")  # Construct annotation file path\n",
        "    preprocess((edf_file, annot_file), identity, OUT_DIR)  # Process and handle data\n"
      ],
      "metadata": {
        "id": "e_h_1wR1LLsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloading Code\n",
        "NOTE: this is a code snippet, we ran all the code locally and the notebooks with embedded results are all checked into our GitHub repository.\n",
        "\n",
        "The CHAT preprocessor is available [here](https://github.com/dlh-team-4/t4/blob/main/CHAT_Data_Loader.ipynb). The NCH preprocessor is available [here](https://github.com/dlh-team-4/t4/blob/main/NCH_Data_Loader.ipynb)."
      ],
      "metadata": {
        "id": "gBBafnx9LNyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CHAT_Data_Loader.ipynb\n",
        "\n",
        "# Configuration settings for processing the signals\n",
        "CHAT_SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]  # Indices of signals to process\n",
        "chat_s_count = len(CHAT_SIGS)  # Number of signals\n",
        "\n",
        "CHAT_THRESHOLD = 3  # Not used in the provided snippet but could be a threshold value for processing\n",
        "CHAT_PREPROCESSED_PATH = 'D:\\\\data_chat_baseline_30x128_test'  # Directory where preprocessed data is stored\n",
        "CHAT_FREQ = 128  # Sampling frequency of the data\n",
        "CHAT_EPOCH_LENGTH = 30  # Duration of each epoch in seconds\n",
        "CHAT_ECG_SIG = 8  # Index of the ECG signal in the array\n",
        "CHAT_OUT_PATH = \"D:\\\\\"  # Output directory for the results\n",
        "\n",
        "# ...\n",
        "\n",
        "# Define a function to extract respiration rate intervals (RRI) from an ECG signal\n",
        "def extract_rri(signal, ir, CHUNK_DURATION):\n",
        "    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))  # Generate a time array for interpolation\n",
        "\n",
        "    # Filter the ECG signal to isolate the QRS complex\n",
        "    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * CHAT_FREQ),\n",
        "                                      frequency=[3, 45], sampling_rate=CHAT_FREQ)\n",
        "    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=CHAT_FREQ)  # Detect R peaks\n",
        "    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=CHAT_FREQ, tol=0.05)  # Correct R peak detection\n",
        "\n",
        "    # If an acceptable number of R-peaks are detected, proceed with further processing\n",
        "    if 4 < len(rpeaks) < 200:\n",
        "        rri_tm, rri_signal = rpeaks[1:] / float(CHAT_FREQ), np.diff(rpeaks) / float(CHAT_FREQ)\n",
        "        ampl_tm, ampl_signal = rpeaks / float(CHAT_FREQ), signal[rpeaks]\n",
        "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n",
        "        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n",
        "\n",
        "        # Return interpolated signals, clipped to reasonable values\n",
        "        return np.clip(rri_interp_signal, 0, 2) * 100, np.clip(amp_interp_signal, -0.001, 0.002) * 10000, True\n",
        "    else:\n",
        "        # Return zero arrays if the number of R-peaks is too few or too many\n",
        "        return np.zeros((CHAT_FREQ * CHAT_EPOCH_LENGTH)), np.zeros((CHAT_FREQ * CHAT_EPOCH_LENGTH)), False\n",
        "\n",
        "# Function to load and process data\n",
        "def load_data(path):\n",
        "    root_dir = os.path.expanduser(path)\n",
        "    file_list = os.listdir(root_dir)  # List all files in the preprocessed directory\n",
        "    length = len(file_list)\n",
        "\n",
        "    # Create folds for cross-validation, assuming data split for 5-fold cross-validation\n",
        "    study_event_counts = [i for i in range(0, length)]\n",
        "    folds = []\n",
        "    for i in range(5):\n",
        "        folds.append(study_event_counts[i::5])\n",
        "\n",
        "    x = []  # List to hold data arrays\n",
        "    y_apnea = []  # List to hold apnea labels\n",
        "    y_hypopnea = []  # List to hold hypopnea labels\n",
        "    counter = 0\n",
        "    for idx, fold in enumerate(folds):\n",
        "        first = True\n",
        "        for patient in fold:\n",
        "            rri_succ_counter = 0\n",
        "            rri_fail_counter = 0\n",
        "            counter += 1\n",
        "            print(counter)\n",
        "            study_data = np.load(CHAT_PREPROCESSED_PATH + \"\\\\\" + file_list[patient - 1], allow_pickle = True)  # Load study data\n",
        "\n",
        "            signals = study_data['data']\n",
        "            labels_apnea = study_data['labels_apnea']\n",
        "            labels_hypopnea = study_data['labels_hypopnea']\n",
        "\n",
        "            y_c = labels_apnea + labels_hypopnea  # Combine labels for apnea and hypopnea\n",
        "            neg_samples = np.where(y_c == 0)[0]  # Identify negative samples\n",
        "            pos_samples = list(np.where(y_c > 0)[0])  # Identify positive samples\n",
        "            ratio = len(pos_samples) / len(neg_samples)  # Calculate the ratio of positive to negative samples\n",
        "            neg_survived = [s for s in neg_samples if random.random() < ratio]  # Randomly down-sample negative cases\n",
        "            samples = neg_survived + pos_samples\n",
        "            signals = signals[samples, :, :]\n",
        "            labels_apnea = labels_apnea[samples]\n",
        "            labels_hypopnea = labels_hypopnea[samples]\n",
        "\n",
        "            # Initialize data array for this batch\n",
        "            data = np.zeros((signals.shape[0], CHAT_EPOCH_LENGTH * CHAT_FREQ, chat_s_count + 2))\n",
        "            for i in range(signals.shape[0]):  # Process each epoch\n",
        "                data[i, :, -1], data[i, :, -2], status = extract_rri(signals[i, CHAT_ECG_SIG, :], CHAT_FREQ,\n",
        "                                                                     float(CHAT_EPOCH_LENGTH))\n",
        "\n",
        "                if status:\n",
        "                    rri_succ_counter += 1\n",
        "                else:\n",
        "                    rri_fail_counter += 1\n",
        "\n",
        "                for j in range(chat_s_count):  # Copy signal data to the respective positions\n",
        "                    data[i, :, j] = signals[i, CHAT_SIGS[j], :]\n",
        "\n",
        "            # Aggregate data and labels\n",
        "            if first:\n",
        "                aggregated_data = data\n",
        "                aggregated_label_apnea = labels_apnea\n",
        "                aggregated_label_hypopnea = labels_hypopnea\n",
        "                first = False\n",
        "            else:\n",
        "                aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n",
        "                aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n",
        "                aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n",
        "            print(rri_succ_counter, rri_fail_counter)\n",
        "\n",
        "        x.append(aggregated_data)\n",
        "        y_apnea.append(aggregated_label_apnea)\n",
        "        y_hypopnea.append(aggregated_label_hypopnea)\n",
        "\n",
        "    return x, y_apnea, y_hypopnea\n",
        "\n",
        "x, y_apnea, y_hypopnea = load_data(CHAT_PREPROCESSED_PATH)  # Load and process data\n",
        "\n",
        "# Save the processed data to files and print the shape of the data and labels for each fold\n",
        "for i in range(5):\n",
        "        print(x[i].shape, y_apnea[i].shape, y_hypopnea[i].shape)\n",
        "        np.savez_compressed(CHAT_OUT_PATH + \"_\" + str(i), x=x[i], y_apnea=y_apnea[i], y_hypopnea=y_hypopnea[i])\n"
      ],
      "metadata": {
        "id": "N-mMqlZSLV8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NCH_Data_Loader.ipynb\n",
        "\n",
        "# Configuration settings\n",
        "NCH_SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Indices of signals to process\n",
        "nch_s_count = len(NCH_SIGS)  # Number of signals to process\n",
        "\n",
        "NCH_THRESHOLD = 3  # Threshold value, possibly for signal processing or data selection\n",
        "NCH_PREPROCESSED_PATH = \"./data/nch/preprocessed\"  # Directory containing preprocessed data files\n",
        "NCH_FREQ = 128  # Sampling frequency of the data\n",
        "NCH_EPOCH_LENGTH = 30  # Duration of each epoch in seconds\n",
        "NCH_ECG_SIG = 4  # Index of the ECG signal in the dataset\n",
        "NCH_OUT_PATH = \"./data/nch/dataloading/\"  # Output directory for the processed data\n",
        "\n",
        "# ...\n",
        "\n",
        "# Define a function to extract Respiration Rate Interval (RRI) from ECG signals\n",
        "def extract_rri(signal, ir, CHUNK_DURATION):\n",
        "    # Time vector for interpolation\n",
        "    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))\n",
        "\n",
        "    # Bandpass filter the ECG signal to isolate heartbeats\n",
        "    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\",\n",
        "                                      order=int(0.3 * NCH_FREQ), frequency=[3, 45],\n",
        "                                      sampling_rate=NCH_FREQ)\n",
        "\n",
        "    # Detect and correct R peaks using the Hamilton segmenter algorithm\n",
        "    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=NCH_FREQ)\n",
        "    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=NCH_FREQ, tol=0.05)\n",
        "\n",
        "    # Only process segments with an appropriate number of detected R-peaks\n",
        "    if 4 < len(rpeaks) < 200:\n",
        "        # Calculate RRI and amplitude signals\n",
        "        rri_tm, rri_signal = rpeaks[1:] / float(NCH_FREQ), np.diff(rpeaks) / float(NCH_FREQ)\n",
        "        ampl_tm, ampl_signal = rpeaks / float(NCH_FREQ), signal[rpeaks]\n",
        "\n",
        "        # Interpolate RRI and amplitude signals\n",
        "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n",
        "        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n",
        "\n",
        "        # Return interpolated and scaled signals\n",
        "        return np.clip(rri_interp_signal, 0, 2) * 100, np.clip(amp_interp_signal, -0.001, 0.002) * 10000, True\n",
        "    else:\n",
        "        # Return zero arrays if the number of R-peaks is too few or too many\n",
        "        return np.zeros((NCH_FREQ * NCH_EPOCH_LENGTH)), np.zeros((NCH_FREQ * NCH_EPOCH_LENGTH)), False\n",
        "\n",
        "# Function to load data, balance the dataset, and organize it for analysis\n",
        "def load_data(path):\n",
        "    root_dir = os.path.expanduser(path)  # Convert path to absolute path\n",
        "    file_list = os.listdir(root_dir)  # List files in the directory\n",
        "    length = len(file_list)\n",
        "\n",
        "    # Divide files into folds for cross-validation\n",
        "    study_event_counts = [i for i in range(0, length)]\n",
        "    folds = [study_event_counts[i::5] for i in range(5)]\n",
        "\n",
        "    x = []\n",
        "    y_apnea = []\n",
        "    y_hypopnea = []\n",
        "    counter = 0\n",
        "    for idx, fold in enumerate(folds):\n",
        "        first = True\n",
        "        for patient in fold:\n",
        "            rri_succ_counter = 0\n",
        "            rri_fail_counter = 0\n",
        "            counter += 1\n",
        "            print(counter)\n",
        "            study_data = np.load(NCH_PREPROCESSED_PATH + \"\\\\\" + file_list[patient - 1])\n",
        "\n",
        "            signals = study_data['data']\n",
        "            labels_apnea = study_data['labels_apnea']\n",
        "            labels_hypopnea = study_data['labels_hypopnea']\n",
        "\n",
        "            # Balance the dataset by undersampling negative samples\n",
        "            y_c = labels_apnea + labels_hypopnea\n",
        "            neg_samples = np.where(y_c == 0)[0]\n",
        "            pos_samples = list(np.where(y_c > 0)[0])\n",
        "            ratio = len(pos_samples) / len(neg_samples)\n",
        "            neg_survived = [s for s in neg_samples if random.random() < ratio]\n",
        "            samples = neg_survived + pos_samples\n",
        "            signals = signals[samples, :, :]\n",
        "            labels_apnea = labels_apnea[samples]\n",
        "            labels_hypopnea = labels_hypopnea[samples]\n",
        "\n",
        "            # Prepare data for each epoch including RRI extraction\n",
        "            data = np.zeros((signals.shape[0], NCH_EPOCH_LENGTH * NCH_FREQ, nch_s_count + 2))\n",
        "            for i in range(signals.shape[0]):\n",
        "                data[i, :, -1], data[i, :, -2], status = extract_rri(signals[i, NCH_ECG_SIG, :], NCH_FREQ, float(NCH_EPOCH_LENGTH))\n",
        "\n",
        "                if status:\n",
        "                    rri_succ_counter += 1\n",
        "                else:\n",
        "                    rri_fail_counter += 1\n",
        "\n",
        "                for j in range(nch_s_count):\n",
        "                    data[i, :, j] = signals[i, NCH_SIGS[j], :]\n",
        "\n",
        "            # Aggregate data for the current fold\n",
        "            if first:\n",
        "                aggregated_data = data\n",
        "                aggregated_label_apnea = labels_apnea\n",
        "                aggregated_label_hypopnea = labels_hypopnea\n",
        "                first = False\n",
        "            else:\n",
        "                aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n",
        "                aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n",
        "                aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n",
        "            print(rri_succ_counter, rri_fail_counter)\n",
        "\n",
        "        x.append(aggregated_data)\n",
        "        y_apnea.append(aggregated_label_apnea)\n",
        "        y_hypopnea.append(aggregated_label_hypopnea)\n",
        "\n",
        "    return x, y_apnea, y_hypopnea\n",
        "\n",
        "# Load and process data\n",
        "x, y_apnea, y_hypopnea = load_data(NCH_PREPROCESSED_PATH)\n",
        "\n",
        "# Output the shape of the processed datasets and save them to files\n",
        "for i in range(5):\n",
        "    print(x[i].shape, y_apnea[i].shape, y_hypopnea[i].shape)\n",
        "    np.savez_compressed(NCH_OUT_PATH + \"_\" + str(i), x=x[i], y_apnea=y_apnea[i], y_hypopnea=y_hypopnea[i])\n"
      ],
      "metadata": {
        "id": "wv76FqwGN6Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model : Multi-modal Transformer (Fayyaz et al., 2023)\n",
        "\n",
        "A model statistics report was produced as a result of running the provided model.py code. According to the report, there are six Conv1D layers and three SeparableConv1D layers. Six Dropout layers were counted, as well as nine Dense layers. There is one MultiHeadAttention mechanism present. The total number of parameters is 389284, which is also equal to the number of trainable parameters. The untrained model occupies 1.49 MB of storage.\n",
        "\n",
        "### Citation to the Original Paper\n",
        "\n",
        "- Fayyaz H, Strang A, Beheshti R. Bringing At-home Pediatric Sleep Apnea Testing Closer to Reality: A Multi-modal Transformer Approach. Proc Mach Learn Res. 2023 Aug;219:167-185. PMID: 38344396; PMCID: PMC10854997.\n",
        "\n",
        "### Citation to the Original Paper\n",
        "- Original Paper: https://github.com/healthylaife/Pediatric-Apnea-Detection\n",
        "\n",
        "### Architecture\n",
        "The model architecture is designed for processing sequential data to indentify complex patterns in high-dimensional data spaces. It begins with an input layer that normalizes data per instance, followed by a transformation into a series of patches to simulate a vision Transformer-like mechanism. These patches are then encoded and processed through multiple Transformer encoder layers, which include multi-head attention for capturing contextual relationships, and dense layers with non-linear activation for advanced feature transformation. The architecture utilizes skip connections and layer normalization to stabilize training and enhance learning effectiveness. This setup culminates in a global average pooling layer that prepares a condensed feature representation for the final prediction output, making the model highly capable of handling complex, multi-modal datasets.\n",
        "\n",
        "| Layer (type)                    | Output Shape          | Param # | Connected to                        |\n",
        "|---------------------------------|-----------------------|---------|-------------------------------------|\n",
        "| input_5 (InputLayer)            | (None, 3840, 4)       | 0       | []                                  |\n",
        "| instance_normalization_2 (InstanceNormalization) | (None, 3840, 4)  | 0   | ['input_5[0][0]']                |\n",
        "| patches_1 (Patches)             | (None, None, 768)     | 0       | ['instance_normalization_2[0][0]'] |\n",
        "| patch_encoder_1 (PatchEncoder)  | (None, None, 32)      | 24,608  | ['patches_1[0][0]']                 |\n",
        "| multi_head_attention_2 (MultiHeadAttention) | (None, None, 32) | 16,800 | ['patch_encoder_1[0][0]', 'patch_encoder_1[0][0]'] |\n",
        "| add_4 (Add)                     | (None, None, 32)      | 0       | ['multi_head_attention_2[0][0]', 'patch_encoder_1[0][0]'] |\n",
        "| layer_normalization_3 (LayerNormalization) | (None, None, 32) | 64    | ['add_4[0][0]']                    |\n",
        "| dense_20 (Dense)                | (None, None, 64)      | 2,112   | ['layer_normalization_3[0][0]']    |\n",
        "| tf.nn.gelu_6 (TFOpLambda)       | (None, None, 64)      | 0       | ['dense_20[0][0]']                 |\n",
        "| dropout_13 (Dropout)            | (None, None, 64)      | 0       | ['tf.nn.gelu_6[0][0]']             |\n",
        "| dense_21 (Dense)                | (None, None, 32)      | 2,080   | ['dropout_13[0][0]']               |\n",
        "| tf.nn.gelu_7 (TFOpLambda)       | (None, None, 32)      | 0       | ['dense_21[0][0]']                 |\n",
        "| dropout_14 (Dropout)            | (None, None, 32)      | 0       | ['tf.nn.gelu_7[0][0]']             |\n",
        "| add_5 (Add)                     | (None, None, 32)      | 0       | ['dropout_14[0][0]', 'add_4[0][0]'] |\n",
        "| multi_head_attention_3 (MultiHeadAttention) | (None, None, 32) | 16,800 | ['add_5[0][0]', 'add_5[0][0]']    |\n",
        "| add_6 (Add)                     | (None, None, 32)      | 0       | ['multi_head_attention_3[0][0]', 'add_5[0][0]'] |\n",
        "| layer_normalization_4 (LayerNormalization) | (None, None, 32) | 64    | ['add_6[0][0]']                    |\n",
        "| dense_22 (Dense)                | (None, None, 64)      | 2,112   | ['layer_normalization_4[0][0]']    |\n",
        "| tf.nn.gelu_8 (TFOpLambda)       | (None, None, 64)      | 0       | ['dense_22[0][0]']                 |\n",
        "| dropout_15 (Dropout)            | (None, None, 64)      | 0       | ['tf.nn.gelu_8[0][0]']             |\n",
        "| dense_23 (Dense)                | (None, None, 32)      | 2,080   | ['dropout_15[0][0]']               |\n",
        "| tf.nn.gelu_9 (TFOpLambda)       | (None, None, 32)      | 0       | ['dense_23[0][0]']                 |\n",
        "| dropout_16 (Dropout)            | (None, None, 32)      | 0       | ['tf.nn.gelu_9[0][0]']             |\n",
        "| add_7 (Add)                     | (None, None, 32)      | 0       | ['dropout_16[0][0]', 'add_6[0][0]'] |\n",
        "| multi_head_attention_4 (MultiHeadAttention) | (None, None, 32) | 16,800 | ['add_7[0][0]', 'add_7[0][0]']    |\n",
        "| add_8 (Add)                     | (None, None, 32)      | 0       | ['multi_head_attention_4[0][0]', 'add_7[0][0]'] |\n",
        "| layer_normalization_5 (LayerNormalization) | (None, None, 32) | 64    | ['add_8[0][0]']                    |\n",
        "| dense_24 (Dense)                | (None, None, 64)      | 2,112   | ['layer_normalization_5[0][0]']    |\n",
        "| tf.nn.gelu_10 (TFOpLambda)      | (None, None, 64)      | 0       | ['dense_24[0][0]']                 |\n",
        "| dropout_17 (Dropout)            | (None, None, 64)      | 0       | ['tf.nn.gelu_10[0][0]']            |\n",
        "| dense_25 (Dense)                | (None, None, 32)      | 2,080   | ['dropout_17[0][0]']               |\n",
        "| tf.nn.gelu_11 (TFOpLambda)      | (None, None, 32)      | 0       | ['dense_25[0][0]']                 |\n",
        "| dropout_18 (Dropout)            | (None, None, 32)      | 0       | ['tf.nn.gelu_11[0][0]']            |\n",
        "| add_9 (Add)                     | (None, None, 32)      | 0       | ['dropout_18[0][0]', 'add_8[0][0]'] |\n",
        "| multi_head_attention_5 (MultiHeadAttention) | (None, None, 32) | 16,800 | ['add_9[0][0]', 'add_9[0][0]']    |\n",
        "| add_10 (Add)                    | (None, None, 32)      | 0       | ['multi_head_attention_5[0][0]', 'add_9[0][0]'] |\n",
        "| layer_normalization_6 (LayerNormalization) | (None, None, 32) | 64    | ['add_10[0][0]']                   |\n",
        "| dense_26 (Dense)                | (None, None, 64)      | 2,112   | ['layer_normalization_6[0][0]']    |\n",
        "| tf.nn.gelu_12 (TFOpLambda)      | (None, None, 64)      | 0       | ['dense_26[0][0]']                 |\n",
        "| dropout_19 (Dropout)            | (None, None, 64)      | 0       | ['tf.nn.gelu_12[0][0]']            |\n",
        "| dense_27 (Dense)                | (None, None, 32)      | 2,080   | ['dropout_19[0][0]']               |\n",
        "| tf.nn.gelu_13 (TFOpLambda)      | (None, None, 32)      | 0       | ['dense_27[0][0]']                 |\n",
        "| dropout_20 (Dropout)            | (None, None, 32)      | 0       | ['tf.nn.gelu_13[0][0]']            |\n",
        "| add_11 (Add)                    | (None, None, 32)      | 0       | ['dropout_20[0][0]', 'add_10[0][0]'] |\n",
        "| layer_normalization_7 (LayerNormalization) | (None, None, 32) | 64    | ['add_11[0][0]']                   |\n",
        "| global_average_pooling1d_1 (GlobalAveragePooling1D) | (None, 32) | 0  | ['layer_normalization_7[0][0]']  |\n",
        "| dense_28 (Dense)                | (None, 256)           | 8,448   | ['global_average_pooling1d_1[0][0]'] |\n",
        "| tf.nn.gelu_14 (TFOpLambda)      | (None, 256)           | 0       | ['dense_28[0][0]']                 |\n",
        "| dropout_21 (Dropout)            | (None, 256)           | 0       | ['tf.nn.gelu_14[0][0]']            |\n",
        "| dense_29 (Dense)                | (None, 128)           | 32,896  | ['dropout_21[0][0]']               |\n",
        "| tf.nn.gelu_15 (TFOpLambda)      | (None, 128)           | 0       | ['dense_29[0][0]']                 |\n",
        "| dropout_22 (Dropout)            | (None, 128)           | 0       | ['tf.nn.gelu_15[0][0]']            |\n",
        "| dense_30 (Dense)                | (None, 1)             | 129     | ['dropout_22[0][0]']               |\n",
        "\n",
        "#### Core Components and Data Flow\n",
        "1. **Input Layer**: The model starts with an InputLayer that accepts a specific input shape, which in this case, is a sequence with a length of 3840 and 4 features. This layer is purely for accepting input data, and no computations are performed here.\n",
        "2. **Instance Normalization**: Immediately after the input, an InstanceNormalization layer normalizes the data. Unlike batch normalization that works across batch dimensions, instance normalization normalizes across each channel in each data instance. This helps in stabilizing the learning process and is particularly useful in tasks involving high internal covariate shift.\n",
        "3. **Patches Creation**: The Patches layer takes the normalized data and breaks it into patches. This is typical in models that process data similarly to Vision Transformers, where the input is split into numerous patches to be processed individually. Each patch then serves as an input token for the Transformer encoder.\n",
        "4. **Patch Encoder**: The PatchEncoder layer maps these patches into a higher-dimensional space, turning each patch into an embedded representation that the Transformer can process. This is akin to word embeddings in natural language processing.\n",
        "5. **Transformer Encoder Layers**:\n",
        "Multi-Head Attention: Several MultiHeadAttention layers allow the model to focus on different parts of the input sequence, gathering contextual information from different positions simultaneously. This is key in capturing the dependencies and relationships between different parts of the input.\n",
        "Skip Connections (Add Layers): Each attention layer is followed by an Add layer, implementing skip connections that help in gradient flow during backpropagation, preventing the vanishing gradient problem common in deep networks.\n",
        "6. **Layer Normalization**: Each Add layer is followed by LayerNormalization which helps stabilize the network's training by normalizing the outputs of each layer.\n",
        "Feed-Forward Network: Post attention, the data passes through a series of Dense layers with GELU activation and Dropout. These layers act as feed-forward neural networks that transform the attention-augmented features further. This portion of the network is crucial for learning non-linear transformations and relationships in the data.\n",
        "7. **Output Preparation**:\n",
        "Global Average Pooling: Before the final output, a GlobalAveragePooling1D layer condenses the sequence of processed data into a single vector, capturing the essence of the entire sequence in a fixed-size output. This is especially useful in reducing model complexity and preparing the data for final output.\n",
        "8. **Output Layers**: The condensed data vector is then processed through more Dense layers and non-linear activations to finally output a prediction.\n",
        "\n",
        "\n",
        "### Training objectives\n",
        "The purpose of training the model is to update the parameters until a minimized loss function is achieved. The parameters will be updated by adjusting the connection weights and output biases. The goal of training the model is to obtain an accurate prediction of a sleep apnea event, based on sleep signals, including SpO2 and ECG.\n",
        "\n",
        "The ultimate goal of training is to improve the prediction accuracy of sleep apnea/hypopnea occurences using ECG and SpO2 signals.\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Creation Code\n",
        "NOTE: this is a code snippet, we ran all the code locally and the notebooks with embedded results are all checked into our GitHub repository.\n",
        "\n",
        "The notebook with the results from the Hybrid Transformer model (Hu et al., 2022) is found [here](https://github.com/dlh-team-4/t4/blob/main/Trainer_Evaluator.ipynb).\n",
        "\n",
        "The notebook with the results from the Multi-Modal Transformer model (Fayyaz et al., 2023) is found [here](https://github.com/dlh-team-4/t4/blob/main/Trainer_Evaluator_Transformer.ipynb)."
      ],
      "metadata": {
        "id": "b5gyHxTQPhSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the `Patches` class to break down images into smaller patches\n",
        "class Patches(Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size  # Size of each patch to extract\n",
        "\n",
        "    def call(self, input):\n",
        "        # Reshape the input to add a new axis\n",
        "        input = input[:, tf.newaxis, :, :]\n",
        "        batch_size = tf.shape(input)[0]  # Get the batch size\n",
        "        # Extract patches from the input image\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=input,\n",
        "            sizes=[1, 1, self.patch_size, 1],\n",
        "            strides=[1, 1, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        # Reshape the extracted patches\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "# Define the `PatchEncoder` class to project image patches into an embedding space\n",
        "class PatchEncoder(Layer):\n",
        "    def __init__(self, num_patches, projection_dim, l2_weight):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.projection_dim = projection_dim\n",
        "        self.l2_weight = l2_weight\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = Dense(units=projection_dim, kernel_regularizer=L2(l2_weight),\n",
        "                                bias_regularizer=L2(l2_weight))\n",
        "        self.position_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch)\n",
        "        return encoded\n",
        "\n",
        "# Define a function to build a multi-layer perceptron (MLP)\n",
        "def mlp(x, hidden_units, dropout_rate, l2_weight):\n",
        "    for _, units in enumerate(hidden_units):\n",
        "        x = Dense(units, activation=None, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight))(x)\n",
        "        x = tf.nn.gelu(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "# Define a function to create a transformer-based model\n",
        "def create_transformer_model(input_shape, num_patches, projection_dim, transformer_layers, num_heads, transformer_units, mlp_head_units, num_classes, drop_out, reg, l2_weight, demographic=False):\n",
        "    if reg:\n",
        "        activation = None\n",
        "    else:\n",
        "        activation = 'sigmoid'\n",
        "    inputs = Input(shape=input_shape)\n",
        "    patch_size = input_shape[0] / num_patches\n",
        "    # Normalize input features and possibly handle demographic information\n",
        "    if demographic:\n",
        "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
        "                                                             beta_initializer=\"glorot_uniform\",\n",
        "                                                             gamma_initializer=\"glorot_uniform\")(inputs[:, :, :-1])\n",
        "        demo = inputs[:, :12, -1]\n",
        "    else:\n",
        "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
        "                                                             beta_initializer=\"glorot_uniform\",\n",
        "                                                             gamma_initializer=\"glorot_uniform\")(inputs)\n",
        "\n",
        "    # Extract patches and encode them\n",
        "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
        "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
        "    # Apply transformer layers\n",
        "    for i in range(transformer_layers):\n",
        "        x1 = encoded_patches\n",
        "        attention_output = MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),\n",
        "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
        "        x2 = Add()([attention_output, encoded_patches])\n",
        "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)\n",
        "        encoded_patches = Add()([x3, x2])\n",
        "\n",
        "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
        "\n",
        "    logits = Dense(num_classes, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
        "                   activation=activation)(features)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\n",
        "        \"model_name\": \"hybrid\",\n",
        "        \"regression\": False,\n",
        "\n",
        "        \"transformer_layers\": 4,\n",
        "        \"drop_out_rate\": 0.25,\n",
        "        \"num_patches\": 20,\n",
        "        \"transformer_units\": 32,\n",
        "        \"regularization_weight\": 0.001,\n",
        "        \"num_heads\": 4,\n",
        "        \"epochs\": 100,\n",
        "        \"channels\": [14, 18, 19, 20],\n",
        "    }\n",
        "    model = get_model(config)\n",
        "    model.build(input_shape=(1, 30 * DATA_FREQ, 10))\n",
        "    print(model.summary())\n",
        "\n",
        "    # Save the model for later use\n",
        "    torch.save(model, f\"./{config['model_name']}_model.pt\")\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "Many hyperparameters can be found in the “config” variable in the program that calls the training function, which is train(). The training of both datasets utilizes a dropout rate of 0.25, a regularization weight of 0.001, and 100 epochs. Additionally, the number of transformer layers were set to 5 and the number of patches were set to 30. A table is presented below with more detailed information about the parameter settings.\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>Hyperparameter</th>\n",
        "        <th>Value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>transformer_layers</td>\n",
        "        <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>drop_out_rate</td>\n",
        "        <td>0.25</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>num_patches</td>\n",
        "        <td>20</td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>transformer_units</td>\n",
        "        <td>32</td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>regularization_weight</td>\n",
        "        <td>0.001</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>num_heads</td>\n",
        "        <td>4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>epochs</td>\n",
        "        <td>100</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>channels</td>\n",
        "        <td>[14,18,19,20]</td>\n",
        "    </tr>\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "</table>\n",
        "\n",
        "## Computational Requirements\n",
        "\n",
        "Initially, when testing and debugging, the CPU runtime on Google Colab was sufficient. When we arrived at the training stage, the CPU runtime was determined to be too time intensive. One epoch would complete after about 30 minutes. Therefore, we determined it was necessary to utilize a team member's available GPU (RTX 4090) for faster training epochs, which brought down the duration to 26s for chat and 1s for nch per epoch. All in all, we executed 2 trials (CHAT and NCH) with different configurations, and a total of <4 hours GPU hrs used while running around 60 epochs. (While we initially set 200 epochs, the actual run only reached 78 epochs for CHAT and 54 epochs for NCH)."
      ],
      "metadata": {
        "id": "XrKpzknCw20g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Code\n",
        "NOTE: this is a code snippet, we ran all the code locally and the notebooks with embedded results are all checked into our GitHub repository.\n",
        "\n",
        "The notebook with the results from the Hybrid Transformer model (Hu et al., 2022) is found [here](https://github.com/dlh-team-4/t4/blob/main/Trainer_Evaluator.ipynb).\n",
        "\n",
        "The notebook with the results from the Multi-Modal Transformer model (Fayyaz et al., 2023) is found [here](https://github.com/dlh-team-4/t4/blob/main/Trainer_Evaluator_Transformer.ipynb)."
      ],
      "metadata": {
        "id": "fJklDsIdRML0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic Training Function\n",
        "\n",
        "# Define a learning rate scheduler function that reduces the learning rate by half every 5 epochs after 50 epochs\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch > 50 and (epoch - 1) % 5 == 0:\n",
        "        lr *= 0.5\n",
        "    return lr\n",
        "\n",
        "# Generator function that loads .npz files from a directory, yielding batches of data\n",
        "def data_generator(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.npz'):\n",
        "            data = np.load(os.path.join(directory, filename), allow_pickle=True)\n",
        "            x = data['x']  # Extracts the feature array\n",
        "            y = data['y_apnea'] + data['y_hypopnea']  # Sum of apnea and hypopnea labels\n",
        "            del data  # Remove the reference to data to help with memory management\n",
        "            gc.collect()  # Explicitly invoke garbage collection\n",
        "            yield x, y  # Yield a tuple of features and labels for training\n",
        "\n",
        "# Main training function\n",
        "def train(config, fold=None):\n",
        "    # Preallocate memory for training and label data for each fold\n",
        "    x = [None]*FOLD\n",
        "    y = [None]*FOLD\n",
        "    data_gen = data_generator(config[\"data_path\"])  # Initialize the data generator\n",
        "    for i in range(FOLD):\n",
        "        x[i], y[i] = next(data_gen)  # Load data for each fold\n",
        "        x[i], y[i] = shuffle(x[i], y[i])  # Shuffle the data to prevent order bias\n",
        "        x[i] = np.nan_to_num(x[i], nan=1)  # Replace NaN values with 1 in the feature set\n",
        "        if config[\"regression\"]:\n",
        "            y[i] = np.sqrt(y[i])  # Apply square root transformation for regression problems\n",
        "            y[i][y[i] != 0] += 2  # Increment non-zero entries by 2\n",
        "        else:\n",
        "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)  # Apply a threshold to create binary labels\n",
        "\n",
        "        x[i] = x[i][:, :, config[\"channels\"]]  # Select the specified channels from the data\n",
        "\n",
        "    del data_gen  # Clean up the generator\n",
        "    gc.collect()  # Collect garbage to free memory\n",
        "\n",
        "    # Determine which folds to use for training based on input parameters\n",
        "    folds = range(FOLD) if fold is None else [fold]\n",
        "\n",
        "    for fold in folds:\n",
        "        first = True\n",
        "        for i in range(5):\n",
        "            if i != fold:\n",
        "                if first:\n",
        "                    x_train = x[i]\n",
        "                    y_train = y[i]\n",
        "                    first = False\n",
        "                else:\n",
        "                    # Aggregate training data from different folds\n",
        "                    x_train = np.concatenate((x_train, x[i]))\n",
        "                    y_train = np.concatenate((y_train, y[i]))\n",
        "\n",
        "        # Initialize the model\n",
        "        model = get_model(config)\n",
        "        if config[\"regression\"]:\n",
        "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
        "        else:\n",
        "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
        "                          metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "\n",
        "        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "        print(f\"train {x_train.shape}\")  # Print the shape of the training dataset\n",
        "        # Fit the model\n",
        "        model.fit(x=x_train, y=y_train, batch_size=512, epochs=config[\"epochs\"], validation_split=0.1,\n",
        "                  callbacks=[early_stopper, lr_scheduler])\n",
        "\n",
        "        # Save the trained model\n",
        "        model.save(config[\"model_path\"] + str(fold))\n",
        "        keras.backend.clear_session()  # Clear the TensorFlow session to free memory\n"
      ],
      "metadata": {
        "id": "frx9yb7sRVJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHAT Training\n",
        "\n",
        "# Mapping from signal types to indices, clarifying how each signal is processed and stored\n",
        "sig_dict_chat = {\n",
        "    \"EOG\": [0, 1],  # Electrooculography signals\n",
        "    \"EEG\": [4, 5],  # Electroencephalography signals for two channels\n",
        "    \"ECG\": [15, 16],  # Electrocardiography signals, including RRI (Respiratory Rate Index) and Ramp\n",
        "    \"Resp\": [9, 10],  # Respiratory signals: Cannula flow and airflow\n",
        "    \"SPO2\": [13],  # Oxygen saturation level\n",
        "    \"CO2\": [14],  # Carbon dioxide levels\n",
        "}\n",
        "\n",
        "# List of combinations of channels to use in different model configurations\n",
        "channel_list_chat = [\n",
        "    [\"ECG\", \"SPO2\"],\n",
        "]\n",
        "\n",
        "# Loop through each channel combination specified in channel_list_chat\n",
        "for ch in channel_list_chat:\n",
        "    chs = []  # To collect indices of the channels from sig_dict_chat\n",
        "    chstr = \"\"  # To create a string representation of the channels for naming purposes\n",
        "    for name in ch:\n",
        "        chstr += name  # Append the name of the channel to the string\n",
        "        chs += sig_dict_chat[name]  # Append channel indices to the list\n",
        "\n",
        "    print(chstr, chs)  # Output the combined name of the channels and their indices\n",
        "\n",
        "    # Configuration for the training of the model\n",
        "    config = {\n",
        "        \"data_path\": \"D:\\\\chat_128_FREQ_45\\\\\",  # Path where the training data is stored\n",
        "        \"model_path\": \"C:\\\\Users\\William\\\\Documents\\\\Development\\\\Python projects\\\\Deeplearning healthcare\\\\Pediatric-Apnea-Detection\\\\chat_Transformer_model.pt\",\n",
        "        \"model_name\": \"Transformer_chat_\" + chstr,  # Name of the model, including the channel combination\n",
        "        \"regression\": False,  # Indicates if the model should perform regression; False implies classification\n",
        "\n",
        "        \"transformer_layers\": 5,  # Number of transformer layers in the model\n",
        "        \"drop_out_rate\": 0.25,  # Dropout rate for regularization\n",
        "        \"num_patches\": 30,  # Number of patches to divide the input into for the Transformer\n",
        "        \"transformer_units\": 32,  # Size of the Transformer embeddings\n",
        "        \"regularization_weight\": 0.001,  # L2 regularization weight\n",
        "        \"num_heads\": 4,  # Number of attention heads in the Transformer model\n",
        "        \"epochs\": 200,  # Number of epochs to train the model\n",
        "        \"channels\": chs,  # Channels selected for training based on the current combination\n",
        "    }\n",
        "\n",
        "    train(config, 0)  # Call the training function with the specified configuration and fold index\n"
      ],
      "metadata": {
        "id": "WgHsc2ULR-uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NCH Training\n",
        "\n",
        "# Dictionary mapping signal types to their corresponding channel indices\n",
        "sig_dict_nch = {\n",
        "    \"EOG\": [0, 1],  # Electrooculography signals from two channels\n",
        "    \"EEG\": [2, 3],  # Electroencephalography signals from two channels\n",
        "    \"RESP\": [5, 6],  # Two respiratory signals\n",
        "    \"SPO2\": [9],     # Oxygen saturation\n",
        "    \"CO2\": [10],     # Carbon dioxide levels\n",
        "    \"ECG\": [11, 12], # Electrocardiogram signals, including RRI and Ramp\n",
        "    \"DEMO\": [13],    # Demographic data, which might include patient-specific information\n",
        "}\n",
        "\n",
        "# List of signal combinations to be used for training different model configurations\n",
        "channel_list_nch = [\n",
        "    [\"ECG\", \"SPO2\"],\n",
        "]\n",
        "\n",
        "# Loop through each specified channel combination to configure and train a model\n",
        "for ch in channel_list_nch:\n",
        "    chs = []  # Initialize an empty list to hold the indices of channels\n",
        "    chstr = \"\"  # Initialize an empty string to concatenate signal names for model naming\n",
        "    for name in ch:\n",
        "        chstr += name  # Concatenate signal names to form part of the model name\n",
        "        chs += sig_dict_nch[name]  # Add channel indices for the current signal name to the list\n",
        "\n",
        "    # Configuration dictionary for the model training\n",
        "    config = {\n",
        "        \"data_path\": \"D:\\\\nch_128_FREQ_45\\\\\",  # Directory containing the training data\n",
        "        \"model_path\": \"C:\\\\Users\\\\William\\\\Documents\\\\Development\\\\Python projects\\\\Deeplearning healthcare\\\\Pediatric-Apnea-Detection\\\\nch_Transformer_model.pt\",\n",
        "        \"model_name\": \"Transformer_nch_\" + chstr,  # Constructed model name indicating type and channel combination\n",
        "        \"regression\": False,  # Specifies if the model is for regression; False indicates classification\n",
        "\n",
        "        # Model parameters\n",
        "        \"transformer_layers\": 5,  # Number of transformer layers\n",
        "        \"drop_out_rate\": 0.25,  # Dropout rate to prevent overfitting\n",
        "        \"num_patches\": 30,  # Number of patches the input is divided into\n",
        "        \"transformer_units\": 32,  # Size of each transformer unit\n",
        "        \"regularization_weight\": 0.001,  # L2 regularization to penalize large weights\n",
        "        \"num_heads\": 4,  # Number of attention heads in the transformer model\n",
        "        \"epochs\": 200,  # Total number of training epochs\n",
        "        \"channels\": chs,  # Channels used from the data, based on the current combination\n",
        "    }\n",
        "    # Call the training function, passing in the configuration and the specific fold to train on (fold 0 in this case)\n",
        "    train(config, 0)\n"
      ],
      "metadata": {
        "id": "I9F7j6M3SLDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "According to the original paper, the evaluation was conducted using AUROC and F1 scores. The original author's results from both the CHAT and NCH datasets were found in Table 3. In our project, we intended to replicate these results as close as possible, using the same evaluation approaches.\n",
        "\n",
        "## Metrics descriptions\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>Metric</th>\n",
        "        <th>Description</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>AUROC (Area under the receiver operating characteristic curve)</td>\n",
        "        <td> The capability of discerning different classes. The ideal AUROC score is 1.0.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>F1 Score</td>\n",
        "        <td> The average of precision and recall. The ideal F1 score is 1.0.</td>\n",
        "    </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "wzlkLJ74w_XX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Code\n",
        "\n",
        "NOTE: this is a code snippet, we ran all the code locally and the notebooks with embedded results are all checked into our GitHub repository.\n",
        "\n",
        "The notebook with the results from the Hybrid Transformer model (Hu et al., 2022) is found [here](https://github.com/dlh-team-4/t4/blob/main/Trainer_Evaluator.ipynb).\n",
        "\n",
        "The notebook with the results from the Multi-Modal Transformer model (Fayyaz et al., 2023) is found [here](https://github.com/dlh-team-4/t4/blob/main/Trainer_Evaluator_Transformer.ipynb)."
      ],
      "metadata": {
        "id": "zO7SFFHbS6_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic Evaluation Function\n",
        "\n",
        "class Result:\n",
        "    def __init__(self):\n",
        "        # Initialize lists to hold metric values across tests\n",
        "        self.accuracy_list = []\n",
        "        self.sensitivity_list = []\n",
        "        self.specificity_list = []\n",
        "        self.f1_list = []\n",
        "        self.auroc_list = []\n",
        "        self.auprc_list = []\n",
        "        self.precision_list = []\n",
        "\n",
        "    def add(self, y_test, y_predict, y_score):\n",
        "        # Calculate confusion matrix elements\n",
        "        C = confusion_matrix(y_test, y_predict, labels=(1, 0))\n",
        "        TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
        "\n",
        "        # Calculate and append performance metrics\n",
        "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
        "        sn = TP / (TP + FN)\n",
        "        sp = TN / (TN + FP)\n",
        "        pr = TP / (TP + FP)\n",
        "        f1 = f1_score(y_test, y_predict)\n",
        "        auc = roc_auc_score(y_test, y_score)\n",
        "        auprc = average_precision_score(y_test, y_score)\n",
        "\n",
        "        # Multiply by 100 for percentage format\n",
        "        self.accuracy_list.append(acc * 100)\n",
        "        self.precision_list.append(pr * 100)\n",
        "        self.sensitivity_list.append(sn * 100)\n",
        "        self.specificity_list.append(sp * 100)\n",
        "        self.f1_list.append(f1 * 100)\n",
        "        self.auroc_list.append(auc * 100)\n",
        "        self.auprc_list.append(auprc * 100)\n",
        "\n",
        "    def get(self):\n",
        "        # Generate a formatted string of results for output\n",
        "        out_str = \"=========================================================================== \\n\"\n",
        "        # Adds each metric's list to the output string\n",
        "        metrics = [\"accuracy\", \"precision\", \"sensitivity\", \"specificity\", \"f1\", \"auroc\", \"auprc\"]\n",
        "        for metric, values in zip(metrics, [\n",
        "            self.accuracy_list, self.precision_list, self.sensitivity_list,\n",
        "            self.specificity_list, self.f1_list, self.auroc_list, self.auprc_list]):\n",
        "            out_str += f\"{metric.upper()}: {values} \\n\"\n",
        "            out_str += f\"Mean {metric.capitalize()}: {np.mean(values):.2f} ± {np.std(values):.3f} \\n\"\n",
        "\n",
        "        return out_str\n",
        "\n",
        "    def print(self):\n",
        "        # Print formatted results\n",
        "        print(self.get())\n",
        "\n",
        "    def visualize(self):\n",
        "        # Visualization of metrics using matplotlib\n",
        "        metrics = {\n",
        "            'Accuracy': self.accuracy_list,\n",
        "            'Precision': self.precision_list,\n",
        "            'Recall (Sensitivity)': self.sensitivity_list,\n",
        "            'Specificity': self.specificity_list,\n",
        "            'F1 Score': self.f1_list,\n",
        "            'AUROC': self.auroc_list,\n",
        "            'AUPRC': self.auprc_list\n",
        "        }\n",
        "        fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(14, 22))\n",
        "        axes = axes.flatten()\n",
        "        for ax, (metric_name, values) in zip(axes, metrics.items()):\n",
        "            ax.plot(values, label=f'{metric_name}', marker='o', linestyle='-')\n",
        "            ax.set_title(metric_name)\n",
        "            ax.legend(loc='best')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save(self, path, config):\n",
        "        # Save results to a file\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w+\") as file:\n",
        "            file.write(str(config))\n",
        "            file.write(\"\\n\")\n",
        "            file.write(self.get())\n",
        "\n",
        "# ...\n",
        "\n",
        "# Define a sigmoid function which is commonly used as an activation function in neural networks\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define a function to test the model with a given configuration, possibly across specific folds of data\n",
        "def test(config, fold=None):\n",
        "    # Initialize lists to store data for each fold\n",
        "    x = [None] * FOLD\n",
        "    y = [None] * FOLD\n",
        "    # Create a data generator to fetch batches of data\n",
        "    data_gen = data_generator(config[\"data_path\"])\n",
        "\n",
        "    # Iterate over each fold to retrieve and preprocess the data\n",
        "    for i in range(FOLD):\n",
        "        x[i], y[i] = next(data_gen)  # Load next batch of data\n",
        "        x[i], y[i] = shuffle(x[i], y[i])  # Shuffle the data to ensure random distribution\n",
        "        x[i] = np.nan_to_num(x[i], nan=-1)  # Convert NaN values to -1 in the dataset\n",
        "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)  # Binarize the output based on a predefined threshold\n",
        "        x[i] = x[i][:, :, config[\"channels\"]]  # Select specific channels as defined in the configuration\n",
        "\n",
        "    # Clean up resources and run garbage collection to free memory\n",
        "    del data_gen\n",
        "    gc.collect()\n",
        "\n",
        "    # Initialize a result object to store performance metrics\n",
        "    result = Result()\n",
        "    # Determine the range of folds to test based on the input parameter 'fold'\n",
        "    folds = range(FOLD) if fold is None else [fold]\n",
        "\n",
        "    # Iterate through the specified folds to perform testing\n",
        "    for fold in folds:\n",
        "        x_test = x[fold]  # Select the test data for the current fold\n",
        "        if config.get(\"test_noise_snr\"):\n",
        "            # Optionally add noise to the data for robustness testing\n",
        "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
        "\n",
        "        y_test = y[fold]  # Select the test labels for the current fold\n",
        "\n",
        "        # Load the model specific to the current fold\n",
        "        model = tf.keras.models.load_model(config[\"model_path\"] + str(fold), compile=False)\n",
        "\n",
        "        # Predict outcomes using the model\n",
        "        predict = model.predict(x_test)\n",
        "        y_score = predict  # Probabilities of predictions\n",
        "        # Generate binary predictions from the probabilities\n",
        "        y_predict = np.where(predict > 0.5, 1, 0)\n",
        "\n",
        "        # Add the results of the current test to the result object\n",
        "        result.add(y_test, y_predict, y_score)\n",
        "\n",
        "    # Output the results to the console\n",
        "    result.print()\n",
        "    # Visualize the results using plots\n",
        "    result.visualize()\n",
        "    # Save the results to a file\n",
        "    result.save(\"./results/\" + config[\"model_name\"] + \".txt\", config)\n",
        "\n",
        "    # Clean up variables to free memory\n",
        "    del x_test, y_test, model, predict, y_score, y_predict\n"
      ],
      "metadata": {
        "id": "Z3_jiQSKS6Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHAT Evaluation\n",
        "\n",
        "# Dictionary mapping signal types to specific channel indices as used in data processing\n",
        "sig_dict_chat = {\n",
        "    \"EOG\": [0, 1],\n",
        "    \"EEG\": [4, 5],\n",
        "    \"ECG\": [15, 16],\n",
        "    \"Resp\": [9, 10],\n",
        "    \"SPO2\": [13],\n",
        "    \"CO2\": [14],\n",
        "}\n",
        "\n",
        "# List defining specific combinations of channels to be used for different model configurations\n",
        "channel_list_chat = [\n",
        "    [\"ECG\", \"SPO2\"],\n",
        "]\n",
        "\n",
        "# Iterating through each channel combination to set up and evaluate the model\n",
        "for ch in channel_list_chat:\n",
        "    chs = []  # List to store channel indices for the current configuration\n",
        "    chstr = \"\"  # String to concatenate channel names for display and model identification\n",
        "    for name in ch:\n",
        "        chstr += name  # Append the name of each channel to the string\n",
        "        chs += sig_dict_chat[name]  # Extend the list of channel indices by fetching from the dictionary\n",
        "\n",
        "    # Output the combination of channels being used for the current model configuration\n",
        "    print(chstr, chs)\n",
        "\n",
        "    # Configuration dictionary specifying model settings and paths\n",
        "    config = {\n",
        "        \"data_path\": \"D:\\\\chat_128_FREQ_45\\\\\",\n",
        "        \"model_path\": \"C:\\\\Users\\\\William\\\\Documents\\\\Development\\\\Python projects\\\\Deeplearning healthcare\\\\Pediatric-Apnea-Detection\\\\chat_Transformer_model.pt\",\n",
        "        \"model_name\": \"Transformer_chat_\" + chstr,\n",
        "        \"regression\": False,\n",
        "        \"transformer_layers\": 5,\n",
        "        \"drop_out_rate\": 0.25,\n",
        "        \"num_patches\": 30,\n",
        "        \"transformer_units\": 32,\n",
        "        \"regularization_weight\": 0.001,\n",
        "        \"num_heads\": 4,\n",
        "        \"epochs\": 200,\n",
        "        \"channels\": chs,\n",
        "    }\n",
        "\n",
        "    # Call the test function passing the configuration and specifying fold '0' (this could be any or all folds)\n",
        "    test(config, 0)\n"
      ],
      "metadata": {
        "id": "TFFhhh7fTSbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NCH Evaluation\n",
        "\n",
        "# Define a dictionary mapping signal types to specific channel indices\n",
        "sig_dict_nch = {\n",
        "    \"EOG\": [0, 1],\n",
        "    \"EEG\": [2, 3],\n",
        "    \"RESP\": [5, 6],\n",
        "    \"SPO2\": [9],\n",
        "    \"CO2\": [10],\n",
        "    \"ECG\": [11, 12],\n",
        "    \"DEMO\": [13],\n",
        "}\n",
        "\n",
        "# List of signal combinations to evaluate with the model\n",
        "channel_list_nch = [\n",
        "    [\"ECG\", \"SPO2\"],\n",
        "]\n",
        "\n",
        "# Iterate through each channel combination to set up and train models\n",
        "for ch in channel_list_nch:\n",
        "    chs = []  # List to hold indices for selected channels\n",
        "    chstr = \"\"  # String to hold concatenated names for model identification\n",
        "    for name in ch:\n",
        "        chstr += name  # Append channel name to identifier string\n",
        "        chs += sig_dict_nch[name]  # Append channel indices to list\n",
        "\n",
        "    # Display the selected channels and their corresponding indices\n",
        "    print(chstr, chs)\n",
        "\n",
        "    # Configuration dictionary for model training\n",
        "    config = {\n",
        "        \"data_path\": \"D:\\\\nch_128_FREQ_45\\\\\",  # Data directory\n",
        "        \"model_path\": \"C:\\\\Users\\\\William\\\\Documents\\\\Development\\\\Python projects\\\\Deeplearning healthcare\\\\Pediatric-Apnea-Detection\\\\nch_Transformer_model.pt\",  # Model save path\n",
        "        \"model_name\": \"Transformer_nch_\" + chstr,  # Model name including channel info\n",
        "        \"regression\": False,  # Task type (classification here)\n",
        "\n",
        "        # Model parameters\n",
        "        \"transformer_layers\": 5,\n",
        "        \"drop_out_rate\": 0.25,\n",
        "        \"num_patches\": 30,\n",
        "        \"transformer_units\": 32,\n",
        "        \"regularization_weight\": 0.001,\n",
        "        \"num_heads\": 4,\n",
        "        \"epochs\": 200,\n",
        "        \"channels\": chs,\n",
        "    }\n",
        "\n",
        "    # Initiate evaluation with the current configuration\n",
        "    test(config, 0)\n"
      ],
      "metadata": {
        "id": "DOMKZhzDTdAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "We ran the training with different configurations:\n",
        "1. Using the Hybrid Transformer Model (Hu et al., 2022)\n",
        "2. Using the Multi-modal Transformer Model (Fayyaz et al., 2023) - Complete run, but with preprocessing errors for the NCH dataset\n",
        "3. Rerun of the NCH dataset using the Multi-modal Transformer Model (Fayyaz et al., 2023) - Partial run, with fixes to the preprocessing errors for the NCH dataset.\n",
        "\n",
        "## Table of Results\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>Model</th>\n",
        "        <th>Dataset</th>\n",
        "        <th>Accuracy</th>\n",
        "        <th>Precision</th>\n",
        "        <th>Recall</th>\n",
        "        <th>Specifity</th>\n",
        "        <th>F1</th>\n",
        "        <th>AUROC</th>\n",
        "        <th>AUPRC</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td rowspan=\"2\">Hybrid Transformer Model</td>\n",
        "        <td>CHAT</td>\n",
        "        <td>75.32</td>\n",
        "        <td>71.98</td>\n",
        "        <td>83.28</td>\n",
        "        <td>67.28</td>\n",
        "        <td>77.22</td>\n",
        "        <td>84.25</td>\n",
        "        <td>83.71</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> NCH<br>(preprocessing <br>not fixed)</td>\n",
        "        <td>76.08</td>\n",
        "        <td>72.92</td>\n",
        "        <td>85.37</td>\n",
        "        <td>66.17</td>\n",
        "        <td>78.65</td>\n",
        "        <td>85.50</td>\n",
        "        <td>84.13</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td rowspan=\"2\">Multi-Modal Transformer Model</td>\n",
        "        <td>CHAT</td>\n",
        "        <td>83.70</td>\n",
        "        <td>83.89</td>\n",
        "        <td>83.60</td>\n",
        "        <td>83.80</td>\n",
        "        <td>83.75</td>\n",
        "        <td>90.58</td>\n",
        "        <td>89.58</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> NCH<br>(preprocessing <br>not fixed)</td>\n",
        "        <td>75.36</td>\n",
        "        <td>73.89</td>\n",
        "        <td>80.84</td>\n",
        "        <td>69.52</td>\n",
        "        <td>77.20</td>\n",
        "        <td>81.56</td>\n",
        "        <td>77.59</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "## Claims\n",
        "\n",
        "With our F1 Score for the Multi-Modal Transformer model being 0.8 and values very close to the original paper, it shows that ECG signals can be used to detect Apnea/Hypopnea in children. However, because we have a preprocessing error for NCH, we cannot say the same for SPO2 because CHAT signal data does not include it.\n",
        "\n",
        "## Ablation Study\n",
        "The original code base for NCH uses an AHI file (.csv) computed from the signal to identify the presence of apnea/hypopnea. Since the computation code for it was not checked in with the codebase, we modified our approach for training the NCH dataset to use the ECG in labeling the signal data, a similar approach used in preprocessing the CHAT dataset.\n",
        "\n",
        "We were originally going to experiment between the different frequency values used in the configuration but because running even a signle epoch is time consuming, we were not able to execute this plan.\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison\n",
        "\n",
        "The original paper compares the model's results with four different models, each presented in a separate research paper, with an idential goal of detecting sleep apnea. Chang et al. proposed a CNN approach to sleep apnea detection. Chen et al. created a fusion network combines with various CNNs. Zarei et al. approached the task using a CNN with an LSTM (Long short-term memory). Hu et al. implemented a hybrid transformer approach.\n",
        "\n",
        "In reference to the CHAT dataset, the original multi-modal transformer model resulted in an F1 score of 83.9 and an AUROC of 90.6, which was higher than the previous four approaches presented. In reference to the NCH dataset, the original multi-modal transformer model resulted in an F1 score of 82.9 and an AUROC of 90.7, which was also higher than the previous four approaches presented.\n",
        "\n",
        "Our results for CHAT yielded an F1 score of 83.75 and an AUROC of 90.58, which was very close to the values reported by the original paper, but for NCH, it was off by a wide margin, with an F1 score of 77.2 and an AUROC of 81.56.\n",
        "\n",
        "<table border=\"1\">\n",
        "    <tr>\n",
        "        <th>Method</th>\n",
        "        <th>Total params</th>\n",
        "        <th colspan=\"2\">CHAT</th>\n",
        "        <th colspan=\"2\">NCH Data Bank</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td></td>\n",
        "        <td></td>\n",
        "        <td>F1</td>\n",
        "        <td>AUROC</td>\n",
        "        <td>F1</td>\n",
        "        <td>AUROC</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>CNN (Chang et al., 2020)</td>\n",
        "        <td></td>\n",
        "        <td>77.5 (0.8)</td>\n",
        "        <td>86.8 (1.0)</td>\n",
        "        <td>77.2 (1.1)</td>\n",
        "        <td>86.4 (1.2)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>SE-MSCNN (Chen et al., 2022)</td>\n",
        "        <td></td>\n",
        "        <td>73.9 (2.1)</td>\n",
        "        <td>82.9 (1.8)</td>\n",
        "        <td>73.0 (2.4)</td>\n",
        "        <td>82.2 (1.9)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>CNN+LSTM (Zarei et al., 2022)</td>\n",
        "        <td></td>\n",
        "        <td>81.7 (0.9)</td>\n",
        "        <td>89.7 (0.7)</td>\n",
        "        <td>81.7 (0.8)</td>\n",
        "        <td>89.4 (0.6)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Hybrid Transformer (Hu et al., 2022)</td>\n",
        "        <td>454,820</td>\n",
        "        <td>81.3 (1.0)</td>\n",
        "        <td>89.6 (0.5)</td>\n",
        "        <td>81.0 (0.9)</td>\n",
        "        <td>89.4 (0.7)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Multi-modal Transformer (Fayyaz et al., 2023)</td>\n",
        "        <td>150,369</td>\n",
        "        <td>83.1 (1.0)</td>\n",
        "        <td>90.0 (0.8)</td>\n",
        "        <td>82.6 (0.5)</td>\n",
        "        <td>90.4 (0.4)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Multi-modal Transformer (Replication)</td>\n",
        "        <td>150,369</td>\n",
        "        <td>83.75 (0.8)</td>\n",
        "        <td>90.58</td>\n",
        "        <td>77.20 (0.8)</td>\n",
        "        <td>81.56</td>\n",
        "    </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "## Assessment and next steps\n",
        "Our results had two trials. The first one is testing the model against the CHAT dataset, and for this, we were able to achieve a slightly better result but very close to the original. So for this, we assess that it is reproducible. As for the second trial, where we tested the model against the NCH dataset, and our results were off by a wide margin (see model comparison section). There was an error in preprocessing that we had to fix very close to the deadline and we did not have enough time to rerun against the whole NCH dataset. This should be the immediate next step for us.\n",
        "\n",
        "## Factors for Success during Replication\n",
        "\n",
        "These following are the factors that made it easier for us to replicate the study:\n",
        "\n",
        "- **Quick Dataset Access Approval**: Unlike the other dataset providers we have asked before the paper selection who made it clear that access grants take several months to get approved, NSRR approved our access request within 2 weeks of requesting. It was also helpful the UIUC had HIPAA trainings that were readily available because it was one of the prerequisites of NSRR.\n",
        "- **Availability of the Original Codebase**: The original code was clearly written and the intent was apparent. It was a good starting point for us, even though it did not work right out of the box.\n",
        "- **GPU Availability**: one of our members had a performant GPU which made our preprocessing and training code run faster and with the complete CHAT and NCHDB dataset from NSRR.\n",
        "\n",
        "## Challenges\n",
        "\n",
        "We encountered several challenges as data formats, software libraries, and data structures evolve over time. Here's a discussion of the difficulties we faced:\n",
        "\n",
        "- **Dataset Size**: The signal files we had to parse were huge. The .edf files were around half a gigabyte in size each. This also meant that the code may run really slowly unless more performant hardware is used. Also, we initially set 200 epochs, the actual run only reached 78 epochs for CHAT and 54 epochs for NCH because of memory limitations.\n",
        "- **Processing Bottleneck**: Since only one teammate had a GPU which can store and process the whole NCH and CHAT dataset, it was a bottleneck in producing valid results. We had some findings that we had to fix a bug in preprocessing very close to the submission day and we did not have enough time to run it properly due to this bottleneck.\n",
        "- **Changes in File Formats**: The file format used for storing data has changed (e.g., from .tsv to .xml), so we had to change the way data is parsed and processed.\n",
        "- **Missing Data or Code**: Our main difficulty with NCH is that it the AHI file which was referenced in the original codebase was not available to us, or if it was programmatically computed from the EDF files, the script was not checked in. This led us to change our approach very close to the deadline.\n",
        "- **Altered Field Names**: Changes in the names of fields within datasets require a thorough examination of how data is now structured versus how it was in the original study.\n",
        "- **Lack of Documentation**: We had to infer how the data in the CHAT dataset is mapped to the data in NCH because it was not explicitly mentioned anywhere in the NSRR documentation.\n",
        "- **Modification in Tokens**: In data processing, especially in text or event data, specific tokens or markers seem to have changed (e.g., \"Wake\" to \"Wake|0\"). We had to understand how the tokens were used in the context of the analysis and update the scripts.\n",
        "- **Updates in Libraries and Functions**: Software libraries evolve, and functions may become deprecated, moved, or removed in newer versions. We had to update a lot of imports and in some cases, downgrade the versions of the libraries we used.\n",
        "\n",
        "## Suggestions for Improvement\n",
        "\n",
        "Here are some of our suggestions to the authors of the study or to other reproducers on how to improve the reproducibility:\n",
        "- **Regular Codebase Maintenance**: Ensuring that the scripts checked into the repository are at the very least consistent in terms of dimensions. e.g. Preprocessing code produces (3840, 3) dimension but the training code expects (1920, 3).\n",
        "- **Documentation Quality Improvements**: The codebase includes some annotations, but might be useful to elaborate in terms of intent. It would also be optimal to include the actual versions used in running the code to prevent conflicts between libraries.\n",
        "- **Including Pre-trained Models in the Repository**: Because of the intricate process needed for obtaining data, building the model and training it, it would have been beneficial if a pre-trained model is included in the GitHub repository.\n",
        "- **Better Processing Hardware**: Debugging results that are far from the orignal would be faster if performant processing hardware\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Fayyaz H, Strang A, Beheshti R. Bringing At-home Pediatric Sleep Apnea Testing Closer to Reality: A Multi-modal Transformer Approach. Proc Mach Learn Res. 2023 Aug;219:167-185. PMID: 38344396; PMCID: PMC10854997.\n",
        "\n",
        "2. Lee H, Li B, DeForte S, Splaingard ML, Huang Y, Chi Y, Linwood SL. A large collection of real-world pediatric sleep studies. Sci Data. 2022 Jul 19;9(1):421. doi: 10.1038/s41597-022-01545-6. PMID: 35853958; PMCID: PMC9296671.\n",
        "\n",
        "3. Marcus CL, Moore RH, Rosen CL, Giordani B, Garetz SL, Taylor HG, Mitchell RB, Amin R, Katz ES, Arens R, Paruthi S, Muzumdar H, Gozal D, Thomas NH, Ware J, Beebe D, Snyder K, Elden L, Sprecher RC, Willging P, Jones D, Bent JP, Hoban T, Chervin RD, Ellenberg SS, Redline S; Childhood Adenotonsillectomy Trial (CHAT). A randomized trial of adenotonsillectomy for childhood sleep apnea. N Engl J Med. 2013 Jun 20;368(25):2366-76. doi: 10.1056/NEJMoa1215881. Epub 2013 May 21. PMID: 23692173; PMCID: PMC3756808.\n",
        "\n",
        "4. Zhang GQ, Cui L, Mueller R, Tao S, Kim M, Rueschman M, Mariani S, Mobley D, Redline S. The National Sleep Research Resource: towards a sleep data commons. J Am Med Inform Assoc. 2018 Oct 1;25(10):1351-1358. doi: 10.1093/jamia/ocy064. PMID: 29860441; PMCID: PMC6188513.\n",
        "\n",
        "5. Chen Xianhui, Chen Ying, Ma Wenjun, Fan Xiaomao, and Li Ye. Toward sleep apnea detection with lightweight multi-scaled fusion network. Knowledge-Based Systems, 247: 108783, 2022.\n",
        "\n",
        "6. Zarei Asghar, Beheshti Hossein, and Asl Babak Mohammadzadeh. Detection of sleep apnea using deep neural networks and single-lead ecg signals. Biomedical Signal Processing and Control, 71:103125, 2022.\n",
        "\n",
        "7. Hu Shuaicong, Cai Wenjie, Gao Tijie, and Wang Mingjie. A hybrid transformer model for obstructive sleep apnea detection based on self-attention mechanism using single-lead ecg. IEEE Transactions on Instrumentation and Measurement, 71:1-11, 2022.\n",
        "\n",
        "8. Chang HY, Yeh CY, Lee CT, Lin CC. A Sleep Apnea Detection System Based on a One-Dimensional Deep Convolution Neural Network Model Using Single-Lead Electrocardiogram. Sensors (Basel). 2020 Jul 26;20(15):4157. doi: 10.3390/s20154157. PMID: 32722630; PMCID: PMC7435835."
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}